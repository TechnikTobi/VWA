\documentclass[a4paper,12pt,ngerman,oneside]{scrreprt}	%Festlegen der Dokumentenklasse				%Erstellen der Dokument-Klasse

% Das ist ein Kommentar
% Bei Anführungszeichen: unteres -> Anführungszeichen + Shift+Akzent; oberes -> Anführungszeichen + Shift+Hashtag
% Mathmatischer Ausdruck -> $AUSDRUCK$ oder \(AUSDRUCK\)
% Tiefgestelte Zahlen: x_1

\usepackage[ngerman]{babel} 																		%Deutsche Einstellungen
\usepackage[utf8]{inputenc} 																		%UTF8
\usepackage[T1]{fontenc}																			%Schriftart für den Titel
\usepackage{csquotes}																				%Für Anführungszeichen
\usepackage[onehalfspacing]{setspace}																%1,5 Zeilenabstand
\usepackage{geometry}																				%Für die Bestimmung der Größe der Seiten
\usepackage[backend=bibtex, citestyle=authoryear, date=short,]{biblatex}											%Fürs Literaturverzeichnis
\usepackage{graphicx}																				%Fürs Einbinden von Grafiken
\usepackage{wrapfig}																				%Fürs Verschieben von Grafiken
\usepackage{amsmath}																				%Für Vektoren
\usepackage{abstract}																				%Für das Abstract
\usepackage{url}																					%Für URLs im Literaturverzeichnis
\usepackage{etoolbox}	
\usepackage{chngcntr}	
\usepackage[nopostdot, toc]{glossaries}																%Fürs Glossar
\usepackage{acronym}																				%Fürs Abkürzungsverzeichnis
\usepackage{listings}																				%Fürs Formatieren von Code Teil 1
\usepackage{color}																					%Fürs Formatieren von Code Teil 2
\usepackage{lstlinebgrd}																			%Fürs Formatieren von Code Teil 3
\usepackage{pgfplots}																				%Für Graphen
\usepackage{subcaption}																				%Für subcaptions für subBilder -> Nicht ganz klar ob das Package benötigt wird
\usepackage[format=plain,indention=0cm,labelfont=bf]{caption}										%Formatierung von Bildunterschriften
\usepackage{lineno}																					%Für Zeilennummerierung für Korrektur
\usepackage[hang,flushmargin]{footmisc} 															%Für Indention in Fußnoten
\usepackage{amsfonts,amssymb}

\usetikzlibrary{datavisualization}																	%Ebenfalls für Graphen
\usetikzlibrary{decorations.markings}																%Pfeilspitzen in der Mitte

\geometry{a4paper,left=35mm,right=25mm,top=10mm} 													%Format der Datei + Abstand zu den Rändern

\addtokomafont{chapter}{\rmfamily} 																	%Schriftart für Kapitelüberschriften
\addtokomafont{chapterentry}{\rmfamily}																%Schriftart für Kapiteleinstiege
\addtokomafont{section}{\rmfamily}																	%Schriftart für Unterkapitelüberschriften
\addtokomafont{subsection}{\rmfamily}																%Schriftart für Unterunterkapitelüberschriften
\addtokomafont{subsubsection}{\rmfamily} 															%Schriftart für Unterunterunterkapitelüberschriften
\addtokomafont{descriptionlabel}{\rmfamily} 														%Schriftart für Glossar

\setlength{\parindent}{0pt}																			%Nach Absatz nicht einrücken

\addbibresource{Literaturverzeichnis.bib} 															%Einbinden des Literaturverzeichnises
   
\DeclareFieldFormat{urldate}{%
	(Zuletzt besucht am \thefield{urlday}. \thefield{urlmonth}. \thefield{urlyear}\isdot)}			%Fürs Datum beim Literaturverzeichnis
\DeclareFieldFormat{url}{URL: \url{#1}} 
\DeclareNameAlias{sortname}{family-given}															%Nachname vor Vorname Teil 1
\DeclareNameAlias{default}{family-given}															%Nachname vor Vorname Teil 2

\renewcommand*{\labelnamepunct}{\addcolon\addspace} 												%Beistrich und Abstand nach dem Namen des Autors
\renewcommand*{\mkbibnamefamily}[1]{\MakeUppercase{#1}}												%Nachname des Autors in Großbuchstaben
\renewcommand{\multinamedelim}{\addslash}															%Mehrere Autoren durch Slash separiert
\renewcommand*{\finalnamedelim}{\addslash}															%Mehrere Autoren durch Slash separiert				


\nocite{Practitioner}																				%Begin Einbinden der Werke																				
\nocite{Nielsen}
\nocite{Gurney}
\nocite{Fundamentals}
\nocite{Rashid}
\nocite{artala}
\nocite{DickeBuch}
\nocite{IBM}
\nocite{CNNklein}
\nocite{CNNgross}																					%Ende Einbinden der Werke

\newcommand{\practitioner}[1]{(vgl. Gibson \& Patterson, 2017, S. {#1})}
\newcommand{\fundamentals}[1]{(vgl. Buduma, 2017, S. {#1})}
\newcommand{\cnnKlein}[1]{(vgl. Nash \& O'Shea, 2015, S. {#1})}
\newcommand{\ebd}[1]{(vgl. ebd., S. {#1})}
	
	
	
%Anfang für Formatierung von Code
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}


\lstset{frame=tb, %https://stackoverflow.com/questions/3175105/writing-code-in-latex-document
	language=Python,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\fontsize{12}{14}\ttfamily},
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{black}\bfseries,
	commentstyle=\itshape\color{gray},
	stringstyle=\itshape,
	breaklines=true,
	tabsize=3,
	%linebackgroundcolor={\ifodd\value{lstnumber}\color{green!5}\else\color{white}\fi} %Für alternierende Zeilenfarben
	%linebackgroundcolor=\oddtest,
	frame=nonuple,			%https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
	keepspaces=false,			%https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
	numbers=left,			%https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
	rulecolor=\color{black},  %https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
	morekeywords={self},		%https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings und https://latex.org/forum/viewtopic.php?t=2320
	breakindent=6em,				%https://tex.stackexchange.com/questions/4239/which-measurement-units-should-one-use-in-latex und https://github.com/olivierverdier/python-latex-highlighting/blob/master/pythonhighlight.sty
}
%Ende für Formatierung von Code	

%Anfang Beispiel für Code
%\begin{lstlisting}
%\end{lstlisting}
%Ende Beispiel für Code


\newglossary[tlg]{Abk}{tld}{tdn}{Abkürzungsverzeichnis} %https://texblog.org/2014/04/01/multiple-glossaries-in-latex/
\makeglossaries

%==============================GLOSSAR=================================
%\newglossaryentry{}{name=, description={}}
\newglossaryentry{Begriff}{name=Begriff, description={Rechnerisches Modell, bestehend aus mehreren künstlichen Neuronen. Siehe auch: Kapitel \ref{DefKNN}}}
\newglossaryentry{Neuronales Netz(werk)}{name=Neuronales Netz(werk), description={Siehe: Künstliches Neuronales Netzwerk}}
\newglossaryentry{Kuenstliches Neuron}{name=Künstliches Neuron, description={Mathematische Funktion mit mehreren Parametern. Siehe auch: Kapitel \ref{DefKN}}}
\newglossaryentry{Neuron}{name=Neuron, description={Siehe: Künstliches Neuron}}
\newglossaryentry{Weight}{name=Weight, description={Parameter zur Gewichtung der Eingabe eines künstlichen Neurones}}
\newglossaryentry{Gewicht}{name=Gewicht, description={Siehe: Weight}}
\newglossaryentry{Layer}{name=Layer, description={Gruppierung von einem oder mehreren Neuronen}}
\newglossaryentry{Schicht}{name=Schicht, description={Siehe: Layer}}
\newglossaryentry{Ausgabewerte}{name=Ausgabewerte, description={Wert eines Neurons, welchen es an Neurone der nächsten Schicht weitergibt}}
\newglossaryentry{Bias}{name=Bias, description={Parameter welcher zu den gewichteten Eingaben eines Neurons addiert wird}}
\newglossaryentry{Aktivierungsfunktion}{name=Aktivierungsfunktion, description={Funktion, in welche die gewichteten Eingaben samt Bias eingegeben werden und den Ausgabewert des Neurons berechnet}}
\newglossaryentry{Aktivierung}{name=Aktivierung, description={Zustand eines Neurons; gilt als aktiviert wenn sein Ausgabewert $\neq 0$}}
\newglossaryentry{Regression}{name=Regression, description={Aufgabenstellungen bei denen der Zusammenhang zwischen Eingabe und Ausgabe und modelliert werden muss um für eine gegebene Eingabe die Ausgabe zu ermitteln}}
\newglossaryentry{Binary Classification}{name=Binary Classification, description={Aufgabenstellungen bei denen entschieden werden muss, ob die Eingabe einer bestimmten Struktur entspricht oder nicht. Oft liegt der Ausgabewert zwischen 0 und 1, weshalb ein Schwellenwert für die Ausgabe festgelegt wird um zwischen den beiden möglichen Kategorien zu unterscheiden}}
\newglossaryentry{Multiclass Classification}{name=Multiclass Classification, description={Aufgabenstellungen bei denen die Eingabe in eine von mehreren Kategorien eingeordnet werden muss. Die Klasse dessen entsprechendes Neuron den höchsten Ausgabewert hat ist die Antwort des KNN}}
\newglossaryentry{Feedforward Neural Network}{name=Feedforward Neural Network, description={Kommt mit Kapitel 2}}
\newglossaryentry{Convolutional Neural Network}{name=Convolutional Neural Network, description={Kommt mit Kapitel 3}}
\newglossaryentry{Recurrent Neural Network}{name=Recurrent Neural Network, description={Kommt mit Kapitel 4}}
\newglossaryentry{Deep Learning}{name=Deep Learning, description={BESCHREIBUNG ERFORDERLICH}}
\newglossaryentry{Machine Learning}{name=Machine Learning, description={BESCHREIBUNG ERFORDERLICH}}
\newglossaryentry{Learningrate}{name=Learningrate, description={BESCHREIBUNG ERFORDERLICH}}
\newglossaryentry{Lernrate}{name=Lernrate, description={BESCHREIBUNG ERFORDERLICH}}
%======================================================================


%==============================ABKÜRZUNGSVERZEICHNIS===================
%\newglossaryentry{}{type=Abk, name=, description={}}
\newglossaryentry{KNN}{type=Abk, name=KNN, description={Künstliches Neuronales Netz(werk)}}
\newglossaryentry{FFNN}{type=Abk, name=FFNN, description={Feedforward Neural Network}}
\newglossaryentry{CNN}{type=Abk, name=CNN, description={Convolutional Neural Network}}
\newglossaryentry{RNN}{type=Abk, name=RNN, description={Recurrent Neural Network}}
\newglossaryentry{MNIST}{type=Abk, name=MNIST, description={Modified National Institute of Standards and Technology}}
%\newglossaryentry{part. Ablt.}{type=Abk, name=part. Ablt., description={partielle Ableitung}}
%======================================================================

\glsaddall 





\interfootnotelinepenalty=10000
%\clubpenalty=9996
%\widowpenalty=9999
%\brokenpenalty=4991
%\predisplaypenalty=10000
%\postdisplaypenalty=1549
%\displaywidowpenalty=1602




\raggedbottom																								%Einstellung, wie das Ende einer Seite ausschauen soll




	
	
																																			
\patchcmd{\abstract}{\null\vfil}{}{}{}																		%Verschieben des Abstracts in der Höhe
																			
\counterwithout{figure}{chapter} 																			%Abbildungen werden nicht mehr per Kapitel, sonder global nummeriert 
\counterwithout{equation}{chapter}																			%Equations werden global nummeriert

%\renewcommand{\mkbibnamefamily}[1]{\ifcitation{#1}{#1}}

\begin{document}
	
	%\linenumbers %Für Zeilennummerierung zur Korrektur

	\begin{titlepage}\label{Titleseite}
		
		\vspace*{80mm}\Huge\centering\textbf{Künstliche Neuronale Netzwerke \newline und ihr Verhalten beim MNIST-Datensatz\break}
		\vspace{0mm}\hrulefill
		\setstretch{1}\vspace{7mm}\Large{\break Verfasser: Tobias Prisching, 8C 2018/19 \break Betreuer: Mag. Christoph Hödl}
		\vspace{15mm}\Large{\break BRG/BORG St. Pölten \break Schulring 16, 3100 St. Pölten}
		\vspace{70mm}\Large{\break Abgabedatum: Noch nicht abgegeben.}
		
	\end{titlepage}
	
	%\begin{flushleft}
	
	\renewcommand{\abstractname}{Abstract}	
	\chapter*{Abstract}\label{Abstract}
		Dieses Abstract ist (noch) leer.\thispagestyle{empty}
	
%	\tableofcontents
%	\clearpage
%	\pagestyle{plain}

	\begingroup
		\renewcommand*{\chapterpagestyle}{empty}
		\pagestyle{empty}
		\tableofcontents
		\clearpage
	\endgroup

	\chapter*{Vorwort}\label{Vorwort}
		\addcontentsline{toc}{chapter}{Vorwort}
		Dieses Vorwort ist (noch) leer.
	
	\chapter*{Einleitung}\label{Einleitung}
		\addcontentsline{toc}{chapter}{Einleitung}
		Diese Einleitung ist (noch) leer.



	\chapter{Bausteine und Grundlegendes zu Künstlichen Neuronalen Netzwerken}\label{1}
		\section{Grundlegendes zur Verwendung von Fachbegriffen und mathematischen \mbox{Ausdrücken in dieser Arbeit}}\label{Fachbegriffe&Mathe}
		Damit man über die verschiedenen Konzepte in dieser Arbeit schreiben kann, benötigt man Fachbegriffe und teilweise auch Variablen, Parameter, Terme und Gleichungen. Wie in anderen wissenschaftlichen Gebieten auch gibt es im Bereich der Künstlichen Neuronalen Netze keine allgemeine Schreibweise. 
			\subsection{Fachbegriffe}\label{Fachbegriffe}
			Es wäre logisch, für eine in Deutsch verfasste VWA deutsche Fachbegriffe zu verwenden. Allerdings ist der Großteil der Literatur, welche für diese Arbeit verwendet wurde, in englischer Sprache verfasst, weshalb mehrere Begriffe auch nur in dieser vorliegen. Um mögliche Übersetzungsfehler und Differenzen zu anderen deutschen Werken zu verhindern, werden in dieser Arbeit hauptsächlich die englischen Fachbegriffe eingedeutscht, wobei diese dabei auch großgeschrieben werden. Dies hat zusätzlich die Vorteile, dass einerseits der/die LeserIn sich in weiterführender Literatur besser zurecht findet, und andererseits, dass die Herleitung der mathematischen Variablenbezeichnungen offensichtlich ist. \newline\\ Falls jedoch auch andere, deutsche Bezeichnungen vorkommen, werden diese bei Erstnennung des Begriffes ebenfalls erwähnt. Zusätzlich sind alle in dieser Arbeit verwendeten Formen im Glossar zu finden. Gleiches gilt auch für Abkürzungen, nur sind diese im Abkürzungsverzeichnis zu finden. 
			
			\subsection{Mathematische Ausdrücke}\label{Mathe}
			Ebenfalls nicht einheitlich sind mathematische Ausdrücke in der Literatur. Häufig werden unterschiedliche Buchstaben, Nummerierungen und Indexierungen für die Variablen und Parameter verwendet. In dieser Arbeit wird versucht, eine eigene Schreibweise zu verwenden, welche möglichst einfach zu verstehen ist, möglicherweise aber nicht die eleganteste oder kürzeste Ausdrucksweise ist. Sämtliche Variablen und Parameter sind im \mbox{Anhang A} in einer Notationstabelle aufgelistet. 
			
		\section{Definition eines Künstlichen Neuronalen Netzes}\label{DefKNN}
		Um den Inhalt in den folgenden Kapiteln zu verstehen, ist eine Definition von Künstlichen Neuronalen Netzwerken, kurz \gls{KNN} oder auch nur Neuronales Netz(werk), notwendig, da andere Netztypen auf dieser Definition aufbauen. Ein KNN ist ein rechnerisches Modell welches ein Netz bestehend aus miteinander verbundenen Knoten, auch künstliche Neurone genannt, dessen Aufbau lose an dem von biologischen Gehirnen orientiert ist, modelliert. Diese Neuronen können miteinander Signale über Verbindungen, den Weights oder Gewichten, welche die Strukturen aus den Informationen lernen, austauschen und sind in verschiedene Schichten, auch Layer genannt, eingeteilt. Die Weights werden in einem Lernprozess, auch als Training bezeichnet, so angepasst, dass das Netz in den ihm eingespeisten Informationen Strukturen erkennen kann. (vgl. Gurney, 1997, S. 1) Aus mathematischer Sicht lassen sich KNNs als komplexe Funktionen mit nur wenigen bis zu Millionen oder Milliarden Parametern aufschreiben. Die einzige Grenze für die Komplexität und Größe dieser Funktion ist die verfügbare Rechenkapazität. 
		\section{Das künstliche Neuron}\label{DefKN}
		Das künstliche Neuron ist der Grundbaustein für alle in dieser Arbeit behandelten Arten von KNNs. Ein Neuron $n$ der Schicht $l$ lässt sich am einfachsten als eine mathematische Funktion beschreiben. Es nimmt die Ausgabewerte $x_{(l-1,1)}$, $x_{(l-1,2)}$, …, $x_{(l-1,m)}$ der Neurone der vorherigen Schicht $l-1$, multipliziert diese Werte mit Weights (auch Gewichte genannt) $w_{(l-1,1),(l,n)}$, $w_{(l-1,2),(l,n)}$, …, $w_{(l-1,m),(l,n)}$, summiert diese auf und addiert einen weiteren Parameter, genannt Bias, $b_{(l,n)}$. Diese Summe wird einer Aktivierungsfunktion $f_{(l)}$ übergeben, deren Wert der endgültige Ausgabewert dieses Neurons ist und an Neurone der Schicht $l+1$ weitergegeben werden kann. Mathematisch lässt sich dies wie folgt in Formel \ref{FormelNeuron} ausdrücken. (vgl. Buduma, 2017, S. 8)
		\vspace*{-0.5cm}
			\begin{equation}\label{FormelNeuron}
				x_{(l,n)} = f_{(l)} \left(\sum_{i=1}^{m}(w_{(l-1,i),(l,n)} \cdot x_{(l-1,i)}) + b_{(l,n)}) = f_{(l)}(z_{(l,n)}\right)
			\end{equation}

		 Diese Formel lässt sich auch so aufschreiben, dass man gleich einen Vektor mit allen Ausgabewerten aller Neurone der Schicht $l$ erhält. (vgl. ebd., S. 8)	Dabei ist $W_{(l-1),(l)}$ eine Matrix der Form $n\times m$, wobei $m$ die Anzahl der Neurone der Schicht $l-1$ und $n$ die Anzahl der Neurone in Schicht $l$ ist. (vgl. Rashid, 2017, \mbox{S. 45-49})
		 	\begin{equation}\label{FormelNeuronVektor}
			 	\vec{x}_{(l)} = f_{(l)}(W_{(l-1),(l)} \cdot \vec{x}_{(l-1)} + \vec{b}_{(l)})  = f_{(l)}(\vec{z}_{(l)})
		 	\end{equation}

		%Grafik machen, ähnlich Practitioner S. 51 mit Bias. -> Wir schreiben hier kein Kinderbuch -> Antrag ABGELEHNT

			%Meistens skaliert auf Intervall, z.B. [0;1] oder [-1;1]
			
			% Falls =0 -> keine Verbindung zwischen Neuronen (BELEG ERFORDERLICH)
			Das Gewicht einer Eingabe gibt an, wie viel Aussagekraft bzw. wie wichtig der Wert eines Neurons ist. Der Bias eines Neurons lässt sich mit einem Schwellenwert vergleichen, welchen die gewichteten Eingaben überwinden müssen, damit dass Neuron aktiviert wird. (vgl. Nielsen, 2015, Kapitel 1/Perceptrons) Allerdings fehlt der Bias bei Netzen mancher Quellen, so zum Beispiel bei Rashid. Welche Auswirkungen das Nutzen eines Bias bzw. sein Fehlen auf die Leistung eines Netzes hat wird in \mbox{Kapitel \ref{Experimente}} untersucht.
			\ \\
			\ \\
			Neurone wie beim menschlichen Gehirn haben keine lineare Funktion für ihren Ausgabewert. Erst wenn ein bestimmter Schwellenwert erreicht ist, geben sie ein Ausgabesignal aus, sie feuern. Dieses Konzept der Nichtlinearität wird bei künstlichen Neuronen übernommen. (vgl. Rashid, 2017, S. 32) Diese Nichtlinearität ist wichtig, da diese es einem KNN erst ermöglicht, komplexere Aufgaben zu lösen. (vgl. Buduma, 2017, S. 13) Erreicht wird sie durch eine Aktivierungsfunktion, welche die Aktivierung eines Neurons steuert. Ein Neuron gilt dann als aktiviert, wenn sein Ausgabewert ungleich Null ist. (vgl. Gibson \& Patterson, 2017, S. 53) Es gibt verschiedene Aktivierungsfunktionen, welche je nach Aufgabe des KNNs bzw. des Layers im KNN eingesetzt werden. (vgl. ebd., S. 255f) Innerhalb eines KNNs können die Layer unterschiedliche Aktivierungsfunktonen verwenden. (vgl. ebd., S. 50) Die Graphen von vier Funktionen sind in den Abbildungen \ref{SigmoidGraph} - \ref{LeakyReLUGraph} dargestellt, die Sigmoid-Funktion, die TanH-Funktion, die Rectified Linear-Funktion\footnote{Neurone mit dieser Aktivierungsfunktion werden auch ReLUs (Rectiefied Linear Units) genannt \practitioner{70}} und die Leaky Rectiefied Linear-Funktion. (vgl. Buduma, 2017, S. 13ff) Die Sigmoid-Funktion ist in der Literatur eine der am häufigsten anzutreffenden Aktivierungsfunktionen, jedoch werden aufgrund von eines Nachteils dieser Aktivierungsfunktion, dem Vanishing Gradient (Siehe \ref{VanishingGradient}), andere Funktionen verwendet. Auch die TanH-Funktion besitzt diesen Nachteil, die Rectiefied Linear-Funktion hat das Problem des "`Dying ReLU"' (Siehe \ref{DyingReLU}), weshalb die Leaky Rectified Linear-Funktion oft bevorzugt wird.\footnote{Es gibt genaue, mathematische Begründungen, warum eine Aktivierungsfunktion besser ist als die andere, welche allerdings nicht zielführend zur Beantwortung der Forschungsfragen sind. Dieser Punkt wird daher auch nur kurz in Kapitel \ref{Lernen} angeschnitten, da es noch weitere Parameter gibt, welche den Erfolg eines KNN ausmachen.} (vgl. Gibson \& Patterson, 2017, S. 254) Welche Auswirkungen die Nutzung einer anderen Aktivierungsfunktion hat, wird in Experiment Nr. X untersucht. 
			

			 
			%Verschiedene Arten von Aktivierungsfunktionen -> ERLEDIGT
			%Notiz 3, ERLEDIGT
			%Innerhalb eines KNNs können verschiedene Aktivierungsfunktionen verwendet werden (BELEG ERFORDERLICH)	
			%Graphen von Heaviside, Sigmoid, TanH, ReLU	ERLEDIGT
			% Zu Sigmoid -> Probleme mit vanishing Gradient	ERLEDIGT (WIRD IN KAPITEL ZU BACKPROP ERKLÄRT)
			% Es gibt sher viel zu diesem Thema zu sagenm alleine was den Vergleich von Aktivierungsfunktionen angeht, wie LeCun in Efficient Backpropagation zeigt. Da diese Vergleiche nicht zeilführend für Forschungsfragen sind, werden diese auch (forerst) ausgelassen. 
	

		
		\begin{figure}[htb] %Vorlage: Practitioner S. 65-70
			\centering
			\begin{minipage}[t]{.48\linewidth}
				\centering
					\begin{tikzpicture}[scale=0.8]
					\begin{axis}[
					%title={$f(x) = \frac{1}{1 + \mathrm{e}^{-x}}$},
					xtick={-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 
					xticklabels={-10,,,,,-5,,,,,0,,,,,5,,,,,10},
					ytick={-1, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}, 
					yticklabels={-1.0,,,,,-0.5,,,,,0,,,,,0.5,,,,,1.0},
					x=10,
					y=100,
					ymin=-1.05, 	ymax=1.05, 
					xmin=-10.5, 	xmax=10.5, 
					axis lines=center, 
					hide obscured x ticks=false, 
					xlabel=$x$,
					ylabel={$f(x) = \frac{1}{1 + \mathrm{e}^{-x}}$},
					every inner x axis line/.append style={-},
					every inner y axis line/.append style={-},
					every axis x label/.style={at={(ticklabel* cs:0.99)},anchor=west,},
					every axis y label/.style={at={(ticklabel* cs:0.99)},anchor=south,},
					] 
					\addplot[domain=-10:10, samples=100, color=red]{1 / (1+exp(-x))}; 
					\end{axis};
					\end{tikzpicture}	
					\caption{Sigmoid-Funktion \\ (Quelle: Eigene Darstellung)}\label{SigmoidGraph}
			\end{minipage}%
			\hfill%
			\begin{minipage}[t]{.48\linewidth}
				\centering
					\begin{tikzpicture}[scale=0.80]
					\begin{axis}[
					%title={$f(x) = \frac{1}{1 + \mathrm{e}^{-x}}$},
					xtick={-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 
					xticklabels={-10,,,,,-5,,,,,0,,,,,5,,,,,10},
					ytick={-1, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}, 
					yticklabels={-1.0,,,,,-0.5,,,,,0,,,,,0.5,,,,,1.0}, 
					x=10,
					y=100,
					ymin=-1.05, 	ymax=1.05, 
					xmin=-10.5, 	xmax=10.5, 
					axis lines=center, 
					hide obscured x ticks=false, 
					xlabel=$x$,
					ylabel={$f(x) = \frac{1}{1 + \mathrm{e}^{-x}}$},
					every inner x axis line/.append style={-},
					every inner y axis line/.append style={-},
					every axis x label/.style={at={(ticklabel* cs:0.99)},anchor=west,},
					every axis y label/.style={at={(ticklabel* cs:0.99)},anchor=south,},
					] 
					\addplot[domain=-10:10, samples=100, color=red]{tanh(x)}; 
					\end{axis}
					\end{tikzpicture}	
					\caption{TanH-Funktion \\ (Quelle: Eigene Darstellung)}\label{TanHGraph}
			\end{minipage}
			\hfill%
			\vspace*{0.7cm}
			\begin{minipage}[t]{.48\linewidth}
				\centering
					\begin{tikzpicture}[scale=0.80]
					\begin{axis}[
					%title={$f(x) = $ max$(0,x)$},
					xtick={-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 
					xticklabels={-10,,,,,-5,,,,,0,,,,,5,,,,,10},
					ytick={-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 
					yticklabels={-10,,,,,-5,,,,,0,,,,,5,,,,,10}, 
					x=10,
					y=20, 
					ymin=-5.5, 	ymax=5.5, 
					xmin=-10.5, 	xmax=10.5, 
					axis lines=center, 
					hide obscured x ticks=false, 
					xlabel=$x$,
					ylabel={$f(x) = $ max$(0,x)$},
					every inner x axis line/.append style={-},
					every inner y axis line/.append style={-},
					every axis x label/.style={at={(ticklabel* cs:0.99)},anchor=west,},
					every axis y label/.style={at={(ticklabel* cs:0.99)},anchor=south,},
					] 
					\addplot[domain=-10:10, samples=1000, color=red]{max(0,x)}; 
					\end{axis}
					\end{tikzpicture}	
					\caption{Rectified Linear \\ (Quelle: Eigene Darstellung)}\label{ReLUGraph}
			\end{minipage}
			\hfill%
			\begin{minipage}[t]{.48\linewidth}
				\centering
				\begin{tikzpicture}[scale=0.80]
				\begin{axis}[
				%title={$f(x) = \left\{\begin{array}{rcl}
				%							0,01 \cdot x &x\leq0\\
				%							x&x>0
				%							\end{array}\right.$},
				xtick={-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 
				xticklabels={-10,,,,,-5,,,,,0,,,,,5,,,,,10},
				ytick={-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 
				yticklabels={-10,,,,,-5,,,,,0,,,,,5,,,,,10}, 
				x=10,
				y=20, 
				ymin=-5.5, 	ymax=5.5, 
				xmin=-10.5, 	xmax=10.5, 
				axis lines=center, 
				hide obscured x ticks=false, 
				xlabel=$x$,
				ylabel={$f(x) = \left\{\begin{array}{rcl}
					0,01 \cdot x &x\leq0\\
					x&x>0
					\end{array}\right.$},
				every inner x axis line/.append style={-},
				every inner y axis line/.append style={-},
				every axis x label/.style={at={(ticklabel* cs:0.99)},anchor=west,},
				every axis y label/.style={at={(ticklabel* cs:0.99)},anchor=south,},
				] 
				\addplot[domain=-10:10, samples=1000, color=red]{
											(x <= 0) * (0.01*x)   +
											(x > 0) * (x)}; 
				\end{axis}
				\end{tikzpicture}	
				\caption{Leaky Rectified Linear \\ (Quelle: Eigene Darstellung)}\label{LeakyReLUGraph}
			\end{minipage}
		\end{figure}

		
		\section{Schichten}
		Wie bereits in \ref{DefKNN} erwähnt werden die Neuronen eines KNNs meistens in drei verschiedene Arten von Schichten gruppiert. Dabei sind die Werte der vorherigen Schicht die Eingabe für die nächste Schicht. (vgl. Gibson \& Patterson, 2017, S. 55f) Es gibt zwar Ausnahmen mit beispielsweise nur einer einzigen Schicht, wie etwa das Single-Layer Perceptron, welche aber für diese Arbeit nicht relevant sind. (vgl. ebd., S. 48) Die in den folgenden Kapiteln behandelten Arten basieren auf folgender Einteilung.
		
		%Erwähnen der Ausnahme eines 1-schichtigen KNN ERLEDIGT 
		%Abbildung tikz11 (Nielsen Kapitel 1) "Aufbau der einzelnen Schichten. Die Kreise symbolisieren Neurone. 
		
		% Notiz 67
		% Practitioner S. 87
		
		%Erwähnung, dass Informationen von einem Layer zum nächsten weitergeleitet werden
		
		\begin{wrapfigure}{}{0cm}	%Vorlage: Nielsen/Kapitel 1/The architecture of neural networks
			\begin{tikzpicture}[scale=0.5, decoration={markings, mark=at position 0.75 with {\arrow{>}}}]
			\draw (0,0) circle (0.5cm);
			\draw (0,1.5) circle (0.5cm);
			\draw (0,3) circle (0.5cm);
			\draw (0,4.5) circle (0.5cm);
			\draw (0,6) circle (0.5cm);
			\draw (0,7.5) circle (0.5cm);
			\draw (0,9) circle (0.5cm);
			
			
			
			\draw (2.5,7.5) circle (0.5cm);
			\draw[postaction={decorate}] (0.5,9) 	-- (2,7.5);
			\draw[postaction={decorate}] (0.5,7.5) -- (2,7.5);
			\draw[postaction={decorate}] (0.5,6) 	-- (2,7.5);
			\draw[postaction={decorate}] (0.5,4.5) -- (2,7.5);
			\draw[postaction={decorate}] (0.5,3) 	-- (2,7.5);
			\draw[postaction={decorate}] (0.5,1.5) -- (2,7.5);
			\draw[postaction={decorate}] (0.5,0) 	-- (2,7.5);
			
			\draw (2.5,6) circle (0.5cm);
			\draw[postaction={decorate}] (0.5,9) 	-- (2,6);
			\draw[postaction={decorate}] (0.5,7.5) -- (2,6);
			\draw[postaction={decorate}] (0.5,6) 	-- (2,6);
			\draw[postaction={decorate}] (0.5,4.5) -- (2,6);
			\draw[postaction={decorate}] (0.5,3) 	-- (2,6);
			\draw[postaction={decorate}] (0.5,1.5) -- (2,6);
			\draw[postaction={decorate}] (0.5,0) 	-- (2,6);
			
			\draw (2.5,4.5) circle (0.5cm);
			\draw[postaction={decorate}] (0.5,9) 	-- (2,4.5);
			\draw[postaction={decorate}] (0.5,7.5) -- (2,4.5);
			\draw[postaction={decorate}] (0.5,6) 	-- (2,4.5);
			\draw[postaction={decorate}] (0.5,4.5) -- (2,4.5);
			\draw[postaction={decorate}] (0.5,3) 	-- (2,4.5);
			\draw[postaction={decorate}] (0.5,1.5) -- (2,4.5);
			\draw[postaction={decorate}] (0.5,0)	-- (2,4.5);
			
			\draw (2.5,3) circle (0.5cm);
			\draw[postaction={decorate}] (0.5,9) 	-- (2,3);
			\draw[postaction={decorate}] (0.5,7.5) -- (2,3);
			\draw[postaction={decorate}] (0.5,6) 	-- (2,3);
			\draw[postaction={decorate}] (0.5,4.5) -- (2,3);
			\draw[postaction={decorate}] (0.5,3) 	-- (2,3);
			\draw[postaction={decorate}] (0.5,1.5) -- (2,3);
			\draw[postaction={decorate}] (0.5,0) 	-- (2,3);
			
			\draw (2.5,1.5) circle (0.5cm);
			\draw[postaction={decorate}] (0.5,9) 	-- (2,1.5);
			\draw[postaction={decorate}] (0.5,7.5) -- (2,1.5);
			\draw[postaction={decorate}] (0.5,6) 	-- (2,1.5);
			\draw[postaction={decorate}] (0.5,4.5) -- (2,1.5);
			\draw[postaction={decorate}] (0.5,3) 	-- (2,1.5);
			\draw[postaction={decorate}] (0.5,1.5) -- (2,1.5);
			\draw[postaction={decorate}] (0.5,0)	-- (2,1.5);
			
			
			
			\draw (5,7.5) circle (0.5cm);
			\draw[postaction={decorate}] (3,7.5) 	-- (4.5,7.5);
			\draw[postaction={decorate}] (3,6)		-- (4.5,7.5);
			\draw[postaction={decorate}] (3,4.5) -- (4.5,7.5);
			\draw[postaction={decorate}] (3,3) 	-- (4.5,7.5);
			\draw[postaction={decorate}] (3,1.5) -- (4.5,7.5);
			
			\draw (5,6) circle (0.5cm);
			\draw[postaction={decorate}] (3,7.5) 	-- (4.5,6);
			\draw[postaction={decorate}] (3,6)		-- (4.5,6);
			\draw[postaction={decorate}] (3,4.5) -- (4.5,6);
			\draw[postaction={decorate}] (3,3) 	-- (4.5,6);
			\draw[postaction={decorate}] (3,1.5) -- (4.5,6);
			
			\draw (5,4.5) circle (0.5cm);
			\draw[postaction={decorate}] (3,7.5) 	-- (4.5,4.5);
			\draw[postaction={decorate}] (3,6)		-- (4.5,4.5);
			\draw[postaction={decorate}] (3,4.5) -- (4.5,4.5);
			\draw[postaction={decorate}] (3,3) 	-- (4.5,4.5);
			\draw[postaction={decorate}] (3,1.5) -- (4.5,4.5);
			
			\draw (5,3) circle (0.5cm);
			\draw[postaction={decorate}] (3,7.5) 	-- (4.5,3);
			\draw[postaction={decorate}] (3,6)		-- (4.5,3);
			\draw[postaction={decorate}] (3,4.5) -- (4.5,3);
			\draw[postaction={decorate}] (3,3) 	-- (4.5,3);
			\draw[postaction={decorate}] (3,1.5) -- (4.5,3);
			
			\draw (5,1.5) circle (0.5cm);
			\draw[postaction={decorate}] (3,7.5) 	-- (4.5,1.5);
			\draw[postaction={decorate}] (3,6)		-- (4.5,1.5);
			\draw[postaction={decorate}] (3,4.5) -- (4.5,1.5);
			\draw[postaction={decorate}] (3,3) 	-- (4.5,1.5);
			\draw[postaction={decorate}] (3,1.5) -- (4.5,1.5);
			
			
			\draw (7.5,7) circle (0.5cm);
			\draw[postaction={decorate}] (5.5,7.5) -- (7,7);
			\draw[postaction={decorate}] (5.5,6)	-- (7,7);
			\draw[postaction={decorate}] (5.5,4.5) -- (7,7);
			\draw[postaction={decorate}] (5.5,3) 	-- (7,7);
			\draw[postaction={decorate}] (5.5,1.5) -- (7,7);
			
			\draw (7.5,5.5) circle (0.5cm);
			\draw[postaction={decorate}] (5.5,7.5) -- (7,5.5);
			\draw[postaction={decorate}] (5.5,6)	-- (7,5.5);
			\draw[postaction={decorate}] (5.5,4.5) -- (7,5.5);
			\draw[postaction={decorate}] (5.5,3) 	-- (7,5.5);
			\draw[postaction={decorate}] (5.5,1.5) -- (7,5.5);
			
			\draw (7.5,4) circle (0.5cm);
			\draw[postaction={decorate}] (5.5,7.5) -- (7,4);
			\draw[postaction={decorate}] (5.5,6)	-- (7,4);
			\draw[postaction={decorate}] (5.5,4.5) -- (7,4);
			\draw[postaction={decorate}] (5.5,3) 	-- (7,4);
			\draw[postaction={decorate}] (5.5,1.5) -- (7,4);
			
			\draw (7.5,2.5) circle (0.5cm);
			\draw[postaction={decorate}] (5.5,7.5) -- (7,2.5);
			\draw[postaction={decorate}] (5.5,6)	-- (7,2.5);
			\draw[postaction={decorate}] (5.5,4.5) -- (7,2.5);
			\draw[postaction={decorate}] (5.5,3) 	-- (7,2.5);
			\draw[postaction={decorate}] (5.5,1.5) -- (7,2.5);
			
			
			\draw (10,6) circle (0.5cm);
			\draw[postaction={decorate}] (8,7) 	-- (9.5,6);
			\draw[postaction={decorate}] (8,5.5)	-- (9.5,6);
			\draw[postaction={decorate}] (8,4) 	-- (9.5,6);
			\draw[postaction={decorate}] (8,2.5) 	-- (9.5,6);
			
			\draw (10,4.5) circle (0.5cm);
			\draw[postaction={decorate}] (8,7) 	-- (9.5,4.5);
			\draw[postaction={decorate}] (8,5.5)	-- (9.5,4.5);
			\draw[postaction={decorate}] (8,4) 	-- (9.5,4.5);
			\draw[postaction={decorate}] (8,2.5) 	-- (9.5,4.5);
			
			\draw (10,3) circle (0.5cm);
			\draw[postaction={decorate}] (8,7) 	-- (9.5,3);
			\draw[postaction={decorate}] (8,5.5)	-- (9.5,3);
			\draw[postaction={decorate}] (8,4) 	-- (9.5,3);
			\draw[postaction={decorate}] (8,2.5) 	-- (9.5,3);
			
			\draw [decorate,decoration={brace,amplitude=5pt},xshift=0pt,yshift=0pt]
			(-0.5,-0.5) -- (-0.5,9.5) node [black,midway,xshift=-0.4cm,rotate=90] 
			{\footnotesize Input Layer};
			
			\draw [decorate,decoration={brace,amplitude=5pt},xshift=0pt,yshift=0pt]
			(2,8) -- (8,8) node [black,midway,yshift=0.4cm] 
			{\footnotesize Hidden Layers};
			
			\draw [decorate,decoration={brace,amplitude=5pt},xshift=0pt,yshift=0pt]
			(10.5,6.5) -- (10.5,2.5) node [black,midway,xshift=0.4cm,rotate=270] 
			{\footnotesize Output Layer};
			\end{tikzpicture}
			\caption{Ein Beispiel für ein KNN: Der Input Layer hat sieben Neurone, es gibt drei Hidden Layer mit je fünf, fünf und vier Neuronen. Der Output Layer hat drei Neurone. (Quelle: Eigene Darstellung)}
		\end{wrapfigure}
		\break
			\subsection{Input Layer}
			Der Input Layer nimmt die dem KNN übergebenen Daten an und leitet diese an den ersten Hidden Layer weiter. Die Anzahl der Neurone in diesem Layer ist oft gleich der Anzahl der Daten in einem Datensatz. (vgl. Gibson \& Patterson, 2017, S. 55) Werden einem KNN beispielsweise Bilder mit einer Auflösung von 28 mal 28 Pixeln übergeben, besteht der erste Layer aus 784 (28$^2$) Neuronen. Des Weiteren haben die Neurone des Input Layers keine Weights oder Biases und es wird keine Aktivierungsfunktion auf diese angewendet, da sie exakt jene Werte ausgeben sollen, welche dem Netz übergeben wurden. Dies hat keinen genauen Grund und hängt mit der Entwicklungsgeschichte von KNNs zusammen. (vgl. Rashid, 2017, S. 41) 
			% Notiz 5
			
			\subsection{Hidden Layer}
			Jede in dieser Arbeit behandelte Art von KNNs besitzt mindestens einen oder mehr Hidden Layer. Diese Layer sind verantworlich für den Erfolg von KNNs. (vgl. Gibson \& Patterson, 2017, S. 55) Der Name dieser Layer hat keine besondere Bedeutung und bedeutet nur, dass die Ausgabewerte ihrer Neurone nicht die finalen Ausgabewerte des Netzes sind. (vgl. Bengio, Courville \& Goodfellow, 2016, S. 165) Der Aufbau der Hidden Layer ist im Gegensatz zu denen der Input und Output Layer nicht so einfach zu entwickeln. Die Anzahl der Neurone in diesen Layern ist meistens durch die Art von Daten gegeben, zudem gibt es auch nur je einen Layer von beiden. (vgl. Nielsen, 2015, Kapitel 1/The architecture of neural networks)
			
			\subsection{Output Layer}\label{OutputLayer}
			Der Output Layer gibt die endgültige Antwort des KNNs aus, welche, je nach Aufgabe des Netzes (Regression\footnote{Regression modelliert den Zusammenhang zwischen Eingabe und Ausgabe und versucht, für eine gegebene Eingabe die Ausgabe zu ermitteln. (vgl. Gibson \& Patterson, 2017, S. 23)} oder Classification\footnote{Classification kategorisiert die Eingabe in zwei oder mehr Klassen. Bei zwei Klassen spricht man von Binary Classification. In diesem Fall hat der Output Layer ein Neuron, bei dem der Ausgabewert, welcher oft zwischen 0 und 1 liegt, mit einem Schwellenwert aufgeteilt wird. Für den Fall von $x$ Klassen ($x > 2$), Multiclass Classification genannt, gibt es $x$ Neurone. Die Klasse dessen entsprechendes Neuron den höchsten Wert hat ist die Antwort des KNN. (vgl. ebd., 2017, S. 25f)}), eine bestimmte Dimension hat. Abhängig von der in diesem Layer benutzten Aktivierungsfunktion und der Anzahl der Neuronen handelt es sich bei der Ausgabe meistens um entweder einen reellen Wert (Regression) oder einer (Menge von) Wahrscheinlichkeit(en) (Classification).  (vgl. Gibson \& Patterson, 2017, S. 55 \& S. 95) Da der Schwerpunkt dieser Arbeit der MNIST-Datensatz ist (Siehe \ref{MNIST}) und es sich bei diesem um eine Multiclass Classifications-Aufgabe handelt, wird nur auf diese Kategorie von Aufgaben Rücksicht genommen. 
	
%		\section{Aufgaben von KNNs}\label{Aufgaben}
%			\subsection{Regression}
%			\subsection{Binary Classification}
%			\subsection{Multiclass Classification}
%			% Binary/Multiclass Classification vs Regression
%			% Kann ich nicht einfach sagen was MNIST ist
				
		\section{Arten von Künstlichen Neuronalen Netzwerken}\label{Arten}
		In den folgenden Kapiteln werden drei Netzwerktypen behandelt und erklärt. Ausgewählt wurden dafür das Feedforward Neural Network (FFNN), das Convolutional Neural Network (CNN) und das Recurrent Neural Network (RNN). Alle drei Netzwerktopologien gehören zum Supervised Learning (engl. für überwachtes Lernen), einem Teilgebiet des Deep Learning\footnote{Deep Learning ist ein Teilgebiet der Künstlichen Neuronalen Netzen, welche ein Teilgebiet von Machine Learning sind. (vgl. Wartala, 2018, S. 23) Zu Deep Learning zählen alle KNNs mit mehr als einem Hidden Layer. (vgl. Hurwitz \& Kirsch, 2018, S. 31) Unter Machine Learning versteht man das Erkennen von Strukturen in Beispielen von Daten durch Algorithmen. (vgl. Gibson \& Patterson, 2017, S. 2) } bei dem das KNN mithilfe von Trainingsdaten lernt, bei denen Eingabe und Ausgabe gegeben sind. Diese Netze eignen sich vor allem für automatische Klassifizierung. (vgl. Wartala, 2018, S. 23ff) Das FFNN wurde ausgewählt, da es im Vergleich zu anderen Netztypen sehr einfach aufgebaut ist, das CNN, weil es vor allem bei Bilderkennung sehr erfolgreich ist, und das RNN, weil es durch die Rückkopplung von Daten interessant ist. (vgl. ebd., S. 26 \& S. 29)
		% In folgenden Kapiteln 3 verschiedene Arten erläutert; Leser soll wissen, dass es natürlich noch viel mehr gibt und hier nur Ausblick
		% Begründung warum gerade diese 3 Arten (MLP weil einfach, CNN wegen erfolge und RNN weil...?)
		% KEINE genau detaillierte Beschreibung des Stammbaums von KNNS
		% Alle Arten -> Supervised Learning
		% MLP weil (relativ) einfach
		% CNN weil tolle Erfolge in Bilderkennung
		% RNN/LSTM weil interessant bezüglich des Zeit Begriffs
		
		\section{Der MNIST-Datensatz}\label{MNIST}
		Der MNIST Datensatz ist eine \textbf{m}odifizierte Version zweier Datensätze des \textbf{N}ational \textbf{I}nstitute of \textbf{S}tandards and \textbf{T}echnology der USA. Der Datensatz besteht aus zwei Teilen: Der erste Teil dient zum Trainieren des KNNs und besteht aus 60.000 Ziffern, welche von einer Gruppe, bestehend aus 250 Personen, handgeschrieben wurden. Diese Gruppe setzt sich zusammen aus 125 Mitarbeitern des US Census Bureau und 125 High School Schülern. Der zweite Teil besteht aus 10.000 Ziffern, welche von einer zweiten Gruppe (Größe und Zusammensetzung gleich der ersten Gruppe) geschrieben wurden, um das KNN auf Daten zu testen, die es davor noch nie gesehen hat. Die handgeschriebenen Ziffern wurden mit einer Auflösung von 28 mal 28 Pixel digitalisiert und in CSV-Dateien konvertiert. Die CSV-Dateien bestehen aus 785 Spalten, die Erste gibt an, um welche Ziffer es sich handelt, die 784 anderen geben die Helligkeitswerte der einzelnen Pixel Spalte für Spalte an. (vgl. Nielsen, 2015, Kapitel 1/Learning with gradient descent)



	\chapter{Netztypus 1: Feedforward Neural Networks}
	Das Feedforward Neural Network (auch Multilayer Perceptron Network\footnote{Diese Bezeichnung ist irreführend, da MLPs meistens nicht aus den im Namen stehenden Perceptrons, welche die Heaviside-Funktion als Aktivierungsfunktion verwenden, bestehen. (vgl. Nielsen, 2015, Kapitel 1/The architecture of neural networks)} oder Multilayer Feed-Forward Network genannt) ist trotz seiner Einfachheit ein schon relativ leistungsstarkes Netzwerk, da mit diesem jede stetige Funktion approximiert werden kann. (vgl. Wartala, 2018, S. 17; Gibson \& Patterson, 2017, S. 50)
		\section{Aufbau}
		Ein FFNN besteht aus einem Input- und einem Output-Layer und einem oder mehreren Hidden-Layern. \practitioner{50} Diese Layer sind bei diesem Netztypus \textit{fully connected} (engl. für komplett verbunden), was bedeutet, dass jedes Neuron einer Schicht mit jedem Neuron der benachbarten Schichten verbunden ist. \practitioner{54} Dies kann insofern ein Nachteil sein, da bei beispielsweise Bilderkennung mit steigender Auflösung die Anzahl Neuronen allein in der 1. Schicht quadratisch zunimmt. \fundamentals{89} Jeder Layer besteht aus einem oder mehreren Neuronen, wobei es nicht empfehlenswert ist, aufeinanderfolgende Layer mit gleicher Anzahl an Neuronen zu verwenden. (vgl. Gibson \& Patterson, 2017, S. 50; Buduma, 2017, S. 11) Wie sich die Anzahl der Neurone und Schichten auswirkt, wird in Experiment Nr. X untersucht. \\
		
		Des Weiteren ist der Begriff Feedforward in der Bezeichnung wichtig. Er bedeutet, dass Information nur in eine Richtung, nämlich vom Input- zum Output-Layer fließt. Dabei gibt es keine Verbindungen zwischen Neuronen im selben Layer oder in eine vorherige Schicht, da sich sonst Schleifen bilden, sodass der Input in ein Neuron von seinem Output abhängt. Solche Schleifen kommen bei RNNs in Kapitel \ref{KapitelRNN} vor. (vgl. Buduma, 2017, S. 11; Nielsen, 2015, Kapitel 1/The architecture of neural networks)
		
		
%	 Erwähnung von Softmax -> Auf CNN verschoben zu den ganzen anderen Arten von Layern. 																										Aufbau										
%	 XOR Problem in Verbindung mit "jede stetige Funktion approximieren" 																														"Einleitung"								NICHT MEHR NÖTIG
%	 Def von Feedforward: Fundamentals, S. 11																																					Definition/Aufbau							ERLEDIGT
%	 Anzahl der Neurone sollte in Layern unterschiedlich sein: Fundamentals, S. 11																												Aufbau										ERLEDIGT
%	 Problem von FFNNs bei größeren bildern -> Anzahl der Neurone in nimmt exponential zu: Fundamentals, S. 89																					"Einleitung"/Weiterführend (Kapitel 2.0)	ERLEDIGT
%	 Mit FFNNs kann jede stetige Funktion approximiert werden: Wartala, S. 17	(Gegenteil zu Single Layer Perceptron)																			Einleitung									ERLEDIGT
%	 Fully connected, siehe Practitioner, S. 54																																					Aufbau										ERLEDIGT
%	 Hinweis, dass verschiedene Anzahl von Layern und Neurone Teil der Experimente ist																											Weiterführend								ERLEDIGT
%	 Aufbau der Schichten eines MLP so wie in 1.4 beschrieben, wobei alle Layer fully connected sind; meistens Softmax für letzten Layer; 2 oder mehr Hidden layer -> deep learning				Aufbau										
%	 Weiterführend zu Kapitel 1 ließe sich eine Funktion aufschreiben, welche den Output-Layer berechnet. -> Vielleicht auch noch was zum Input-Layer $x_(0)$									Funktionsweise								ERLEDIGT
%	 Erwähnung, dass in späteren kapitel erklärt wird, wie ein KNN am Beispiel von diesem Netztyp trainiert wird. 																				Weiterführend 								ERLEDIGT	
%	 Beweis, dass mehrere Aktivierungsfunktionen in einem KNN vorkommen können siehe Practitoner S. 50																							-> KAPITEL 1								ERLEDIGT
		\section{Funktionsweise}
		Bisher gab es nur eine Formel für die Berechnung der Werte eines Layers in Abhängigkeit der Ausgabewerte der vorherigen Schicht. Diese lässt sich durch Einsetzen in sich selbst zu Formel \ref{FormelGesamtesNetz} erweitern, sodass der Vektor des Output-Layers in Abhängigkeit des Input-Layers berechnet werden kann. 
			\begin{equation}\label{FormelGesamtesNetz}
				\vec{x}_{(L)} = f_{(L)} \left( W_{(L-1),(L)} \cdot f_{(L-1)} \left(…\cdot f_{(2)} \left(W_{(1),(2)}\cdot\vec{x}_{(1)}+\vec{b}_{(1)}\right)+…\right)+\vec{b}_{(L)}\right)
			\end{equation}
		Dies erklärt jedoch nicht, wie die einzelnen Parameter so angepasst werden, dass $\vec{x}_{(L)}$ ein brauchbares Ergebnis liefert. Dieser Vorgang wird Training oder auch Lernen genannt und in Kapitel \ref{Lernen} beschrieben. \fundamentals{17; ebd., S. 5}
	
	\chapter{Netztypus 2: Convolutional Neural Networks}
	KNNs, wie das FFNN, sind für für Bildererkennung ungeeignet, da sich mit der Verdopplung der Auflösung entlang der Kanten die Anzahl der benötigten Weights vervierfacht. Handelt es sich dabei noch zusätzlich um Farbfotos (und nicht wie bei MNIST um Schwarz-Weiß-Bilder), verdreifacht sich nochmal die Anzahl der Gewichte.\footnote{Aufgrund der geringen Auflösung und fehlender Farbe sind FFNNs trotzdem gut für MNIST und ein Vergleich mit anderen Netzwerktypen geeignet. \cnnKlein{3}} \fundamentals{89} Je mehr Parameter ein KNN hat, umso mehr Rechenleistung benötigt es. (BELEG ERFORDERLICH) Deshalb gibt es CNNs, welche auf die Klassifizierung von Bildern spezialisiert sind (und sich daher für MNIST sehr gut eignen). (vgl. Nielsen, 2018, Kapitel 6/Introducing convolutional networks)
	
	% Fundamentals S. 89												ERLEDIGT
	% Introduction to CNNs (kurz), Seite 3, Absatz vor Overfitting		ERLEDIGT
	
		\section{Aufbau}
		Der Aufbau von CNNs basiert auf den allgemeinen aus Kapitel \ref{1}, verändert allerdings auch manche Eigenschaften. Zwei der größten Unterschiede zu KNNs allgemein sind, dass 1. die Neurone mancher Layer in drei Dimensionen angeordnet sind, und 2. nicht jedes Neuron mit allen Neuronen der benachbarten Schichten verbunden ist. Des Weiteren werden für unterschiedliche Verbindungen die gleichen Gewichte genutzt. \cnnKlein{4 \& 7}
			% Nicht immer jedes Neuron mit allen Neuronen der benachbarten Schchichten verbunden
			% Tensoren 3. Ordnung (oder gar höher!)
			% "Regeln" zum Strukturieren der Schichten / Wie sollte ein gutes CNN ausschauen?
			
		\section{Arten von Schichten}
		 Ein CNN besteht grundsätzlich aus drei verschiedenen Arten von Schichten: Convolutional Layer, Pooling Layer, und Fully-Connected Layer. Außerdem wird eine häufig als Output Layer verwendete Art von Schicht betrachtet, der Softmax Layer\footnote{Dieser kommt auch in anderen Netztypen vor. \practitioner{55}} \cnnKlein{}
		 
		 	\newpage
			\subsection{Convolutional Layer}
			\begin{wrapfigure}[13]{r}{0cm}	%Vorlage: Nielsen/Kapitel 6/Introducing convolutional networks
				\begin{tikzpicture}[scale=0.5, decoration={markings, mark=at position 1 with {\arrow[scale=2]{>}}}]
				\draw[opacity=0.3] (0,0) circle (0.4cm);
				\draw[opacity=0.3] (0,1) circle (0.4cm);
				\draw (0,2) circle (0.4cm);
				\draw (0,3) circle (0.4cm);
				\draw (0,4) circle (0.4cm);
				
				\draw[opacity=0.3] (1,0) circle (0.4cm);
				\draw[opacity=0.3] (1,1) circle (0.4cm);
				\draw (1,2) circle (0.4cm);
				\draw (1,3) circle (0.4cm);
				\draw (1,4) circle (0.4cm);
				
				\draw[opacity=0.3] (2,0) circle (0.4cm);
				\draw[opacity=0.3] (2,1) circle (0.4cm);
				\draw (2,2) circle (0.4cm);
				\draw (2,3) circle (0.4cm);
				\draw (2,4) circle (0.4cm);
				
				\draw[opacity=0.3] (3,0) circle (0.4cm);
				\draw[opacity=0.3] (3,1) circle (0.4cm);
				\draw[opacity=0.3] (3,2) circle (0.4cm);
				\draw[opacity=0.3] (3,3) circle (0.4cm);
				\draw[opacity=0.3] (3,4) circle (0.4cm);
				
				\draw[opacity=0.3] (4,0) circle (0.4cm);
				\draw[opacity=0.3] (4,1) circle (0.4cm);
				\draw[opacity=0.3] (4,2) circle (0.4cm);
				\draw[opacity=0.3] (4,3) circle (0.4cm);
				\draw[opacity=0.3] (4,4) circle (0.4cm);
				
				
				\draw[opacity=0.3] (7,1) circle (0.4cm);
				\draw[opacity=0.3] (7,2) circle (0.4cm);
				\draw (7,3) circle (0.4cm);
				
				\draw[opacity=0.3] (8,1) circle (0.4cm);
				\draw[opacity=0.3] (8,2) circle (0.4cm);
				\draw[opacity=0.3] (8,3) circle (0.4cm);
				
				\draw[opacity=0.3] (9,1) circle (0.4cm);
				\draw[opacity=0.3] (9,2) circle (0.4cm);
				\draw[opacity=0.3] (9,3) circle (0.4cm);
				
				\draw (0,2) -- (7,3);
				\draw (0,3) -- (7,3);
				\draw (0,4) -- (7,3);
				
				\draw (1,2) -- (7,3);
				\draw[postaction={decorate}] (1,3) -- (7,3);
				\draw (1,4) -- (7,3);
				
				\draw (2,2) -- (7,3);
				\draw (2,3) -- (7,3);
				\draw (2,4) -- (7,3);
				
				\draw (-0.5,1.5) -- (-0.5,4.5) -- (2.5,4.5) -- (2.5,1.5) -- (-0.5,1.5);
				
				\draw [decorate,decoration={brace,amplitude=5pt},xshift=0pt,yshift=0pt] (-0.5,4.5) -- (2.5,4.5) node [black,midway,yshift=0.35cm,] {\footnotesize Local Receptive Field};
				\draw [decorate,decoration={brace,amplitude=5pt},xshift=0pt,yshift=0pt] (-0.5,6) -- (4.5,6) node [black,midway,yshift=0.35cm,] {\footnotesize Input};
				\draw [decorate,decoration={brace,amplitude=5pt},xshift=0pt,yshift=0pt] (6.5,3.5) -- (9.5,3.5) node [black,midway,yshift=0.35cm,] {\footnotesize Convolutional Layer};
				
				\end{tikzpicture}
				\caption{Visualisierung des Convolutional Layers. Es hilft, sich den Input Layer nicht wie bisher als Vektor, sondern als Matrix vorzustellen. (Quelle: Eigene Darstellung)}\label{LRF}
			\end{wrapfigure}
			Das besondere am Convolutional Layer ist, dass bei diesem die Neurone nicht mit allen Neuronen der vorherigen Schicht verbunden sind. Stattdessen ist jedes Neuron mit Neuronen eines Ausschnitts, welcher eine bestimmte Größe hat, der vorherigen Schicht verbunden, dem Local Receptive Field $R_{(l,d,n)}$, kurz LRF (Siehe Abbildung \ref{LRF}). Die Gewichte zwischen den Neuronen des LRFs und dem zugehörigen Neuron des Convolutional Layers befinden sich in einer Matrix, dem Kernel $K_{(l,d)}$. (vgl. Nielsen, 2015, Kapitel 6/Introducing convolutional networks) Es wird das Skalarprodukt des Kernels und des zum Neuron zugehörige LRF gebildet, zu diesem wird noch ein Bias $b_{(l,d)}$ addiert. Auf die Summe wird wieder eine Aktivierungsfunktion $f_{(l)}$ angewandt. \fundamentals{93}
			Dieser Ausschnitt wird über den gesamten Input um einen Wert, den Stride, verschoben. Daraus entsteht eine sogenannte Feature Map, eine zweidimensionale Matrix bestehend aus Neuronen. Zu dieser können noch weitere Feature Maps hinzukommen, wodurch der Convolutional Layer dreidimensional wird. Dabei verwenden alle Neurone innerhalb einer Feature Map den gleichen Kernel und den gleichen Bias. \cnnKlein{7} Formel \ref{ConvolutionFormel} berechnet den Ausgabewert $x_{(l,d,n)}$ des Neurons $n$ in Feature Map $d$ von Layer $l$. (vgl. Nielsen, 2015, Kapitel 6/Introducing convolutional networks)
			
			\begin{equation}\label{ConvolutionFormel}
			x_{(l,d,n)} = f_{(l)} \left( \langle K_{(l,d)},R_{(l,d,n)} \rangle_F + b_{(l,d)} \right)
			\end{equation}
			
			\begin{figure}[htb]
				\centering
				\begin{minipage}[t]{.6\linewidth}
					\centering
					%\begin{wrapfigure}[0]{l}{0cm}	%Vorlage: Nielsen/Kapitel 6/Introducing convolutional networks
					\begin{tikzpicture}[scale=0.45, decoration={markings, mark=at position 1 with {\arrow[scale=2]{>}}}]
					\draw (0,0) circle (0.4cm);
					\draw (0,1) circle (0.4cm);
					\draw (0,2) circle (0.4cm);
					\draw (0,3) circle (0.4cm);
					\draw (0,4) circle (0.4cm);
					
					\draw (1,0) circle (0.4cm);
					\draw (1,1) circle (0.4cm);
					\draw (1,2) circle (0.4cm);
					\draw (1,3) circle (0.4cm);
					\draw (1,4) circle (0.4cm);
					
					%\draw (2,0) circle (0.4cm);
					%\draw (2,1) circle (0.4cm);
					%\draw (2,2) circle (0.4cm);
					\draw[line width=0.5mm] (2,2) circle (0.05cm);
					\draw (2,3) circle (0.4cm);
					\draw (2,4) circle (0.4cm);
					
					%\draw (3,0) circle (0.4cm);
					%\draw (3,1) circle (0.4cm);
					%\draw (3,2) circle (0.4cm);
					\draw[line width=0.5mm] (3,1) circle (0.05cm);
					\draw (3,3) circle (0.4cm);
					\draw (3,4) circle (0.4cm);
					
					%\draw (4,0) circle (0.4cm);
					%\draw (4,1) circle (0.4cm);
					%\draw (4,2) circle (0.4cm);
					\draw[line width=0.5mm] (4,0) circle (0.05cm);
					\draw (4,3) circle (0.4cm);
					\draw (4,4) circle (0.4cm);
					
					
					
					\draw (7,0) circle (0.4cm);
					\draw (7,1) circle (0.4cm);
					\draw (7,2) circle (0.4cm);
					\draw (7,3) circle (0.4cm);
					\draw (7,4) circle (0.4cm);
					
					\draw (8,0) circle (0.4cm);
					\draw (8,1) circle (0.4cm);
					\draw (8,2) circle (0.4cm);
					\draw (8,3) circle (0.4cm);
					\draw (8,4) circle (0.4cm);
					
					%\draw (9,0) circle (0.4cm);
					%\draw (9,1) circle (0.4cm);
					%\draw (9,2) circle (0.4cm);
					\draw[line width=0.5mm] (9,2) circle (0.05cm);
					\draw (9,3) circle (0.4cm);
					\draw (9,4) circle (0.4cm);
					
					%\draw (10,0) circle (0.4cm);
					%\draw (10,1) circle (0.4cm);
					%\draw (10,2) circle (0.4cm);
					\draw[line width=0.5mm] (10,1) circle (0.05cm);
					\draw (10,3) circle (0.4cm);
					\draw (10,4) circle (0.4cm);
					
					%\draw (11,0) circle (0.4cm);
					%\draw (11,1) circle (0.4cm);
					%\draw (11,2) circle (0.4cm);
					\draw[line width=0.5mm] (11,0) circle (0.05cm);
					\draw (11,3) circle (0.4cm);
					\draw (11,4) circle (0.4cm);
					
					
					
					\draw (14,0) circle (0.4cm);
					\draw (14,1) circle (0.4cm);
					\draw (14,2) circle (0.4cm);
					\draw (14,3) circle (0.4cm);
					\draw (14,4) circle (0.4cm);
					
					\draw (15,0) circle (0.4cm);
					\draw (15,1) circle (0.4cm);
					\draw (15,2) circle (0.4cm);
					\draw (15,3) circle (0.4cm);
					\draw (15,4) circle (0.4cm);
					
					%\draw (16,0) circle (0.4cm);
					%\draw (16,1) circle (0.4cm);
					%\draw (16,2) circle (0.4cm);
					\draw[line width=0.5mm] (16,2) circle (0.05cm);
					\draw (16,3) circle (0.4cm);
					\draw (16,4) circle (0.4cm);
					
					%\draw (17,0) circle (0.4cm);
					%\draw (17,1) circle (0.4cm);
					%\draw (17,2) circle (0.4cm);
					\draw[line width=0.5mm] (17,1) circle (0.05cm);
					\draw (17,3) circle (0.4cm);
					\draw (17,4) circle (0.4cm);
					
					%\draw (18,0) circle (0.4cm);
					%\draw (18,1) circle (0.4cm);
					%\draw (18,2) circle (0.4cm);
					\draw[line width=0.5mm] (18,0) circle (0.05cm);
					\draw (18,3) circle (0.4cm);
					\draw (18,4) circle (0.4cm);
					
					
					\draw (-0.5,2.5) -- (-0.5,4.5) -- (1.5,4.5) -- (1.5,2.5) -- (-0.5,2.5);
					
					\draw (-0.5,1.5) -- (-0.5,3.5) -- (1.5,3.5) -- (1.5,1.5) -- (-0.5,1.5);
					\draw (-0.5,0.5) -- (-0.5,2.5) -- (1.5,2.5) -- (1.5,0.5) -- (-0.5,0.5);
					\draw (-0.5,-0.5) -- (-0.5,1.5) -- (1.5,1.5) -- (1.5,-0.5) -- (-0.5,-0.5);
					
					\draw (0.5,2.5) -- (0.5,4.5) -- (2.5,4.5) -- (2.5,2.5) -- (0.5,2.5);
					\draw (1.5,2.5) -- (1.5,4.5) -- (3.5,4.5) -- (3.5,2.5) -- (1.5,2.5);
					\draw (2.5,2.5) -- (2.5,4.5) -- (4.5,4.5) -- (4.5,2.5) -- (2.5,2.5);
					
					
					
					\draw (6.5,2.5) -- (6.5,4.5) -- (8.5,4.5) -- (8.5,2.5) -- (6.5,2.5);
					
					\draw (6.5,1.5) -- (6.5,3.5) -- (8.5,3.5) -- (8.5,1.5) -- (6.5,1.5);
					\draw (6.5,0.5) -- (6.5,2.5) -- (8.5,2.5) -- (8.5,0.5) -- (6.5,0.5);
					\draw (6.5,-0.5) -- (6.5,1.5) -- (8.5,1.5) -- (8.5,-0.5) -- (6.5,-0.5);
					
					\draw (9.5,2.5) -- (9.5,4.5) -- (11.5,4.5) -- (11.5,2.5) -- (9.5,2.5);
					
					
					
					\draw (13.5,2.5) -- (13.5,4.5) -- (15.5,4.5) -- (15.5,2.5) -- (13.5,2.5);
					
					\draw (13.5,-0.5) -- (13.5,1.5) -- (15.5,1.5) -- (15.5,-0.5) -- (13.5,-0.5);
					
					\draw (16.5,2.5) -- (16.5,4.5) -- (18.5,4.5) -- (18.5,2.5) -- (16.5,2.5);
					
					\end{tikzpicture}
					\caption{Visualisierung verschiedener Stride-Werte - Links: (1,1); Mitte: (3,1); Rechts: (3,3) (Quelle: Eigene Darstellung)}
					%\end{wrapfigure}
				\end{minipage}
				\hfill
				\begin{minipage}[t]{.3\linewidth}
					\centering
					%\begin{wrapfigure}[0]{r}{0cm}	%Vorlage: Nielsen/Kapitel 6/Introducing convolutional networks
					\begin{tikzpicture}[scale=0.45, decoration={markings, mark=at position 1 with {\arrow[scale=2]{>}}}]
					\draw (2,1) circle (0.4cm) node [black,midway,xshift=2cm,yshift=1cm,] {1};
					\draw (2,2) circle (0.4cm) node [black,midway,xshift=2cm,yshift=2cm,] {2};
					\draw (2,3) circle (0.4cm) node [black,midway,xshift=2cm,yshift=3cm,] {1};
					
					\draw (3,1) circle (0.4cm) node [black,midway,xshift=3cm,yshift=1cm,] {3};
					\draw (3,2) circle (0.4cm) node [black,midway,xshift=3cm,yshift=2cm,] {0};
					\draw (3,3) circle (0.4cm) node [black,midway,xshift=3cm,yshift=3cm,] {2};
					
					\draw (4,1) circle (0.4cm) node [black,midway,xshift=4cm,yshift=1cm,] {2};
					\draw (4,2) circle (0.4cm) node [black,midway,xshift=4cm,yshift=2cm,] {1};
					\draw (4,3) circle (0.4cm) node [black,midway,xshift=4cm,yshift=3cm,] {3};
					
					
					\draw (7,0) circle (0.4cm) node [black,midway,xshift=7cm,yshift=0cm,] {0};
					\draw (7,1) circle (0.4cm) node [black,midway,xshift=7cm,yshift=1cm,] {0};
					\draw (7,2) circle (0.4cm) node [black,midway,xshift=7cm,yshift=2cm,] {0};
					\draw (7,3) circle (0.4cm) node [black,midway,xshift=7cm,yshift=3cm,] {0};
					\draw (7,4) circle (0.4cm) node [black,midway,xshift=7cm,yshift=4cm,] {0};
					
					\draw (8,0) circle (0.4cm) node [black,midway,xshift=8cm,yshift=0cm,] {0};
					\draw (8,1) circle (0.4cm) node [black,midway,xshift=8cm,yshift=1cm,] {1};
					\draw (8,2) circle (0.4cm) node [black,midway,xshift=8cm,yshift=2cm,] {2};
					\draw (8,3) circle (0.4cm) node [black,midway,xshift=8cm,yshift=3cm,] {1};
					\draw (8,4) circle (0.4cm) node [black,midway,xshift=8cm,yshift=4cm,] {0};
					
					\draw (9,0) circle (0.4cm) node [black,midway,xshift=9cm,yshift=0cm,] {0};
					\draw (9,1) circle (0.4cm) node [black,midway,xshift=9cm,yshift=1cm,] {3};
					\draw (9,2) circle (0.4cm) node [black,midway,xshift=9cm,yshift=2cm,] {0};
					\draw (9,3) circle (0.4cm) node [black,midway,xshift=9cm,yshift=3cm,] {2};
					\draw (9,4) circle (0.4cm) node [black,midway,xshift=9cm,yshift=4cm,] {0};
					
					\draw (10,0) circle (0.4cm) node [black,midway,xshift=10cm,yshift=0cm,] {0};
					\draw (10,1) circle (0.4cm) node [black,midway,xshift=10cm,yshift=1cm,] {2};
					\draw (10,2) circle (0.4cm) node [black,midway,xshift=10cm,yshift=2cm,] {1};
					\draw (10,3) circle (0.4cm) node [black,midway,xshift=10cm,yshift=3cm,] {3};
					\draw (10,4) circle (0.4cm) node [black,midway,xshift=10cm,yshift=4cm,] {0};
					
					\draw (11,0) circle (0.4cm) node [black,midway,xshift=11cm,yshift=0cm,] {0};
					\draw (11,1) circle (0.4cm) node [black,midway,xshift=11cm,yshift=1cm,] {0};
					\draw (11,2) circle (0.4cm) node [black,midway,xshift=11cm,yshift=2cm,] {0};
					\draw (11,3) circle (0.4cm) node [black,midway,xshift=11cm,yshift=3cm,] {0};
					\draw (11,4) circle (0.4cm) node [black,midway,xshift=11cm,yshift=4cm,] {0};
					\end{tikzpicture}
					\caption{Beispiel für unterschiedliche Zero Padding Werte - Links: 0; Rechts: 1 (Quelle: Eigene Darstellung)}
					%\end{wrapfigure}
				\end{minipage}
			\end{figure}
			Der Stride $s$ bestimmt, wie sehr sich die einzelnen LRFs überlappen. Stride und Größe der LRFs beeinflussen die Größe der Feature Maps. \cnnKlein{7} Diese lässt sich zusätzlich durch Zero Padding steuern. Indem um die Input Matrix herum Zeilen und Spalten hinzugefügt werden, können die Feature Maps vergrößert werden, was vor allem dann angewendet wird, wenn die Feature Maps die gleiche Größe wie der Input (ohne Zero Padding) haben sollen. Diese Zeilen und Spalten werden meistens mit dem Wert 0 gefüllt. (vgl. Wu, 2017, S. 12f)
			
			Die Dimensionen des Convolutional Layer lassen sich einfach berechnen: Hat $X_{(l-1)}$ die Form $H({X_{(l-1)}}) \times W({X_{(l-1)}}) \times D({X_{(l-1)}})$ (der vorangehende Layer kann auch dreidimensional sein, wenn es sich dabei z.B. ebenfalls um einen Convolutional Layer handelt) und $K_{(l)}$ die Form $H({K_{(l)}}) \times W({K_{(l)}}) \times D({X_{(l-1)}}) \times D({K_{(l)}})$ (wobei $K_{(l)}$ die Menge aller Kernel des Layer $l$ und $D({K_{(l)}})$ die Anzahl aller Kernel ist), dann hat $X_{(l)}$ die Form $\frac{ H(X_{(l-1)}) - H(K_{(l)}) + 2 \cdot z}{s+1} \times \frac{ W(X_{(l-1)}) - W(K_{(l)}) + 2 \cdot z}{s+1} \times D({K_{(l)}})$. \cnnKlein{7} 
			
			
			% Für jedes Neuron des Convolutional Layers wird dieser Ausschnitt über den vorherigen Layer verschoben. Um wie viel dieser Ausschnitt verschoben wird, legt der Stride-Wert fest.  Größe des Ausschnitts und Stride sind ebenefalls Hyperparameter (?).																	ERLEDIGT
			% Zusätzlich kommt auch noch die Tiefe des Convolutional Layer												ERLEDIGT
			% Die Werte einer Tiefe d des Convolutional Layers werden auch als Feature Map bezeichnet. 					ERLEDIGT
			% Alle Neurone innerhalb einer Feature Map eines Layers haben den gleichen Bias! (Fundamentals, S. 93)		ERLEDIGT
			% Zero Padding																								ERLEDIGT
			% Formel zur Berechnung der Form der Feature Map															
			% Formel zur Berechnung der Werte einer Featur Map
			% Diagramm mit stride = (1,1), (2,1), (2,2)																	ERLEDIGT
			
			\subsection{Pooling Layer}
			Das Ziel eines Pooling Layers ist es, die Größe der Feature Maps zu verkleinern, wodurch gleichzeitig die Komplexität des Models verringert wird. Ähnlich wie die Convolutional Layer haben auch die Pooling Layer drei Dimensionen und ein LRF mit Stride Wert, allerdings keine Kernel oder Biases. Auf das LRF wird eine Pooling-Funktion, wie zum Beispiel Max-Pooling oder Average-Pooling angewandt. Im Fall von Max-Pooling erhält das Neuron des Pooling Layers den höchsten Wert innerhalb des LRF, bei Average-Pooling das arithmetische Mittel der Neurone im LRF. \cnnKlein{8}
			\subsection{Fully-Connected Layer}
			Die Fully-Connected Layer sind gleich den Layern eines FFNN. Jedes ihrer Neurone ist mit jedem Neuron der vorherigen Schicht verbunden und hat seinen eigenen Bias, jede Verbindung hat ihr eigenes Gewicht. \cnnKlein{8}
			\subsection{Softmax}
			Der Softmax Layer ist ein Output Layer, der sich bis auf die Aktivierungsfunktion von bisherigen Output Layern nicht unterscheidet, und hat eine besonderen Eigenschaft: Die Summe der Ausgabewerte aller Neurone dieses Layers hat den Wert 1. Dies ist insofern sinnvoll, da man dadurch die Ausgabewerte als eine Wahrscheinlichkeitsverteilung betrachten kann. Deshalb wird der Softmax Layer v. a. für Classification-Aufgaben als Output Layer eingesetzt, da man den Wert $x_{(L,n)}$ als Wahrscheinlichkeit dafür ansehen kann, das die korrekte Klassifizierung der Fall dieses Neurons ist. Formel \ref{SoftmaxFormel} gibt an, wie die Aktivierung der Neuronen im letzten Layer berechnet wird, wobei $\textrm{len}(L)$ die Anzahl der Neuronen in diesem Layer angibt. (vgl. Nielsen, 2015, Kapitel 3/Softmax)
			\begin{equation}\label{SoftmaxFormel}
				\vec{a}_{(L)} = \frac{ \textrm{e}^{ \vec{z}_{(L)} } }{ \sum\limits_{ n=1 }^{ \textrm{len}(L) } \left( \textrm{e}^{ z_{ (L,n) } } \right)} 
			\end{equation}
			
		\section{Anordnung der Schichten}
		Es gibt keine genauen Regeln, wie die verschiedenen Layer kombiniert werden sollen, allerdings kann man auch nicht einfach ein paar Schichten miteinander kombinieren und brauchbare Ergebnisse erwarten. Es gibt jedoch Reihenfolgen für die Layer, welche sich in der Literatur durchgesetzt haben. So folgt meistens auf einen Convolutional Layer ein Pooling Layer. Diese Abwechslung zwischen Convolutional und Pooling Layer kann mehrmals wiederholt werden, bevor zum Schluss ein oder mehrere Fully-Connected Layer folgen. Des Weiteren hat es sich bewährt, mehrere Convolutional Layer vor einen Pooling Layer zu stapeln, da dadurch die Komplexität des Modells gesteigert werden kann. In den späteren Experimenten werden verschiedene Kombinationen von Schichten untersucht. \cnnKlein{8f}
		% Erklären, dass z. B. ein CNN oft mit C Layer beginnt, und auf einen oder mehreren C Layer ein P Layer folgt. 		ERLEDIGT
	
	\chapter{Netztypus 3: Recurrent Neural Networks}\label{KapitelRNN}
	Dieses Kapitel ist (noch) leer.
		\section{Definition}
	
	
	
	\chapter{Lernen eines KNNs am Beispiel des FFNN}\label{Lernen}
	Bisher wurden in der Arbeit nur verschiedene Arten von KNNs, ihr Aufbau und ihre Funktionsweise behandelt. Jedoch fehlt ein noch ein wichtiger Teil für die Entwicklung von KNNs, das Training (auch Parameter Optimierung genannt). Unter Training versteht man das Bestimmen der passenden Werte für die Parameter des KNNs für eine bestimmte Aufgabe mithilfe von Trainingsdaten. (vgl. Buduma, 2017, S. 17; Gibson \& Patterson, 2017, S. 27) Erklärt wird der Vorgang des Trainings in dieser Arbeit nur anhand des FFNN, und zwar aus drei Gründen: Einerseits ist der Prozess bei diesem Netztypus einfacher als bei anderen zu verstehen. Zweitens funktioniert das Trainieren von CNNs und RNNs ähnlich wie beim FFNN. Drittens ist das Verstehen dieses Prozesses für die Implementierung von KNNs dank vieler moderner Libraries nicht mehr nötig. \\
	
	Es gibt mehrere Optimierungs-Methoden, d. h. Möglichkeiten, wie das Trainieren umgesetzt wird. Diese Arbeit befasst sich mit dem sogenannten Gradient Descent (GD) und dessen zwei bekanntesten Abwandlungen, dem Stochastic Gradient \mbox{Descent (SGD)} und Mini-Batch Gradient Descent. (vgl. Gibson \& Patterson, 2017, S. 98f; ebd., S. 30ff; Buduma, 2017, S. 25) Das Problem beim Herausfinden der Werte für diese Parameter ist, dass diese nicht über ein Gleichungssystem gelöst werden können, sondern iterativ berechnet werden müssen. (vgl. Rashid, 2017, S. 71; Buduma, 2017, S. 18f)
	
	
	%Bisher wurden in der Arbeit nur verschiedene Arten von KNNs, ihr Aufbau und ihre Funktionsweise genannt. Jedoch reicht das nicht aus, damit ein Neuronales Netz beispielsweise Bilder klassifizieren kann. Jedes KNN muss vor seinem Einsatz trainiert werden, d.h. die Weights und Biases so anzupassen, dass das Netz Strukturen in den Daten erkennen kann [BELEG ERFORDERLICH]. Die dafür benötigten Algorithmen sind teilweise erst seit den 1980er Jahren bekannt, aufgrund der mangelnden Rechenleistung haben KNNs erst seit wenigen Jahren Erfolg. [2 BELEGE ERFORDERLICH]. Die zwei häufig miteinander verwendeten Algorithmen sind (Stochastic) Gradient Descent und Backpropagation. 
	% WICHTIG: Zu verstehen, wie Training funktioniert, ist nicht notwendig für die Beantwortung der Forschungsfragen. Jedoch gehört es auch dazu ("der Vollständigkeit wegen"). Außerdem sind die Erkenntnise, was Optimiert werden kann, hilfreich für Experimente (Und somit für Forschungsfragen)
	% Zitat Technologies come and go, but insight is forever -> Notiz 21 			
	% Practitioner S. 92															
	% Bezeichnung Training: Fundamentals, S. 17										
	% Eine Definition von Gradient: Fundamentals, S. 20								
	% Eine Definition von Learningrate: Fundamentals, S. 21							
	% Eine "Definition" von Backpropagation: Fundamentals, S. 23					
	% 3 WICHTIGE OBSERVATION: Fundamentals, S. 30f									
	% Aufbau des Datasets: Fundamentals, S. 31	
	% Notiz 57		Der Prozess, Weights & Biases so zu adjustieren, dass der Fehler kleiner wird, nennt sich Parameter Optimierung (Practitioner S. 27)			ERLEDIGT
	% Notiz 2.5		es wird nicht perfektes Gewicht berechnet, da sonst bisheriger Lernfortschritt zunichte gemacht wird (Rashid S.21)								
	% Notiz 7		Je größer das Gewicht, desto größer die Beteiligung am Fehler (Rashid S. 62)																	
	% Notiz 8		Gefahr, nur lokales und nicht globales Minimum zu erreichen (Rashid S. 77)																		
	% Notiz 9 		Definierung der "klassischen" Loss-Funktion (und warum andere schlecht sind) (Rashid S. 79)														
	%	-> Siehe auch: Loss-Function in Practitioner																												
	% Notiz 13 		Faustregel für Interval für Zufallsinitialisierung der Gewichte (Rashid S. 92)																	
	% Notiz 14		Begründung, warum Gewichte zufällig initiallisiert werden sollten und nicht gleich und oder 0 sein sollten (Rashid S. 93)						
	% Notiz 17		Definition von einem Epoch (Rashid S. 155)																													
	% Notiz 19		Zusammenhang zwischen Lernrate und Epochen (Rashid S. 157)																						
	% Notiz 20 		Zusammenhang zwischen Trefferquote und Anzahl der Hidden Neurone (Rashid S. 158)																
	%	-> eher bei Kapitel zu Experimenten																															
	% Notiz 29		Definiton der Loss-Funktion nach Nielsen (Nielsen Blatt 8)																						
	% Notiz 32		Definiton des Gradienten-Vektor nach Nielsen (Nielsen Blatt 9)																					
	% Notiz 33		Symbol für die Lernrate (Nielsen Blatt 9)																										
	% Notiz 34		Unterschied zwischen GD und SGD nach Nielsen (Nielsen Blatt 10)																					
	% Notiz 36		Über Hyperparameter (Nielsen Blatt 13)																											
	% Notiz 40		Unterschied zwischen SGD und Backpropagation (Nielsen Blatt 16)																					
	% Notiz 43		Ziel von Backpropagation (nach Nielsen) (Nielsen Blatt 16 Rückseite)																			
	% Notiz 44		Error Berechnung bei Backpropagation nach Nielsen (Nielsen Blatt 17)																			
	% Notiz 45		Über BP1 (Nielsen Blatt 18)																														
	% Notiz 46 		Backprop in Worten (Nielsen Blatt 19)																											
	% Notiz 68 		Lernalgorithmen -> keine Garantie effizient oder richtig (Practitioner S. 58)																	
	% Notiz 69		Beschreibung von Backpropagation (Practitioner S. 64)																							
	% Notiz 72		Was genau macht Loss-Funktion (Practitioner S. 71)																								
	% Notiz 79		Wann hört man auf zu Lernen? (Kriesel S. 63)																									
	% Notiz 80		Probleme von Gradientenverfahren (Kriesel S. 65)																								
	% 				Bezeichnung ob Cost oder Loss Funktion (deeplearning S. 80)																						
	%				Backpropagation in Worten (Wartala S. 17)																										
	
	
		\section{Gradient Descent}\label{GradientDescent}
			%Gradient Descent ist ein Algorithmus, welcher die Parameter eines KNNs iterativ, d. h. in mehreren Schritten, verändert. \practitioner{64} 
			Um ein KNN trainieren zu können, muss zunächst herausgefunden werden, wie gut oder schlecht die bisherigen Weights geeignet sind. Dazu wird die Eingabe \mbox{$\vec{x}_{(1)}(\textrm{tr}:i)$}\footnote{Im Fall von MNIST ein Vektor mit 784 Dimensionen; eine pro Pixelwert} eines Trainingsbeispiels $i$ in das Netz eingespeist, dessen gewünschte Ausgabe \mbox{$\vec{y}(\textrm{tr}:i)$}\footnote{Im Fall von MNIST ein Vektor mit 10 Dimensionen; eine pro möglicher Ziffer} bekannt ist, und liefert einen Output \mbox{$\vec{x}_{(L)}(\textrm{tr}:i)$}. Wie nahe dieser Output an die  herankommt, lässt sich mit einer Cost-Funktion, auch als Loss-Funktion bezeichnet, ermitteln. Es gibt mehrere Cost-Funktionen\footnote{Siehe auch \ref{Optimierung:Cost-Funktion}}, eine der einfacheren und gebräuchlicheren ist die mittlere quadratische Abweichung in Formel \ref{MSE}, welche den durchschnittlichen Cost-Wert für alle Trainingsbeispiele berechnet. (vgl. Nielsen, 2015, Kapitel 1/Learning with gradient descent) 
			
			\begin{equation}\label{MSE}
				C = \frac{1}{2\cdot\textrm{Tr}} \cdot \left( \sum_{i=1}^{\textrm{Tr}} |(\vec{y}(\textrm{tr}:i) - \vec{x}_{(L)}(\textrm{tr}:i))^2| \right)
			\end{equation} % Sollte da nicht besser Sigmoid als Bsp. verwendet werden?
			
			Der Wert der Cost-Funktion wird kleiner, je kleiner die Differenz zwischen dem gewünschten und dem eigentlichen Output wird, was wiederum eine Folge von an die Trainingsdaten angepassten Weights und Biases ist. Das Ziel des Gradient Descent Algorithmus ist es, den Wert der Cost-Funktion zu minimieren.\footnote{Das ein kleinerer Cost-Wert nicht umbedingt ein besseres KNN zufolge hat, sieht man in \ref{UnderOverFitting} \fundamentals{31}} (vgl. ebd., Kapitel 1/Learning with gradient descent) 
			
			Vor dem Training müssen die Weights und Biases initialisiert werden, wofür man Zufallswerte verwendet.\footnote{Genaueres dazu in \ref{TodsündenWeights}} (vgl. Nielsen, 2015, Kapitel 1/Implementing our network to classify digits) 
			%Es ist nämlich nicht ratsam für alle Weights und Biases den gleichen Startwert zu verwenden, vor allem nicht 0.\footnote{Genaueres dazu in \ref{TodsündenWeights}}  (vgl. Rashid, 2017, S. 93) 
			
			Da der Cost-Wert nur von den Weights und Biases abhängig ist (Input und Output sind bei Trainingsbeispielen fest vorgegeben und lassen sich nicht verändern), lassen sich diese Parameter mithilfe der partiellen Ableitung der Cost-Funktion nach diesen verändern. Die Formeln zum Aktualisieren der Parameter sind folgende:
			
			\begin{equation}\label{UpdateRuleWeights}
				w_{(l-1,m),(l,n)} = w_{(l-1,m),(l,n)} - \eta \cdot \frac{\partial C}{\partial w_{(l-1,m),(l,n)}}
			\end{equation}
			\begin{equation}\label{UpdateRuleBiases}
				b_{(l,n)} = b_{(l,n)} - \eta \cdot \frac{\partial C}{\partial b_{(l,n)}}
			\end{equation}
			
			Der Vektor mit den partiellen Ableitungen der Cost-Funktion nach den einzelnen Weights und Biases wird Gradient genannt.\footnote{Wie dieser berechnet wird, folgt in \ref{Backpropagation}} (vgl. Nielsen, 2015, Kapitel 1/Learning with gradient descent; ebd., Kapitel 2/The two assumptions we need about the cost function) Die partielle Ableitung $\frac{\partial C}{\partial w_{(l-1,m),(l,n)}}$ bzw. $\frac{\partial C}{\partial b_{(l,n)}}$ gibt an, wie sich die Cost-Funktion ändert, wenn man diesen Parameter ändert. (vgl. Rashid, 2017, S. 81) Ist die partielle Ableitung positiv bzw. negativ, muss der Parameter verringert bzw. erhöht werden. Je größer bzw. kleiner die partielle Ableitung, desto größer bzw. kleiner der Betrag um den der Parameter verändert werden muss. (vgl. ebd., S. 76) Der Faktor $\eta$ wird Learning Rate (oder Lernrate) genannt und steuert wie groß die Veränderungen der Parameter sein sollen. Die Lernrate ist einer der Hyperparameter (Siehe \ref{Hyperparameter}) eines KNNs. \practitioner{31} \newline \break
			Durch mehrfaches, iteratives Anwenden der beiden Formeln \ref{UpdateRuleWeights} und \ref{UpdateRuleBiases} lassen sich die Parameter so anpassen, dass die Cost-Funktion zu einem Minimum kommt. Dabei wird zuerst der durchschnittliche Gradient aller Trainingsbeispiele berechnet, mit welchem dann die Weights und Biases aktualisiert werden. (vgl. Nielsen, 2015, Kapitel 1/Learning with gradient descent) Ein solcher Durchgang durch alle Trainingsdaten wird als Epoch (auch Epoche) bezeichnet. (vgl. Rashid, 2017, S. 155) Die Anzahl der Epochen beim Trainieren ist ebenfalls ein Hyperparameter. \mbox{\practitioner{100}}
			\\\
			\\\
			Es gibt allerdings zwei größere Probleme mit diesem Algorithmus: Erstens gibt es keine Garantie, dass die Cost-Funktion sich dem globalen und nicht nur einem lokalen Minimum annähert. Zweitens dauert das Berechnen eines durchschnittlichen Gradienten bei einer großen Menge von Trainingsdaten relativ lange. (vgl. Nielsen, 2015, Kapitel 1/Learning with gradient descent) Eine Möglichkeit diese Probleme zu umgehen wäre, die Parameter nach jedem Trainingsbeispiel, d. h. mit den einzelnen Gradienten der Beispiele zu adjustieren, was als Stochastic Gradient Descent (SGD) bekannt ist. Der Nachteil dieser Optimierungs-Methode ist, dass die einzelnen Gradienten möglicherweise keine genügende Annäherung an den durchschnittlichen Gradienten sind, wodurch die Werte der Parameter fluktuieren. \fundamentals{33} Um diese Aufgabe zu lösen, verwendet man den Mini-Batch Gradient Descent Algorithmus, welcher den Gradienten für eine kleine Teilmenge (Mini-Batch) aller Trainingsbeispiele berechnet und mit diesen die Weights und Biases aktualisiert.%Die Größe dieses Mini-Batches ist ebenfalls einer der Hyperparameter eines KNNs.
			\footnote{Es kommt in der Literatur, zum Beispiel bei Nielsen, auch vor, dass SGD und Mini-Batch GD zu SGD zusammengefasst werden, wobei SGD wie es hier erklärt ist wie Mini-Batch GD mit einer Teilmengen-Größe von 1 gehandhabt wird.} \fundamentals{27} Zu Beginn eines Epochs werden die Trainingsbeispiele in ihrer Reihenfolge durchgemischt und in die Mini-Batches aufgeteilt, welche dann zum Trainieren verwendet werden. (vgl. Nielsen, 2015, Kapitel 1/Implementing our network to classify digits)
			\\\
			\\\
			Nach dem Trainieren wird das KNN getest, d. h. es werden Testbeispiele, welche nicht bei den Trainingsbeispielen dabei waren, dem Netz übergeben um zu überprüfen, wie gut das Netzwerk verallgemeinert, d. h. wie viele zuvor noch nie gesehene Beispiele richtig klassifiziert\footnote{Unter der Annahme, dass es sich um eine Classifications-Aufgabe handelt} werden. (vgl. Rashid, 2017, S. 148; Buduma, 2017, S. 29) Falls man mit dem Testergebnis zufrieden ist, ist das KNN bereit für seinen Einsatz. \fundamentals{33} Neben den Trainings- und Testdatensätzen gibt es noch den Validationsdatensatz.\footnote{Siehe \ref{EpochsUndMiniBatch}} (vgl. Nielsen, 2015, Kapitel 1/Implementing our network to classify digits)
		
		\section{Backpropagation} \label{Backpropagation}
			Bisher wurde nur behandelt, wie man mithilfe der partiellen Ableitungen die Parameter eines KNNs verändert. Allerdings fehlt dabei die eigentliche Berechnung dieser. Dafür wird der Backpropagation (auch Fehlerrückführung genannt) Algorithmus verwendet, welcher das Problem der Berechnung der partiellen Ableitungen bei mehrschichtigen KNNs gelöst hat. (vgl. Wartala, 2018, S. 17) Um diese zu berechnen wird zunächst ein Zwischenwert eingeführt, der Fehler $\delta_{(l,n)}$ eines Neurons, welcher zunächst über $\frac{\partial C}{\partial z_{(l,n)}}$ definiert wird. Das kommt daher, dass diese partielle Ableitung den Faktor angibt, wie stark sich eine Änderung $\Delta z_{(l,n)}$ auf den Wert der Cost-Funktion auswirkt (nämlich um eine Differenz von $\frac{\partial C}{\partial z_{(l,n)}} \cdot \Delta z_{(l,n)}$). Mit Backpropagation lässt sich dieser Fehler für jedes Neuron jeder Schicht berechnen, mit welchem man auch die partiellen Ableitungen $\frac{\partial C}{\partial w_{(l-1,m),(l,n)}}$ und $\frac{\partial C}{\partial b_{(l,n)}}$ berechnen kann. (vgl. Nielsen, 2015, Kapitel 2/The four fundamental equations behind backpropagation) Der Algorithmus verwendet die folgenden vier Formeln \ref{BP1} bis \ref{BP4} um die Fehler und partiellen Ableitungen zu berechnen: 

			\begin{equation}\label{BP1}
%				\delta_{(L,n)}=\frac{\partial C}{\partial x_{(L,n)}} \cdot f_{(L)}\prime ( \vec{w}_{(L-1), (L,n)} \cdot \vec{x}_{(L-1)} + \vec{b}_{(L)}) 
%				\delta_{(L)} = \nabla_{x_{(L)}}C \odot f_{(L)}\prime \left(W_{(L-1),(L)}\cdot\vec{x}_{(L-1)}+\vec{b}_{(L)}\right)
				\vec{\delta}_{(L)} = \nabla_{\vec{x}_{(L)}}C \odot f_{(L)}\prime (\vec{z}_{(L)})
			\end{equation}
			
			\begin{equation}\label{BP2}
%				\delta_{(l)} = ({W_{(l),(l+1)}}^\intercal \cdot \delta_{(l+1)}) \odot f_{(l)}\prime \left(W_{(l-1),(l)}\cdot\vec{x}_{(l-1)}+\vec{b}_{(l)}\right)
				\vec{\delta}_{(l)} = ({W_{(l),(l+1)}}^\intercal \cdot \vec{\delta}_{(l+1)}) \odot f_{(l)}\prime (\vec{z}_{(l)})
			\end{equation}
			
			\begin{equation}\label{BP3}
				\frac{\partial C}{\partial b_{(l,n)}} = \delta_{(l,n)}
			\end{equation}
			
			\begin{equation}\label{BP4}
				\frac{\partial C}{\partial w_{(l-1,m), (l,n)}} = \delta_{(l,n)} \cdot x_{(l-1,m)}
			\end{equation}
			
			Die erste Formel berechnet den Fehler in der letzten Schicht des KNNs. Mithilfe der zweiten Formel lässt sich der Fehler in die Schichten $L-1$, $L-2$, ..., $1$ rückführen. Die letzten beiden Formeln dienen der Berechnung der partiellen Ableitungen.\footnote{Da diese Formeln für manche Leser nicht einfach verständlich sind, befindet sich im Anhang X (NUMMER EINFÜGEN) eine Herleitung der Formeln. } (vgl. ebd., Kapitel 2/The four fundamental equations behind backpropagation)


			\newpage
		\section{Hyperparameter} \label{Hyperparameter}
			Unter Hyperparametern versteht man Parameter, welche das Training eines KNN steuern. Diese wären: Anzahl der Neurone pro Layer, Learning Rate, Aktivierungsfunktion, Initialisierung der Gewichte und Biases, Cost-Funktion, Anzahl der Epochs, Größe der Mini-Batches, Momentum, Regularization und Vectorization.\footnote{Die letzten drei Hyperparameter, sowie nur für bestimmte Netztypen benötigte Hyperparameter, werden aufgrund des Umfangs der Arbeit hier nicht behandelt.} \newline \mbox{(vgl. Gibson \& Patterson, 2017, S. 78; ebd., S. 100)}
			% Practitioner S. 100
			\subsection{Learning Rate}
			Die Learning Rate steuert, wie bereits erwähnt, wie groß die Schritte relativ zu den partiellen Ableitungen sein sollen, mit denen die Parameter verändert werden. Dabei hat die Lernrate einen großen Einfluss darauf, wie gut und wie lange das KNN trainiert wird. Eine große Learning Rate (z.B. $\eta = 1$) führt zwar schneller zu einem Minimum als eine kleine Lernrate (z.B. $\eta = 0,000001$), hat allerdings den Nachteil, dass der Wert der Cost-Funktion nicht so nah ans Minimum herankommt, sondern sich nur in seiner Umgebung bewegt. Die kleinere Lernrate nähert sich dem Minimum zwar besser an, benötigt dafür aber um einiges länger. Es gibt zwar die Möglichkeit, die Lernrate in Abhängigkeit zur Nähe am Minimum anzupassen, allerdings ist das außerhalb des Rahmens dieser Arbeit und ihrer Experimente. \practitioner{100}
			%Welche genauen Auswirkungen die Learning Rate hat, wird in Experiment Nr. X (NR EINFÜGEN) untersucht. 
			\subsection{Cost-Funktion}\label{Optimierung:Cost-Funktion}
			Es gibt verschiedene Arten von Cost-Funktionen, welche je nach Aufgabe des Netzes verwendet werden. So gibt es beispielsweise eigene Cost-Funktionen für Classification-Aufgaben. \practitioner{71-78} In dieser Arbeit wird allerdings nur die Mean Squared Error Funktion eingesetzt, da andere Cost-Funktionen bestimme Voraussetzungen an den Output Layer haben und auch den Rahmen der Arbeit übersteigen würden.
			% Gründe für MSE: Rashid S. 79/Liste
			\subsection{Epochs \& Mini-Batch}\label{EpochsUndMiniBatch}
			Die Größe der Mini-Batches hat einen Einfluss darauf, wie effizient die Optimierungs-Methode arbeitet. Zu kleine Mini-Batches (z.B. 1) nutzen oft die Fähigkeiten der Hardware nicht komplett aus, wodurch das Trainieren länger dauert. Zu große Mini-Batches sind ebenfalls ineffizient, da eine ausreichende Annäherung an den durchschnittlichen Gradienten auch mit kleineren Mini-Batches erziehlt werden kann.
			%Wie sich die Größe der Mini-Batches auswirkt wird in den späteren Experimenten untersucht. \newline
			\\\
			Die Anzahl der Epochs gibt an, wie oft das KNN die Trainingsbeispiele durcharbeitet. Dabei gibt es einen Zusammenhang zwischen Anzahl der Epochs und der Learning Rate. Je mehr Epochen das KNN beim Training durchläuft, desto kleiner sollte die Learningrate sein, da sich die Optimierungs-Methode in vielen Schritten besser ans Minimum annähern kann, wenn diese kleiner sind. Durchläuft das KNN nur wenige Epochs sollte die Learning Rate größer sein, da die wenigen Aktualisierungen größer sein müssen, um sich dem Minimum zu nähern. (vgl. Rashid, 2017, S. 157)
			
			Das Training kann allerdings mit Early Stopping schon vor Durchlauf aller Epochs beendet werden. Nach jedem Epoch wird der Validationsdatensatz durchgegangen. Sobald sich die Trefferquote nur noch bei den Trainingsbeispielen, nicht aber bei den Validationsbeispielen verbessert, wird das Training gestoppt, da es sonst zu Overfitting\footnote{Siehe \ref{UnderOverFitting}} kommt. (vgl. Nielsen, 2015, Kapitel 3/Overfitting and regularization)
			% Early stopping
			\subsection{Initialisierung der Parameter} \label{TodsündenWeights}
			Für die Effizienz der Optimierungs-Methode sind gute Startwerte für die Parameter ausschlaggebend. (vgl. Nielsen, 2015, Kapitel 3/Weight initialization) Diese werden, wie in \ref{GradientDescent} erwähnt, mit Zufallswerten initialisiert. Der Grund, warum nicht für alle Weights der gleiche Startwert verwendet wird, ist, dass in diesem Fall die Optimierungs-Methode nicht dazu in der Lage wäre, unterschiedliche Gewichte zu erzeugen, d. h. alle Gewichte haben am Ende des Trainings den gleichen Wert (auch wenn dieser nicht gleich dem Startwert sein muss). Wenn alle Gewichte gleich sind, wäre das so, wie wenn alle Hidden Layer nur 1 Neuron hätten.\footnote{Angenommen alle Biases hätten den Wert 0} Einem solchen Netz würde es an Lernkapazität mangeln. (vgl. Rashid, 2017, S. 93; ebd., S. 158) 
			%DER BIAS IST DAVON NICHT BETROFFEN
			Werden die Parameter mit Zufallswerten einer Normalverteilung mit beispielsweise $\varnothing = 0$ und $\sigma = 1$ initialisiert, so kann es zur Sättigung\footnote{Siehe \ref{Sättigung}} von Neuronen kommen, da die Normalverteilung von $z_{(l,n)}$ eines Neurons mit 99 Eingabewerten von vorherigen Neuronen, welche alle den Ausgabewert 1 haben, eine Standardabweichung von $\sqrt{100}=10$ hat. In diesem Fall lernt das KNN zu Beginn nur langsam und benötigt um einiges länger, als wenn man die Parameter intelligenter initialisiert hätte. Daher werden die Gewichte zu einem Neuron $x_{(l,n)}$ mit Zufallswerten einer Normalverteilung mit $\varnothing = 0$ und $\sigma = \frac{1}{\sqrt{\textrm{len}(l-1)}}$, wobei \mbox{$\textrm{len}(l-1)$} die Anzahl der Neuronen in der Schicht \mbox{$l-1$}, deren Werte dem Neuron $x_{(l,n)}$ übergeben werden, initialisiert.\footnote{Es gibt noch andere Methoden, um dieses Problem zu umgehen, welche allerdings nicht Thema dieser Arbeit sein werden.} Die Biases werden hingegen oft mit 0 oder Zufallswerten einer Normalverteilung mit $\varnothing = 0$ und $\sigma = 1$ initialisiert. (vgl. Nielsen, 2015, Kapitel 3/Weight initialization)
			% Gleicher Wert für alle Weights bei Initiallisierung
			% 0 als Startwert für Initiallisierung
			% Zu große/kleine Startwerte -> Sättigung
			% Rashid S. 92
			%Siehe Nielsen Kapitel 3 Absatz 2)	
			
			
				
		\section{Probleme beim Trainieren}
		Das Trainieren von KNNs ist oft mit zusätzlichen Aufgaben und Problemen verbunden. Vier der verbreiteteren Probleme werden im folgenden Unterkapitel angesprochen. Für viele Spezialfälle von KNNs gibt es eigene Schwierigkeiten, welche das Ausmaß dieser Arbeit allerdings bei Weitem übersteigen würden. (vgl. Rashid, 2017, S. 156; Nielsen, 2015, Kapitel 5/Other obstacles to deep learning)
			\subsection{Under- und Overfitting} \label{UnderOverFitting}
			\begin{wrapfigure}{}{0cm} %Vorlage: Fundamentals S. 29
				\begin{tikzpicture}[scale=1]
				\begin{axis}[
				%title={$f(x) = \frac{1}{1 + \mathrm{e}^{-x}}$},
				xtick={-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5}, 
				xticklabels={-5,,,,,0,,,,,5},
				ytick={-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5}, 
				yticklabels={-5,,,,,0,,,,,5},
				x=20,
				y=20,
				ymin=-5.5, 	ymax=5.5, 
				xmin=-5.5, 	xmax=5.5, 
				axis lines=center, 
				hide obscured x ticks=false, 
				xlabel=$x$,
				ylabel={$y$},
				every inner x axis line/.append style={-},
				every inner y axis line/.append style={-},
				every axis x label/.style={at={(ticklabel* cs:0.99)},anchor=west,},
				every axis y label/.style={at={(ticklabel* cs:0.99)},anchor=south,},
				] 
				\addplot[domain=-5:5, samples=100, dashed]{((x-1)^3)+2*((x-1)^2)-x}; 
				\addplot[domain=-5:5, samples=100, thick]{0.45*x+0.25};
				\addplot[domain=-5:5, samples=100, dotted]{-1.5*x-3};
				\addplot[only marks, thick, mark=x, mark options={scale=2}] table[x index=0, y index=1] {mydata.dat};
				\addplot[only marks, thin, mark=*] table[x index=0, y index=1] {mydata2.dat};
				\end{axis};
				\end{tikzpicture}	
				\caption{Die gepunktete Funktion repräsentiert Underfitting, die gestrichelte Overfitting und die durchgehende ein für die Aufgabe passendes KNN. Die Kreuze stellen die Trainingsdaten, die Punkte die Testdaten dar. (Quelle: Eigene Darstellung)}\label{UnderOverFittingGraph}
			\end{wrapfigure}
			Beim Trainieren eines KNNs geht es um das Anpassen der Parameter, damit das Netz die Trainingsbeispiele richtig klassifiziert. Je besser allerdings das Netz an diese Beispiele angepasst ist, desto schlechter verhält sich das Netz mit Daten, welche es zuvor nicht gesehen hat. Sobald sich die Cost-Funktion einem Minimum nähert, die Trefferquote bei den Testbeispielen aber nicht mehr steigt, spricht man von Overfitting. (vgl. Nielsen, 2015, Kapitel 3/Overfitting and regularization)
			
			Underfitting ist das Gegenteil von Overfitting. Es tritt auf, wenn das Netz zu wenig an die Trainingsbeispiele angepasst und die Eingabe nicht in Verbindung mit der Ausgabe setzen kann. \practitioner{26-27; Wartala, 2018, S. 195} Abbildung \ref{UnderOverFittingGraph} veranschaulicht den Unterschied anhand einer Regressions-Aufgabe. 
			% Siehe Fundamentals, S. 29
			% Siehe Deep Learning
			% Notiz 54, 55
			
			\subsection{Vanishing Gradient}\label{VanishingGradient}
			Das Problem des Vanishing Gradient tritt v. a. bei KNNs mit mehreren Hidden Layern auf. Durch das Rückführen des Fehlers mit Formel \ref{BP2} wird die partielle Ableitung für Parameter in den ersten Hidden Layern immer kleiner, wodurch diese Layer langsamer lernen als die Hidden Layer nahe dem Output Layer. (vgl. Nielsen, 2015, Kapitel 5)
			
			\subsection{Dying ReLU}\label{DyingReLU}
			Dieses Problem betrifft normale ReLUs. Es besteht die Möglichkeit, dass $z_{l,n}$ einen Wert kleiner 0 annimmt, wodurch die Ausgabe $x_{(l,n)}$ den Wert 0 annimmt. In diesem Fall schafft es der Backpropagation Algorithmus nicht mehr die Gewichte zu diesem Neuron anzupassen, da Formel \ref{BP4} ebenfalls immer den Wert 0 erzeugt. Um dieses Problem zu umgehen gibt es die Leaky ReLUs, welche von diesem Problem nicht betroffen sind. \practitioner{243}
			\subsection{Sättigung}\label{Sättigung}
			Ein Neuron gilt dann als gesättigt, wenn sich $f_{(l)}\prime(z_{(l,n)})$ dem Wert 0 annähert. In diesem Fall können die Parameter zu Beginn des Trainings nur in sehr kleinen Schritten verändert werden, da $\delta_{(l,n)}$ durch die geringe Steigung der Aktivierungsfunktion ebenfalls relativ klein ist. Von diesem Problem sind allerdings nur bestimmte Aktivierungsfunktionen, wie Sigmoid- oder TanH-Funktion. Die Rectified Linear-Funktion und ihre Abwandlung haben diesen Nachteil nicht. (vgl. Nielsen, 2015, Kapitel 2/The four fundamental equations behind backpropagation)
			%\subsection{Langsames Lernen durch große Gewichte und falsche Cost-Funktion} % Siehe Nielsen Kapitel 3/Cross entropy 
			% Notiz 8, 11, 12
			
	
	%IRGENDWO MUSS DIE GRENZE GEZOGEN WERDEN, WAS ALLES BEI DEN EXPERIMENTEN DABEI IST UND WAS NICHT.
		
		
	\chapter{Aufbau und Durchführung der Experimente} \label{Experimente}
	Dieses Kapitel ist (noch) leer.
	% Das mit Durchprobieren von mehreren Hyperparametern, etc. Siehe Fundamentals, S. 32
	% Layer Size eher eine Potenz von 2 (Practitioner S. 274)
	% Ich habe nicht das Ziel, irgendwelche Rekorde zu brechen. Meine Forschung geht, wie sich das Verhalten von KNNs je nach Änderung von (Hyper)parametern ändern. Würde ich Rekorde aufstellen wollen, müsste ich mich mehr mit Optimiesierungsmethoden beschäftigen. 
	
		\section{Ausformulierung der Forschungsfragen}
		\section{Aufbau der Experimente}
		\section{Verwendete Werkzeuge}
			\subsection{Python und LIBRARY}
			\subsection{Entwicklungsumgebung}
		\section{Optimierung}
		\section{Experimente zum FFNN}
		\section{Experimente zum CNN}
		\section{Experimente zum RNN}
	
			
			
	\chapter{Resümee}
	Dieses Resümee ist (noch) leer.
	% Schade, dass nicht genauer auf Optimierungsmöglichkeiten eingegangen werden konnte. 
	
	\begin{sloppypar}
	\cleardoublepage
	\addcontentsline{toc}{chapter}{Literaturverzeichnis}
	\printbibliography[title={Literaturverzeichnis}]
	\end{sloppypar}
	
	
	\cleardoublepage
	\addcontentsline{toc}{chapter}{Abbildungsverzeichnis}
	\listoffigures

	

%	\chapter*{Abkürzungsverzeichnis}
%		\addcontentsline{toc}{chapter}{Abkürzungsverzeichnis}
%		\begin{acronym}[Bash]
%			\acro{KDE}{K Desktop Environment}
%			\acro{SQL}{Structured Query Language}
%			\acro{Bash}{Bourne-again shell}
%			\acro{JDK}{Java Development Kit}
%		\end{acronym}
	
	
	
	\printglossary[style=list, type=Abk, nonumberlist]
	\printglossary[style=altlistgroup, title=Glossar, nonumberlist] 			%Damit das geht muss zuerst Tools -> Befehle -> Makeglossaries ausgeführt werden
	
	
	
	\chapter*{Anhang}
		\addcontentsline{toc}{chapter}{Anhang}

		
		%\newpage
		%\vspace*{1.03cm}
		%\vspace*{3.73cm}
		\section*{Anhang A: Notationstabelle}
			\addcontentsline{toc}{section}{Anhang A: Notationstabelle}
			
			\vspace{2mm}
			\newcommand{\newNotationRow}{\\[2.5mm]}
			\begin{tabular}{p{30mm}p{110mm}}
			\hline
			\centering\vspace{1mm} $l$ & \vspace{0mm} Nummer einer Schicht (Indexierung: 1)\newNotationRow	
			\centering $L$ & Letzte Schicht eines KNNs\newNotationRow
			\centering $n$ & Nummer eines Neurons der Schicht $l$ (Indexierung: 1)\newNotationRow
			\centering $m$ & Nummer eines Neurons der Schicht $l-1$ (Indexierung: 1)\newNotationRow % Ebenfalls 1, da Indexierung innerhalb eines Layers nicht davon abhängt, der wie vielte Layer es ist
			\centering $x_{(l,n)}$ & Ausgabewert des Neurons $n$ der Schicht $l$\newNotationRow
			\centering $w_{(l-1,m),(l,n)}$ & Verbindungsgewicht zwischen dem Neuron m der Schicht $l-1$ und dem Neuron $n$ des Layers $l$ (Skalar) \newNotationRow
			\centering $\vec{w}_{(l-1),(l,n)}$ & Verbindungsgewichte zwischen allen Neuronen der Schicht \mbox{$l-1$} und dem Neuron $n$ des Layers $l$ (Vektor) \newNotationRow
			\centering $W_{(l-1),(l)}$ & Verbindungsgewichte zwischen allen Neuronen der Schicht \mbox{$l-1$} und allen Neuronen des Layers $l$ (Matrix) \newNotationRow
			\centering $b_{(l,n)}$ & Bias des Neurons $n$ in der Schicht $l$ \newNotationRow
			\centering $f_{(l)}(x)$ & Aktivierungsfunktion der Schicht $l$\newNotationRow
			
			\centering $\odot$ & Hadamard Produkt; Zwei Vektoren gleicher Dimension werden elementweise miteinander multipliziert und ergeben einen Vektor mit der gleichen Dimension. $l$\newNotationRow
			\hline
			\end{tabular}


		\newpage
		\vspace*{2cm}	
		\section*{Anhang B: Beweise für die Formeln von Backpropagation}
			\addcontentsline{toc}{section}{Anhang B: Beweise für die Formeln von Backpropagation}
			Dieser Anhang ist (noch) leer.

		\newpage
		\vspace*{2cm}
		%\vspace*{3.73cm}
		\section*{Anhang C: Programmcode}
			\addcontentsline{toc}{section}{Anhang C: Programmcode}
			{\renewcommand*\ttdefault{txtt} 
			\begin{lstlisting}[title=Beispielcode 1 (Beispiel nur zum Testen der Formatierung)]
from tkinter import *
import numpy

class Paint(object):

def __init__(self):
self.imageArray = numpy.zeros((28, 28))

self.root = Tk()

self.canvas = Canvas(self.root, bg='black', width=280, height=280)
self.canvas.pack()

self.setup()
self.root.mainloop()

def setup(self):
self.old_x, self.old_y = None, None
self.canvas.bind('<B1-Motion>', self.paint)

def paint(self, event):
if self.old_x and self.old_y:
#self.canvas.create_line(self.old_x, self.old_y, event.x, event.y, width=1, fill="black", capstyle=ROUND, smooth=TRUE, splinesteps=36)
self.canvas.create_rectangle(self.old_x, self.old_y, self.old_x+10, self.old_y+10, fill="white", outline = 'white')
self.imageArray[int(self.old_x/10), int(self.old_y/10)] = 1
self.old_x, self.old_y = round(event.x/10)*10, round(event.y/10)*10
print(self.old_x, self.old_y)

def reset(self, event):
self.old_x, self.old_y = None, None

if __name__ == '__main__':
Paint()	
			\end{lstlisting}}
		
		
		\newpage
		\vspace*{2cm}	
		\section*{Anhang D: Ergebnisse der Experimente}
			\addcontentsline{toc}{section}{Anhang D: Ergebnisse der Experimente}
			Dieser Anhang ist (noch) leer.

	
	
	\chapter*{Selbstständigkeitserklärung}
	Name: Tobias Prisching\newline
	
	Ich erkläre, dass ich diese vorwissenschaftliche Arbeit eigenständig angefertigt und nur die im Literaturverzeichnis angeführten Quellen und Hilfsmittel benutzt habe.
	
	\vspace{3cm}
	
	%\hspace{2cm} Ort, Datum \hfill Unterschrift \hspace{2cm}
	
	\parbox{6cm}{\centering\hrule
	\strut \centering\footnotesize Ort, Datum} \hfill\parbox{6cm}{\hrule
	\strut \centering\footnotesize Unterschrift}
	
	%\end{flushleft} 
	
\end{document} 