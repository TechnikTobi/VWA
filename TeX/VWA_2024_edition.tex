\documentclass[
	a4paper,
	12pt,
	ngerman,
	oneside
]{scrreprt}											% Festlegen der Dokumentenklasse

% Das ist ein Kommentar
% Bei Anführungszeichen: unteres -> Anführungszeichen + Shift+Akzent; oberes -> Anführungszeichen + Shift+Hashtag
% Mathmatischer Ausdruck -> $AUSDRUCK$ oder \(AUSDRUCK\)
% Tiefgestelte Zahlen: x_1

\usepackage[ngerman]{babel} 						% Deutsche Einstellungen
\usepackage[utf8]{inputenc} 						% UTF8
\usepackage[T1]{fontenc}							% Schriftart für den Titel
\usepackage{csquotes}								% Für Anführungszeichen
\usepackage[onehalfspacing]{setspace}				% 1,5 Zeilenabstand
\usepackage{geometry}								% Für die Bestimmung der Größe der Seiten
\usepackage[
	backend=bibtex, 
	citestyle=authoryear, 
	date=short
]{biblatex}											% Fürs Literaturverzeichnis

\usepackage{calc}									% Für \heightof
\usepackage{pgfplots}								% Für Graphen

\usepackage{graphicx}								% Fürs Einbinden von Grafiken
\usepackage{wrapfig}								% Fürs Verschieben von Grafiken
\usepackage{amsmath}								% Für Vektoren
\usepackage{abstract}								% Für das Abstract
\usepackage{url}									% Für URLs im Literaturverzeichnis
\usepackage{etoolbox}	
\usepackage{chngcntr}	
\usepackage[nopostdot, toc]{glossaries}				% Fürs Glossar

\usepackage{subcaption}								% Für subcaptions für subBilder -> Nicht ganz klar ob das Package benötigt wird
\usepackage[
	format=plain,
	indention=0cm,
	labelfont=bf,
	figurename=Abb.
]{caption}											% Formatierung von Bildunterschriften
\usepackage[percent]{overpic}
\usepackage{amsfonts,amssymb}

\usepackage{lineno}									% Für Zeilennummerierung für Korrektur
\usepackage[hang,flushmargin]{footmisc} 			% Für Indention in Fußnoten

\usepackage{csvsimple}								% Einlesen von CSV Dateien
\usepackage{longtable}								% Tabellen über mehrere Seiten (?)
\usepackage{multirow}								% Für Mehrere Zellen in einer Zeile

% Nicht verwendete imports
% \usepackage{acronym}								% Fürs Abkürzungsverzeichnis -> doch nicht notwendig?
% \usepackage{lstlinebgrd}							% Fürs Formatieren von Code Teil 3 -> macht Probleme
% \usepackage{booktabs}								% Macht nix?
% \usepackage{float}								% Macht nix?

\usetikzlibrary{datavisualization}					% Ebenfalls für Graphen
\usetikzlibrary{decorations.markings}				% Pfeilspitzen in der Mitte
\usepgfplotslibrary{statistics}						% Boxplots


\geometry{a4paper,left=35mm,right=25mm,top=10mm} 	% Format der Datei + Abstand zu den Rändern

\addtokomafont{chapter}{\rmfamily} 					% Schriftart für Kapitelüberschriften
\addtokomafont{chapterentry}{\rmfamily}				% Schriftart für Kapiteleinstiege
\addtokomafont{section}{\rmfamily}					% Schriftart für Unterkapitelüberschriften
\addtokomafont{subsection}{\rmfamily}				% Schriftart für Unterunterkapitelüberschriften
\addtokomafont{subsubsection}{\rmfamily} 			% Schriftart für Unterunterunterkapitelüberschriften
\addtokomafont{descriptionlabel}{\rmfamily} 		% Schriftart für Glossar

\setlength{\parindent}{0pt}							% Nach Absatz nicht einrücken
\setlength\LTleft\parindent
\setlength\LTright\fill

\patchcmd{\abstract}{\null\vfil}{}{}{}				% Verschieben des Abstracts in der Höhe
\counterwithout{figure}{chapter} 					% Abbildungen werden nicht mehr per Kapitel, sonder global nummeriert 
\counterwithout{equation}{chapter}					% Equations werden global nummeriert
\counterwithout{table}{chapter}						% Tabellen werden global nummeriert

\interfootnotelinepenalty=10000
\displaywidowpenalty=10000
\widowpenalty=10000
\clubpenalty=10000

\raggedbottom										% Einstellung, wie das Ende einer Seite ausschauen soll



% Fürs Zitieren
\addbibresource{Literaturverzeichnis.bib} 			% Einbinden des Literaturverzeichnises

\DeclareFieldFormat{urldate}{%
	(Zuletzt besucht am \thefield{urlday}. \thefield{urlmonth}. \thefield{urlyear}\isdot)
}														% Fürs Datum beim Literaturverzeichnis
\DeclareFieldFormat{url}{URL: \url{#1}} 
\DeclareNameAlias{sortname}{family-given}				% Nachname vor Vorname Teil 1
\DeclareNameAlias{default}{family-given}				% Nachname vor Vorname Teil 2

\renewcommand*{\labelnamepunct}{\addcolon\addspace} 	% Beistrich und Abstand nach dem Namen des Autors
\renewcommand*{\mkbibnamefamily}[1]{\MakeUppercase{#1}}	% Nachname des Autors in Großbuchstaben
\renewcommand{\multinamedelim}{\addslash}				% Mehrere Autoren durch Slash separiert Teil 1
\renewcommand*{\finalnamedelim}{\addslash}				% Mehrere Autoren durch Slash separiert Teil 2		

% Begin Einbinden der Werke
\nocite{Practitioner}
\nocite{Nielsen}
\nocite{Gurney}
\nocite{Fundamentals}
\nocite{Rashid}
\nocite{Wartala}
\nocite{DickeBuch}
\nocite{CNNklein}
\nocite{CNNgross}
\nocite{UnderstandingLSTMs}
\nocite{Gupta}
\nocite{UnreasonableEffectivness}
\nocite{LeCunWebseite}
% Ende Einbinden der Werke

% Shortcuts zum Zitieren
\newcommand{\practitioner}[1]{vgl. Gibson \& Patterson, 2017, S. {#1}}
\newcommand{\fundamentals}[1]{vgl. Buduma, 2017, S. {#1}}
\newcommand{\cnnKlein}[1]{vgl. Nash \& O'Shea, 2015, S. {#1}}
\newcommand{\ebd}[1]{vgl. ebd., S. {#1}}



\usepackage{listings}								% Fürs Formatieren von Code Teil 1
\usepackage{color}									% Fürs Formatieren von Code Teil 2
%\usepackage{lstlinebgrd}							% Fürs Formatieren von Code Teil 3

%Anfang für Formatierung von Code
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb, %https://stackoverflow.com/questions/3175105/writing-code-in-latex-document
	language=Python,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\fontsize{10}{10}\ttfamily},
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{black}\bfseries,
	commentstyle=\itshape\color{gray},
	stringstyle=\itshape,
	breaklines=true,
	tabsize=3,
	frame=nonuple,			%https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
	keepspaces=false,			%https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
	numbers=left,			%https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
	rulecolor=\color{black},  %https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
	morekeywords={self},		%https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings und https://latex.org/forum/viewtopic.php?t=2320
	breakindent=6em,				%https://tex.stackexchange.com/questions/4239/which-measurement-units-should-one-use-in-latex und https://github.com/olivierverdier/python-latex-highlighting/blob/master/pythonhighlight.sty
}
\lstset{literate=%
	{Ö}{{\"O}}1
	{Ä}{{\"A}}1
	{Ü}{{\"U}}1
	{ß}{{\ss}}1
	{ü}{{\"u}}1
	{ä}{{\"a}}1
	{ö}{{\"o}}1
}
%Ende für Formatierung von Code	



\newglossary[tlg]{Abk}{tld}{tdn}{Abkürzungsverzeichnis}
\makeglossaries

%==============================GLOSSAR=================================
\newglossaryentry{Kuenstliches Neuronales Netz(werk)}{name=Künstliches Neuronales Netzwerk, description={Rechnerisches Modell, bestehend aus mehreren künstlichen Neuronen}}
\newglossaryentry{Neuronales Netz(werk)}{name=Neuronales Netz(werk), description={Siehe: Künstliches Neuronales Netzwerk}}
\newglossaryentry{Kuenstliches Neuron}{name=Künstliches Neuron, description={Mathematische Funktion mit mehreren Parametern}}
\newglossaryentry{Neuron}{name=Neuron, description={Siehe: Künstliches Neuron}}
\newglossaryentry{Weight}{name=Weight, description={Parameter zur Gewichtung der Eingabe eines künstlichen Neurones}}
\newglossaryentry{Gewicht}{name=Gewicht, description={Siehe: Weight}}
\newglossaryentry{Layer}{name=Layer, description={Gruppierung von einem oder mehreren Neuronen}}
\newglossaryentry{Schicht}{name=Schicht, description={Siehe: Layer}}
\newglossaryentry{Ausgabewert}{name=Ausgabewert, description={Wert eines Neurons, welchen es an Neurone der nächsten Schicht weitergibt}}
\newglossaryentry{Bias}{name=Bias, description={Parameter welcher zu den gewichteten Eingaben eines Neurons addiert wird}}
\newglossaryentry{Aktivierungsfunktion}{name=Aktivierungsfunktion, description={Funktion, in welche die gewichteten Eingaben samt Bias eingegeben werden und den Ausgabewert des Neurons berechnet}}
\newglossaryentry{Aktivierung}{name=Aktivierung, description={Zustand eines Neurons; gilt als aktiviert wenn sein Ausgabewert $\neq 0$}}
\newglossaryentry{Regression}{name=Regression, description={Aufgabenstellungen bei denen der Zusammenhang zwischen Eingabe und Ausgabe modelliert werden muss um für eine gegebene Eingabe die Ausgabe zu ermitteln}}
\newglossaryentry{Classification}{name=Classification,description={Aufgabenstellung, bei der die Eingabe in zwei oder mehr Klassen kategorisiert wird}}
\newglossaryentry{Binary Classification}{name=Binary Classification, description={Aufgabenstellungen bei denen entschieden werden muss, ob die Eingabe einer bestimmten Struktur entspricht oder nicht. Oft liegt der Ausgabewert zwischen 0 und 1, weshalb ein Schwellenwert für die Ausgabe festgelegt wird um zwischen den beiden möglichen Kategorien zu unterscheiden}, parent=Classification}
\newglossaryentry{Multiclass Classification}{name=Multiclass Classification, description={Aufgabenstellungen bei denen die Eingabe in eine von mehreren Kategorien eingeordnet werden muss. Die Klasse dessen entsprechendes Neuron den höchsten Ausgabewert hat ist die Antwort des KNN}, parent=Classification}
\newglossaryentry{Feedforward Neural Network}{name=Feedforward Neural Network, description={KNN mit einem Input, einem Output und einem oder mehreren Hidden Layern, deren Neurone mit allen Neuronen benachbarter Schichten verbunden sind}}
\newglossaryentry{Convolutional Neural Network}{name=Convolutional Neural Network, description={Art von KNNs; besteht grundsätzlich aus drei unterschiedlichen Arten von Layern: Convolutional, Pooling und Fully-connected Layer}}
\newglossaryentry{Recurrent Neural Network}{name=Recurrent Neural Network, description={KNNs, deren Ausgabe zu einem Zeitschritt $t$ die Ausgabe zum Zeitschritt $t+1$ beeinflusst}}
\newglossaryentry{Learning Rate}{name=Learning Rate, description={Hyperparamter zur Bestimmung der Größe der Veränderungen der Parameter relativ zu dessen partieller Ableitung}}
\newglossaryentry{Lernrate}{name=Lernrate, description={Siehe: Learning Rate}}
\newglossaryentry{Parameter}{name=Parameter, description={Weights und Biases eines KNN}}
\newglossaryentry{Sigmoid-Funktion}{name=Sigmoid-Funktion, description={Häufige Aktivierungsfunktion, definiert als: $\sigma (x) = \frac{1}{1+\textrm{e}^{-x}}$}}
\newglossaryentry{Rectified Linear-Funktion}{name=Rectified Linear-Funktion, description={Häufige Aktivierungsfunktion, definiert als: $f(x) = $ max$(0,x)$}}
\newglossaryentry{Rectified Linear Unit}{name=Rectified Linear Unit, description={Neuron, welches die Rectified Linear-Funktion als Aktivierungsfunktion verwendet}}
\newglossaryentry{Leaky Rectified Linear-Funktion}{name=Leaky Rectified Linear-Funktion, description={Häufige Aktivierungsfunktion, definiert als: $f(x) = \left\{\begin{array}{rcl}
			k \cdot x &x\leq0\\
			x&x>0
		\end{array}\right.$}}
\newglossaryentry{Input Layer}{name=Input Layer, description={Erste Layer eines KNN, nimmt die Eingabedaten an und übergibt diese dem ersten Hidden Layer}, parent=Layer}
\newglossaryentry{Output Layer}{name=Output Layer, description={Letzte Layer eines KNN; Neurone geben endültigen Output des KNN an}, parent=Layer}
\newglossaryentry{Hidden Layer}{name=Hidden Layer, description={Layer eines KNN, dessen Neurone nicht die finalen Ausgabewerte angeben}, parent=Layer}
\newglossaryentry{Long Short Term Memory (Neural) Network}{name={Long Short Term Memory (Neural) Network},description={Typ von RNNs, welche LSTM Zellen verwenden}}
\newglossaryentry{Modified National Institue of Standards and Technology (Datensatz)}{name={Modified National Institue of Standards and Technology (Datensatz)},description={Datensatz aus insgesamt 70.000 handgeschriebenen, in einer Auflösung von 28 mal 28 Pixel eingescannte Ziffern}}
\newglossaryentry{Algorithmus}{name=Algorithmus, description={Eine Sequenz von (rechnerischen) Verarbeitungsschritten, die eine Eingabe in eine Ausgabe transformiert}}
\newglossaryentry{fully connected}{name=fully connected, description={zwei (benachbarte) Layer sind fully connected, wenn alle Neurone eines Layers mit allen Neuronen des anderen verbunden sind}}
\newglossaryentry{Training}{name=Training, description={Anpassen der Parameter eines KNN für eine bestimmte Aufgabe mithilfe von Trainingsbeispielen}}
\newglossaryentry{Convolutional Layer}{name=Convolutional Layer, description={Typ von Layer, der Grundbestandteil eines CNNs ist; Neurone dieses Layers sind nur mit einem Ausschnitt, dem LRF, des vorherigen Layers verbunden}, parent=Layer}
\newglossaryentry{Pooling Layer}{name=Pooling Layer, description={Layer eines CNN, dessen Ziel es ist, kleinere Feature Maps als der vorhergehende Layer zu haben}, parent=Layer}
\newglossaryentry{Fully-connected Layer}{name=Fully-connected Layer, description={Layer, bei dem jedes Neuron mit jedem Neuron des vorherigen Layers verbunden ist}, parent=Layer}
\newglossaryentry{Softmax Layer}{name=Softmax Layer, description={Output Layer, dessen Ausgabe als Wahrscheinlichkeitsverteilung angesehen werden kann}, parent=Layer}
\newglossaryentry{LernenGlossar}{name=Lernen, description={Siehe: Training}}
\newglossaryentry{Local Receptive Field}{name=Local Receptive Field, description={Ausschnitt einer Input Matrix für ein Neuron eines Convolutionsl Layers}}
\newglossaryentry{Kernel}{name=Kernel, description={Matrix mit den Gewichten zwischen einem Neuron eines Convolutional Layer und seinem dazugehörigen LRF}}
\newglossaryentry{Stride}{name=Stride, description={Hyperparameter, welcher angibt, wie eine Input Matrix in die verschiedenen LRFs aufgeteilt werden soll}}
\newglossaryentry{Feature Map}{name=Feature Map, description={(meist) zweidimensionale Matrix von Neuronen eines Convolutional Layer}}
\newglossaryentry{Zero Padding}{name=Zero Padding, description={Hinzufügen von zusätzlichen, mit dem Wert 0 gefüllten Zeilen und Spalten um eine Input Matrix, um die Größe der Feature Maps des Convolutional Layers zu steuern}}
\newglossaryentry{Max-Pooling}{name=Max-Pooling, description={Mögliche Funktion eines Pooling Layer, die seinen Neuronen die arithemtischen Mittel ihrer jeweiligen Local Receptive Fields als Ausgabewert annehmen lässt}}
\newglossaryentry{Average-Pooling}{name=Average-Pooling, description={Mögliche Funktion eines Pooling Layer, die jedem seiner Neurone den größten Wert des jeweiligen Local Receptive Fields als Ausgabewert zuordnet}}
\newglossaryentry{Vanishing Gradient}{name=Vanishing Gradient, description={Problem beim Lernen von KNNs, bei dem der Gradient für die Parameter von Layern, die näher beim Input Layer liegen, sich dem Wert 0 nähert und das Lernen verlangsamt wird}}
\newglossaryentry{Many-To-One}{name=Many-To-One, description={Eine Sequenz von Eingaben erzeugt eine Ausgabe}, parent=Classification}
\newglossaryentry{One-To-Many}{name=One-To-Many, description={Eine Eingabe erzeut eine Sequenz von Ausgaben}, parent=Classification}
\newglossaryentry{Many-To-Many}{name=Many-To-Many, description={Eine Sequenz von Eingaben erzeugt eine Sequenz von Ausgaben}, parent=Classification}
\newglossaryentry{Gate}{name=Gate, description={Steuereinheiten zur Veränderung des Zell Status}}
\newglossaryentry{Gatter}{name=Gatter, description={Siehe: Gate}}
\newglossaryentry{Forget Gate}{name=Forget Gate, description={Gate zur Bestimmung, welche Informationen des vorherigen Zell Status erhalten bleiben sollen}, parent=Gate}
\newglossaryentry{Input Gate}{name=Input Gate, description={Gate zur Bestimmung, welche neuen Informationen in den aktuellen Zell Status aufgenommen werden sollen}, parent=Gate}
\newglossaryentry{Output Gate}{name=Output Gate, description={Gate zur Bestimmung, welche Informationen an die nächste Schicht und an den nächsten Zeitschritt weitergegeben werden sollen}, parent=Gate}
\newglossaryentry{Zell Status}{name=Zell Status, description={Variable eines LSTMs, welche Informationen über mehrere Zeitschritte hinweg speichern kann}}
\newglossaryentry{Output}{name=Output, description={Ausgabe eines KNN}}
\newglossaryentry{Trainingsdatensatz}{name=Trainingsdatensatz, description={Menge von Beispielen, deren Eingaben und Ausgaben bekannt sind, welche zum Trainieren eines KNN verwendet werden}}
\newglossaryentry{Cost-Funktion}{name=, description={Funktion, welche angibt, wie stark der Output eines KNN vom gewünschten Output der Trainingsbeispiele abweicht}}
\newglossaryentry{Loss-Funktion}{name=Loss-Funktion, description={Siehe: Cost-Funktion}}
\newglossaryentry{Gradient Descent}{name=Gradient Descent, description={Algorithmus zum Trainieren von KNNs}}
\newglossaryentry{Stochastic Gradient Descent}{name=Stochastic Gradient Descent, description={Gradient Descent, bei dem die Parameter des KNNs nach jedem Trainingsbeispiel verändert werden}}
\newglossaryentry{Epoch(e)}{name=Epoch, description={ein kompletter Durchlauf beim Lernen durch alle Trainingsbeispiele}}
\newglossaryentry{HyperparameterGlossar}{name=Hyperparameter, description={Parameter, die das Trainieren eines KNN steuern}}
\newglossaryentry{Gradient}{name=Gradient, description={Vektor mit partiellen Ableitungen}}
\newglossaryentry{Mini-Batch Gradient Descent}{name=Mini-Batch Gradient Descent, description={Gradient Descent, bei dem nach einer kleinen Teilmenge der Trainingsdaten die Parameter des KNNs verändert werden}, parent={Gradient Descent}}
\newglossaryentry{Mini-Batch}{name=Mini-Batch, description={Kleine Teilmenge von Trainingsdaten}}
\newglossaryentry{Testdatensatz}{name=Testdatensatz, description={Menge von Beispielen, deren Eingaben und Ausgaben bekannt sind, welche dazu verwendet werden, um zu überprüfen, wie gut ein KNN mit Daten umgehen kann, die es zuvor noch nie gesehen hat}}
\newglossaryentry{Validationsdatensatz}{name=Validationsdatensatz, description={Menge von Daten, deren Eingaben und Ausgaben bekannt sind, welche dazu dienen, um zu überprüfen, ob es beim Trainieren zu Overfitting kommt}}
\newglossaryentry{BackpropagationGlossar}{name=Backpropagation, description={Algorithmus zur Berechnung partieller Ableitungen bei mehrschichtigen KNNs}}
\newglossaryentry{Optimierungs-Methode}{name=Optimierungs-Methode, description={Methode zum Trainieren eines KNN (auch Optimieren der Parameter genannt)}}
\newglossaryentry{Underfitting}{name=Underfitting, description={Problem beim Lernen eines KNN; tritt ein, wenn das KNN sich an die Trainingsdaten nicht anpassen kann und eine Eingabe nicht in Verbindung mit der dazugehörigen Ausgabe setzen kann}}
\newglossaryentry{Overfitting}{name=Overfitting, description={Problem beim Lernen eines KNN; tritt ein, sobald die Trefferquote bei den Validationsdaten nicht weiter steigt, die Cost-Funktion aber weiterhin sinkt}}
\newglossaryentry{Dying ReLU}{name=Dying ReLU, description={Problem beim Lernen von KNNs mit ReLUs}}
\newglossaryentry{SaettigungGlossar}{name=Sättigung, description={Nähert sich eine Ableitung dem Wert 0, so sind die Veränderungen der Parameter relativ gering und das KNN benötigt zum Lernen mehr Zeit}}
\newglossaryentry{Trefferquote}{name=Trefferquote, description={Quotient von Anzahl der erkannten Ziffern einer Menge und Gesamtanzahl der Ziffern einer Menge}}
\newglossaryentry{Python}{name={Python},description={häufig im Bereich der KNNs eingesetzte Programmiersprache}}
\newglossaryentry{Library}{name={Library},description={Vorgefertigter Programmcode, auf den ein Programmierer zurückgreifen kann}}
\newglossaryentry{Keras}{name={Keras},description={Library zur Vereinfachung der Implementierung von KNNs mittels Tensorflow}}
\newglossaryentry{Tensorflow}{name={Tensorflow},description={Library von Google für C++/Python zur Implementierung von KNNs}}
\newglossaryentry{Grid Search}{name={Grid Search},description={Suche nach geeigneten Werten für Hyperparameter durch Iterieren durch eine/mehrere Liste/n von verschiedenen Werten}}
\newglossaryentry{CSV-Datei}{name={CSV-Datei},description={Datei mit einer Tabelle, derer Zellen durch z. B. Kommas getrennt sind}}
\newglossaryentry{GFLOPS}{name={GFLOPS},description={Milliarden Gleitkommaoperationen pro Sekunde; Einheit zur Messung der Rechengeschwindigkeit von Prozessoren}}
\newglossaryentry{Boxplot}{name={Boxplot},description={Graphische Darstellung des Minimums, 1., 2. und 3. Quartils und Maximum einer Liste}}
\newglossaryentry{Leistung}{name={Leistung},description={Trefferquote eines KNNs bei einem vollständigen Durchlauf aller Beispiele des Testdatensatzes}}
%======================================================================


%==============================ABKÜRZUNGSVERZEICHNIS===================
\newglossaryentry{KNNglossar}{type=Abk, name=KNN, description={Künstliches Neuronales Netz(werk)}}
\newglossaryentry{FFNNglossar}{type=Abk, name=FFNN, description={Feedforward Neural Network}}
\newglossaryentry{CNNglossar}{type=Abk, name=CNN, description={Convolutional Neural Network}}
\newglossaryentry{RNNglossar}{type=Abk, name=RNN, description={Recurrent Neural Network}}
\newglossaryentry{MNISTglossar}{type=Abk, name=MNIST, description={Modified National Institute of Standards and Technology (Dataset)}}
\newglossaryentry{CSVglossar}{type=Abk, name=CSV,description={Comma Separated Values}}
\newglossaryentry{ReLUglossar}{type=Abk, name=ReLU,description={Rectified Linear Unit}}
\newglossaryentry{LSTMglossar}{type=Abk, name=LSTM,description={Long Short Term Memory (Neural Network)}}
\newglossaryentry{LRFglossar}{type=Abk, name=LRF,description={Local Receptive Field}}
\newglossaryentry{GDglossar}{type=Abk, name=GD,description={Gradient Descent}}
\newglossaryentry{SGDglossar}{type=Abk, name=SGD,description={Stochastic Gradient Descent}}
\newglossaryentry{v. a.}{type=Abk, name={v. a.},description={vor allem}}
\newglossaryentry{E. D.}{type=Abk, name={E. D.},description={Eigene Darstellung}}
\newglossaryentry{GFLOPSabk}{type=Abk, name={GFLOPS},description={Giga Floating Point Operations per Second}}
\newglossaryentry{Aktf.}{type=Abk, name={Aktf.},description={Aktivierungsfunktion}}
\newglossaryentry{rek. Aktf.}{type=Abk, name={rek. Aktf.},description={rekurrente Aktivierungsfunktion}}
\newglossaryentry{Abb.}{type=Abk, name={Abb.},description={Abbildung}}
\newglossaryentry{z. B.}{type=Abk, name={z. B.}, description={zum Beispiel}}
%======================================================================

\glsaddall 




\begin{document}
	
	%\linenumbers %Für Zeilennummerierung zur Korrektur
	
	% Abstandsdefinitionen die später in Tabellen verwendet werden
	\newlength{\shiftdown}
	\setlength{\shiftdown}{\heightof{f}-\heightof{A}}
	\newlength{\myshiftdown}
	\setlength{\myshiftdown}{\heightof{f}-\heightof{A}+\heightof{A}}
	
	% Titelseite
	\begin{titlepage}\label{Titleseite}
		\vspace*{80mm}\Huge\centering\textbf{Künstliche Neuronale Netzwerke \newline und ihr Verhalten beim MNIST-Datensatz\break}
		\vspace{0mm}\hrulefill
		\setstretch{1}\vspace{7mm}\Large{\break Verfasser: Tobias Prisching, 8C 2018/19 \break Betreuer: Mag. Christoph Hödl}
		\vspace{15mm}\Large{\break BRG/BORG St. Pölten \break Schulring 16, 3100 St. Pölten}
		\vspace{70mm}\Large{\break Abgabe: Februar 2019}
	\end{titlepage}



	% Abstract
	\renewcommand{\abstractname}{Abstract}	
	\chapter*{Abstract}\label{Abstract}
		Die vorliegende Arbeit aus dem Bereich der Informatik beschäftigt sich mit Künstlichen Neuronalen Netzwerken, genauer den Feedforward Neural Networks, Convolutional Neural Networks und Long Short-Term Memory Neural Networks, und wie sich diese beim MNIST-Datensatz, einem bekannten Vergleichstest bestehend aus insgesamt 70.000 Ziffern, verhalten. Dabei werden in den ersten Kapiteln die Grundlagen für Künstliche Neuronale Netzwerke, die drei in der Arbeit behandelten Typen und der Lernprozess anhand des Feedforward Neural Networks erklärt. Im darauf folgenden Kapitel werden die Programmier-Experimente, welche mit der Sprache Python und der Library Keras erstellt wurden, mit den verschiedenen Netztypen und dem MNIST-Datensatz beschrieben und ihre Ergebnisse ausgewertet. Dabei stellt sich heraus, dass jeder Netztypus brauchbare Netze hervorgebracht hat. Des Weiteren werden die Zusammenhänge zwischen der Trefferquote eines KNN und den verschiedenen sogenannten Hyperparametern festgestellt, welche sich auch teilweise untereinander beeinflussen. 
		\thispagestyle{empty}
	
	
	
	% Vorwort
	\chapter*{Vorwort}\label{Vorwort}
		\addcontentsline{toc}{chapter}{Vorwort}
		Im Verlauf der letzten Monate wurde ich immer wieder von MitschülerInnen, Freund-Innen, Bekannten und Verwandten nach meinem VWA Thema gefragt. Prompt kam jedes Mal die auswendig gelernte Antwort zurück. Bis heute konnte noch keiner auf Anhieb etwas damit anfangen oder sich vorstellen, worum es dabei gehen könnte. Mit der Antwort „Ich versuche einem Computer beizubringen, einzelne Ziffern erkennen zu können“ konnten die meisten schon mehr anfangen, beließen es allerdings dabei. 
		\\ \
		\\ \
		Ich möchte mich im Folgenden bei allen Personen bedanken, ohne die diese Arbeit in ihrer jetzigen Form nie möglich gewesen wäre. Zuerst möchte ich mich bei meinem Betreuer Mag. Christoph Hödl für seine Unterstützung, Ratschläge und konstruktive Kritik bedanken. Auch möchte ich mich bei Herrn Mag. Scheibenpflug für seinen hilfreichen Unterricht im Wahlpflichtfach VWA und Frau Mag.$^{\textrm{a}}$ Gertrud Aumayr für die Beantwortung von Fragen bezüglich mathematischer Ausdrücke bedanken. Des Weiteren möchte ich mich bei meinen Eltern bedanken, welche mich bei meiner Arbeit unterstützt und diese auf mathematische, logische und Rechtschreib- sowie Grammatikfehler gegengelesen haben. 
		\\ \
		\\ \
		Noch lange werden mir die langen Sommernächte, welche bis 3 Uhr morgens mit der Arbeit an der VWA gefüllt waren, sowie das grausame Gefühl, ein Stück Information zu wissen, von dem man weiß, dass es sich irgendwo im Papierberg vor einem befindet, man es jedoch nicht finden kann, in Erinnerung bleiben. 
		\\ \ 
		\\ \
		% Zum Abschluss möchte ich noch ein Zitat von Adam Savage aus der us-amerikanischen TV-Serie „MythBusters“ nennen, an das ich immer wieder bei meinen Experimenten mit KNNs denken musste: „Remember kids, the only difference between science and screwing around is writing it down.“\footnote{?}
		St. Pölten, am 20. 01. 2019 \\ \
		Tobias Prisching
		\thispagestyle{empty}
		
		
	
	% Inhaltsverzeichnis
	\begingroup
		\renewcommand*{\chapterpagestyle}{empty}
		\pagestyle{empty}
		\tableofcontents
		\clearpage
	\endgroup
	
	
	
	% Einleitung
	\chapter{Einleitung}\label{Einleitung}
		Das Interesse in das Gebiet der Künstlichen Neuronalen Netzwerke ist in den vergangen Jahren stark gestiegen. Entwicklungen, die auf dieser Technologie beruhen, von automatischer Sprach- und Bilderkennung bis hin zum autonomen Fahren, sind bereits teilweise Realität. Und obwohl das Gebiet der Künstlichen Neuronalen Netzwerke schon über 50 Jahre alt ist, waren diese Entwicklungen vor noch zwei Jahrzehnten unvorstellbar, da viele der notwendigen Erkenntnisse erst um die Jahrtausendwende herum gewonnen wurden und die notwendige Rechenkapazität erst seit kurzer Zeit verfügbar ist.\footnote{\practitioner{1}} 
		\\ \
		\\ \
		Diese Arbeit beschäftigt sich mit den Grundlagen der Künstlichen Neuronalen Netzwerke, wie diese aufgebaut sind und funktionieren. Die Kapitel 3, 4 und 5 beschäftigen sich mit drei Arten von Netzwerken und im sechsten Kapitel wird beschrieben, wie diese lernen. Dieser Teil der Arbeit beruht rein auf Literatur, welche sowohl in gedruckter Form als auch digital im Internet zu finden ist. Da die behandelte Thematik erst seit relativ kurzer Zeit relevant und interessant ist, waren die meisten Werke erst seit nur wenige Monaten zur Zeit des Verfassens dieser Arbeit alt. In Kapitel 7 wird untersucht, wie sich verschiedene Netztypen beim MNIST-Datensatz, einem Vergleichstest für Künstliche Neuronale Netzwerke, verhalten und wie ein Netz beschaffen sein muss, um diese Aufgabe weitgehendst zu bewältigen. Um diese Fragen zu beantworten, werden zusätzlich zur Literatur auch Experimente in Form von Programmiertätigkeiten ausgewertet. 
		
		% Kapitel \ref{BausteineGrundlegendes} gibt einen Einstieg in die verschiedenen Grundlagen, welche für die weiteren Kapitel benötigt werden. Die Kapitel \ref{FFNN Kapitel} bis \ref{LSTM Kapitel} erklären die verschiedenen, in dieser Arbeit betrachteten Netztypen. Im Kapitel \ref{Lernen} wird der Prozess des Trainings eines KNNs erläutert. Die zu den verschiedenen Netztypen ausgeführten Experimente und ihre Ergebnisse werden in Kapitel \ref{Experimente} genannt. 
		
	
	
	\chapter{Bausteine und Grundlegendes zu Künstlichen Neuronalen Netzwerken}\label{BausteineGrundlegendes}
	
		\section{Grundlegendes zur Verwendung von Fachbegriffen und mathematischen \mbox{Ausdrücken in dieser Arbeit}}\label{Fachbegriffe&Mathe}
			Damit man über die verschiedenen Konzepte in dieser Arbeit schreiben kann, benötigt man Fachbegriffe, Terme und Gleichungen. Wie in anderen wissenschaftlichen Gebieten auch gibt es im Bereich der Künstlichen Neuronalen Netze keine standardisierte Schreibweise. 
		
		\subsection{Fachbegriffe}\label{Fachbegriffe}
			Da der Großteil der verwendeten Literatur in englischer Sprache verfasst ist, liegen auch sämtliche Fachbegriffe nur in dieser vor. Um mögliche Übersetzungsfehler und Differenzen zu anderen deutschen Werken zu verhindern, werden in dieser Arbeit hauptsächlich die englischen Fachbegriffe eingedeutscht. Das hat die Vorteile, dass einerseits der/die LeserIn sich in weiterführender Literatur besser zurecht findet, und andererseits, dass die Herleitungen der mathematischen Variablenbezeichnungen offensichtlich sind. Falls jedoch auch andere, deutsche Bezeichnungen vorkommen, werden diese bei Erstnennung des Begriffes ebenfalls erwähnt.
		
		\subsection{Mathematische Ausdrücke}\label{Mathe}
			Ebenfalls nicht einheitlich sind mathematische Ausdrücke in der Literatur. Häufig werden unterschiedliche Buchstaben, Nummerierungen und Indexierungen für die Variablen verwendet. In dieser Arbeit wird versucht, eine eigene Schreibweise zu verwenden, welche möglichst einfach zu verstehen ist, jedoch nicht die eleganteste oder kürzeste Ausdrucksweise ist. Sämtliche Parameter sind im \mbox{Anhang A} in einer Notationstabelle aufgelistet. 
		
		\section{Definition eines Künstlichen Neuronalen Netzes}\label{DefKNN}
			Um den Inhalt in den folgenden Kapiteln zu verstehen, ist eine Definition von Künstlichen Neuronalen Netzwerken, kurz KNN oder auch nur Neuronales Netz(werk), notwendig, da die verschiedenen Netztypen auf dieser Definition aufbauen. Ein KNN ist ein rechnerisches Modell, welches ein Netz bestehend aus miteinander verbundenen Knoten, auch künstliche Neurone genannt, dessen Aufbau lose an dem von biologischen Gehirnen orientiert ist, modelliert. Diese Neuronen können miteinander Signale über Verbindungen, den Weights (auch Gewichte genannt), welche die Strukturen aus den Informationen lernen, austauschen und sind in verschiedene Schichten, auch Layer genannt, eingeteilt. Die Weights werden in einem Lernprozess, auch als Training bezeichnet, so angepasst, dass das Netz in den ihm eingespeisten Informationen Strukturen erkennen kann.\footnote{vgl. Gurney, 1997, S. 1} Aus mathematischer Sicht lassen sich KNNs als komplexe Funktionen mit einigen wenigen bis zu Milliarden Parametern aufschreiben. Die einzige Grenze für die Komplexität und Größe dieser Funktion ist die verfügbare Rechenkapazität. 
			
		\section{Das künstliche Neuron}\label{DefKN}
			Das künstliche Neuron ist der Grundbaustein für alle in dieser Arbeit behandelten Arten von KNNs. Ein Neuron $n$ der Schicht $l$ lässt sich am einfachsten als eine mathematische Funktion beschreiben. Es nimmt die Ausgabewerte $x_{(l-1,1)}$, $x_{(l-1,2)}$, …, $x_{(l-1,m)}$ der Neurone der vorherigen Schicht $l-1$, multipliziert diese Werte mit Weights $w_{(l-1,1),(l,n)}$, $w_{(l-1,2),(l,n)}$, …, $w_{(l-1,m),(l,n)}$, summiert diese auf und addiert einen weiteren Parameter, genannt Bias, $b_{(l,n)}$. Diese Summe wird einer Aktivierungsfunktion $f_{(l)}$ übergeben, deren Wert der endgültige Ausgabewert dieses Neurons ist und an Neurone der Schicht $l+1$ weitergegeben werden kann. Mathematisch lässt sich dies in Formel \ref{FormelNeuron} ausdrücken.\footnote{vgl. Buduma, 2017, S. 8}
			\begin{equation}\label{FormelNeuron}
				x_{(l,n)} = f_{(l)} \left(\sum_{i=1}^{m}(w_{(l-1,i),(l,n)} \cdot x_{(l-1,i)}) + b_{(l,n)} \right) = f_{(l)}(z_{(l,n)})
			\end{equation}

			Diese Formel lässt sich auch so aufschreiben, dass man gleich einen Vektor mit allen Ausgabewerten aller Neurone der Schicht $l$ erhält (Siehe Formel \ref{FormelNeuronVektor}).\footnote{vgl. ebd., S. 8} Dabei ist $W_{(l-1),(l)}$ eine Matrix der Form $n\times m$, wobei $m$ die Anzahl der Neurone der Schicht $l-1$ und $n$ die Anzahl der Neurone in Schicht $l$ ist.\footnote{vgl. Rashid, 2017, S. 45-49}
			\begin{equation}\label{FormelNeuronVektor}
				\vec{x}_{(l)} = f_{(l)}(W_{(l-1),(l)} \cdot \vec{x}_{(l-1)} + \vec{b}_{(l)})  = f_{(l)}(\vec{z}_{(l)})
			\end{equation}
			
			% Grafik machen, ähnlich Practitioner S. 51 mit Bias. -> Wir schreiben hier kein Kinderbuch -> Antrag ABGELEHNT
			% Meistens skaliert auf Intervall, z.B. [0;1] oder [-1;1]
			% Falls =0 -> keine Verbindung zwischen Neuronen (BELEG ERFORDERLICH)
			
			Das Gewicht einer Eingabe gibt an, wie viel Aussagekraft bzw. wie wichtig der Wert eines Neurons ist. Der Bias eines Neurons lässt sich mit einem Schwellenwert vergleichen, welchen die gewichteten Eingaben überwinden müssen, damit dass Neuron aktiviert wird.\footnote{vgl. Nielsen, 2015, Kapitel 1/Perceptrons} Allerdings fehlt der Bias bei Netzen mancher Quellen, so z. B. bei Rashid.
			\ \\
			\ \\
			Neurone wie beim menschlichen Gehirn haben keine lineare Funktion für ihren Ausgabewert. Erst wenn ein bestimmter Schwellenwert erreicht ist, geben sie ein Ausgabesignal aus. Dieses Konzept der Nichtlinearität wird bei künstlichen Neuronen übernommen.\footnote{vgl. Rashid, 2017, S. 32} Diese Nichtlinearität ist wichtig, da diese es einem KNN erst ermöglicht, komplexere Aufgaben zu lösen.\footnote{vgl. Buduma, 2017, S. 13} Erreicht wird sie durch eine Aktivierungsfunktion, welche die Aktivierung eines Neurons steuert. Ein Neuron gilt dann als aktiviert, wenn sein Ausgabewert ungleich 0 ist.\footnote{vgl. Gibson \& Patterson, 2017, S. 53} Es gibt verschiedene Aktivierungsfunktionen, welche je nach Aufgabe des KNNs bzw. des Layers im KNN eingesetzt werden.\footnote{vgl. ebd., S. 255f} Innerhalb eines KNNs können die Layer unterschiedliche Aktivierungsfunktionen verwenden.\footnote{vgl. ebd., S. 50} Die Graphen von vier Funktionen sind in den Abbildungen \ref{SigmoidGraph} - \ref{LeakyReLUGraph} dargestellt, die Sigmoid-Funktion, die TanH-Funktion, die Rectified Linear-Funktion und die Leaky Rectified Linear-Funktion.\footnote{vgl. Buduma, 2017, S. 13ff} Die Sigmoid-Funktion ist in der Literatur eine der am häufigsten anzutreffenden Aktivierungsfunktionen, jedoch werden aufgrund eines Nachteils dieser Aktivierungsfunktion, dem Vanishing Gradient (Siehe \ref{VanishingGradient}), andere Funktionen verwendet. Auch die TanH-Funktion besitzt diesen Nachteil, die Rectified Linear-Funktion hat das Problem des "`Dying ReLU"' (Siehe \ref{DyingReLU}), weshalb die Leaky Rectified Linear-Funktion oft bevorzugt wird\footnote{Es gibt genaue, mathematische Begründungen, warum eine Aktivierungsfunktion besser ist als die andere, welche allerdings nicht zielführend zur Beantwortung der Forschungsfragen sind.}.\footnote{vgl. Gibson \& Patterson, 2017, S. 254}
		
			% Dieser Punkt wird daher auch nur kurz in Kapitel \ref{Lernen} angeschnitten, da es noch weitere Parameter gibt, welche den Erfolg eines KNN ausmachen.
			% Verschiedene Arten von Aktivierungsfunktionen -> ERLEDIGT
			% Notiz 3, ERLEDIGT
			% Innerhalb eines KNNs können verschiedene Aktivierungsfunktionen verwendet werden (BELEG ERFORDERLICH)	
			% Graphen von Heaviside, Sigmoid, TanH, ReLU	ERLEDIGT
			% Zu Sigmoid -> Probleme mit vanishing Gradient	ERLEDIGT (WIRD IN KAPITEL ZU BACKPROP ERKLÄRT)
			% Es gibt sher viel zu diesem Thema zu sagenm alleine was den Vergleich von Aktivierungsfunktionen angeht, wie LeCun in Efficient Backpropagation zeigt. Da diese Vergleiche nicht zeilführend für Forschungsfragen sind, werden diese auch (forerst) ausgelassen. 
		
			\newpage
		
			\begin{figure}[htb] % Vorlage: Practitioner S. 65-70
				\centering
				\begin{minipage}[t]{.48\linewidth}
					\centering
					\begin{tikzpicture}[scale=0.79]
						\begin{axis}[
							%title={$f(x) = \frac{1}{1 + \mathrm{e}^{-x}}$},
							xtick={-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 
							xticklabels={-10,,,,,-5,,,,,0,,,,,5,,,,,10},
							ytick={-1, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}, 
							yticklabels={-1.0,,,,,-0.5,,,,,0,,,,,0.5,,,,,1.0},
							x=10,
							y=100,
							ymin=-1.05, 	ymax=1.05, 
							xmin=-10.5, 	xmax=10.5, 
							axis lines=center, 
							hide obscured x ticks=false, 
							xlabel=$x$,
							ylabel={$f(x) = \sigma(x) = \frac{1}{1 + \mathrm{e}^{-x}}$},
							every inner x axis line/.append style={-},
							every inner y axis line/.append style={-},
							every axis x label/.style={at={(ticklabel* cs:0.99)},anchor=west,},
							every axis y label/.style={at={(ticklabel* cs:0.99)},anchor=south,},
							] 
							\addplot[domain=-10:10, samples=100, color=black, very thick]{1 / (1+exp(-x))}; 
						\end{axis};
					\end{tikzpicture}	
					\caption{Sigmoid-Funktion \\ (Quelle: E. D.)}\label{SigmoidGraph}
				\end{minipage}%
				\hfill%
				\begin{minipage}[t]{.48\linewidth}
					\centering
					\begin{tikzpicture}[scale=0.79]
						\begin{axis}[
							%title={$f(x) = \frac{1}{1 + \mathrm{e}^{-x}}$},
							xtick={-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 
							xticklabels={-10,,,,,-5,,,,,0,,,,,5,,,,,10},
							ytick={-1, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}, 
							yticklabels={-1.0,,,,,-0.5,,,,,0,,,,,0.5,,,,,1.0}, 
							x=10,
							y=100,
							ymin=-1.05, 	ymax=1.05, 
							xmin=-10.5, 	xmax=10.5, 
							axis lines=center, 
							hide obscured x ticks=false, 
							xlabel=$x$,
							ylabel={$f(x) = \textrm{tanh}(x) = \frac{\mathrm{e}^{x} - \mathrm{e}^{-x}}{\mathrm{e}^{x} + \mathrm{e}^{-x}}$},
							every inner x axis line/.append style={-},
							every inner y axis line/.append style={-},
							every axis x label/.style={at={(ticklabel* cs:0.99)},anchor=west,},
							every axis y label/.style={at={(ticklabel* cs:0.99)},anchor=south,},
							] 
							\addplot[domain=-10:10, samples=100, color=black, very thick]{tanh(x)}; 
						\end{axis}
					\end{tikzpicture}	
					\caption{TanH-Funktion (Quelle: E. D.)}\label{TanHGraph}
				\end{minipage}
				\hfill%
				\vspace*{0.7cm}
				\begin{minipage}[t]{.48\linewidth}
					\centering
					\begin{tikzpicture}[scale=0.79]
						\begin{axis}[
							%title={$f(x) = $ max$(0,x)$},
							xtick={-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 
							xticklabels={-10,,,,,-5,,,,,0,,,,,5,,,,,10},
							%ytick={-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 
							%yticklabels={-10,,,,,-5,,,,,0,,,,,5,,,,,10}, 
							ytick={-1, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}, 
							yticklabels={-1.0,,,,,-0.5,,,,,0,,,,,0.5,,,,,1.0}, 
							x=10,
							y=100, 
							ymin=-1.05, 	ymax=1.05, 
							xmin=-10.5, 	xmax=10.5, 
							axis lines=center, 
							hide obscured x ticks=false, 
							xlabel=$x$,
							ylabel={$f(x) = $ max$(0,x)$},
							every inner x axis line/.append style={-},
							every inner y axis line/.append style={-},
							every axis x label/.style={at={(ticklabel* cs:0.99)},anchor=west,},
							every axis y label/.style={at={(ticklabel* cs:0.99)},anchor=south,},
							] 
							\addplot[domain=-10:10, samples=1000, color=black, very thick]{max(0,x)}; 
						\end{axis}
					\end{tikzpicture}	
					\caption{Rectified Linear-Funktion \\ (Quelle: E. D.)}\label{ReLUGraph}
				\end{minipage}
				\hfill%
				\begin{minipage}[t]{.48\linewidth}
					\centering
					\begin{tikzpicture}[scale=0.79]
						\begin{axis}[
							%title={$f(x) = \left\{\begin{array}{rcl}
									%							0,01 \cdot x &x\leq0\\
									%							x&x>0
									%							\end{array}\right.$},
							xtick={-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 
							xticklabels={-10,,,,,-5,,,,,0,,,,,5,,,,,10},
							%ytick={-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 
							%yticklabels={-10,,,,,-5,,,,,0,,,,,5,,,,,10}, 
							ytick={-1, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}, 
							yticklabels={-1.0,,,,,-0.5,,,,,0,,,,,0.5,,,,,1.0}, 
							x=10,
							y=100, 
							ymin=-1.05, 	ymax=1.05, 
							xmin=-10.5, 	xmax=10.5, 
							axis lines=center, 
							hide obscured x ticks=false, 
							xlabel=$x$,
							ylabel={$f(x) = \left\{\begin{array}{rcl}
									k \cdot x &x\leq0\\
									x&x>0
								\end{array}\right.$},
							every inner x axis line/.append style={-},
							every inner y axis line/.append style={-},
							every axis x label/.style={at={(ticklabel* cs:0.99)},anchor=west,},
							every axis y label/.style={at={(ticklabel* cs:0.99)},anchor=south,},
							] 
							\addplot[domain=-10:10, samples=1000, color=black, very thick]{
								(x <= 0) * (0.01*x)   +
								(x > 0) * (x)}; 
						\end{axis}
					\end{tikzpicture}	
					\caption{Leaky Rectified Linear-Funktion, $k=0,01$ (Quelle: E. D.)}\label{LeakyReLUGraph}
				\end{minipage}
			\end{figure}


		\section{Schichten}
			Die Neuronen eines KNNs werden meistens in drei verschiedene Arten von Schichten gruppiert. Dabei sind die Werte der vorherigen Schicht die Eingabe für die nächste Schicht.\footnote{vgl. Gibson \& Patterson, 2017, S. 55f} Es gibt zwar Ausnahmen mit beispielsweise nur einer einzigen Schicht welche aber für diese Arbeit nicht relevant sind.\footnote{vgl. ebd., S. 48} Die in den folgenden Kapiteln behandelten Arten basieren auf folgender Einteilung, welche durch Abb. 5 visualisiert wird.

			\newpage
		
			% Erwähnen der Ausnahme eines 1-schichtigen KNN ERLEDIGT 
			% Abbildung tikz11 (Nielsen Kapitel 1) "Aufbau der einzelnen Schichten. Die Kreise symbolisieren Neurone. 
		
			% Notiz 67
			% Practitioner S. 87
		
			%Erwähnung, dass Informationen von einem Layer zum nächsten weitergeleitet werden
			
			
			\begin{wrapfigure}{r}{0cm}\label{EinBeispielFuerEinKNN}	
				% Vorlage: Nielsen/Kapitel 1/The architecture of neural networks
				\begin{tikzpicture}[scale=0.75, decoration={markings, mark=at position 0.75 with {\arrow{>}}}]
					\draw (0,0) circle (0.5cm);
					\draw (0,1.5) circle (0.5cm);
					\draw (0,3) circle (0.5cm);
					\draw (0,4.5) circle (0.5cm);
					\draw (0,6) circle (0.5cm);
					\draw (0,7.5) circle (0.5cm);
					\draw (0,9) circle (0.5cm);
					
					
					
					\draw (2.5,7.5) circle (0.5cm);
					\draw[postaction={decorate}] (0.5,9) 	-- (2,7.5);
					\draw[postaction={decorate}] (0.5,7.5) -- (2,7.5);
					\draw[postaction={decorate}] (0.5,6) 	-- (2,7.5);
					\draw[postaction={decorate}] (0.5,4.5) -- (2,7.5);
					\draw[postaction={decorate}] (0.5,3) 	-- (2,7.5);
					\draw[postaction={decorate}] (0.5,1.5) -- (2,7.5);
					\draw[postaction={decorate}] (0.5,0) 	-- (2,7.5);
					
					\draw (2.5,6) circle (0.5cm);
					\draw[postaction={decorate}] (0.5,9) 	-- (2,6);
					\draw[postaction={decorate}] (0.5,7.5) -- (2,6);
					\draw[postaction={decorate}] (0.5,6) 	-- (2,6);
					\draw[postaction={decorate}] (0.5,4.5) -- (2,6);
					\draw[postaction={decorate}] (0.5,3) 	-- (2,6);
					\draw[postaction={decorate}] (0.5,1.5) -- (2,6);
					\draw[postaction={decorate}] (0.5,0) 	-- (2,6);
					
					\draw (2.5,4.5) circle (0.5cm);
					\draw[postaction={decorate}] (0.5,9) 	-- (2,4.5);
					\draw[postaction={decorate}] (0.5,7.5) -- (2,4.5);
					\draw[postaction={decorate}] (0.5,6) 	-- (2,4.5);
					\draw[postaction={decorate}] (0.5,4.5) -- (2,4.5);
					\draw[postaction={decorate}] (0.5,3) 	-- (2,4.5);
					\draw[postaction={decorate}] (0.5,1.5) -- (2,4.5);
					\draw[postaction={decorate}] (0.5,0)	-- (2,4.5);
					
					\draw (2.5,3) circle (0.5cm);
					\draw[postaction={decorate}] (0.5,9) 	-- (2,3);
					\draw[postaction={decorate}] (0.5,7.5) -- (2,3);
					\draw[postaction={decorate}] (0.5,6) 	-- (2,3);
					\draw[postaction={decorate}] (0.5,4.5) -- (2,3);
					\draw[postaction={decorate}] (0.5,3) 	-- (2,3);
					\draw[postaction={decorate}] (0.5,1.5) -- (2,3);
					\draw[postaction={decorate}] (0.5,0) 	-- (2,3);
					
					\draw (2.5,1.5) circle (0.5cm);
					\draw[postaction={decorate}] (0.5,9) 	-- (2,1.5);
					\draw[postaction={decorate}] (0.5,7.5) -- (2,1.5);
					\draw[postaction={decorate}] (0.5,6) 	-- (2,1.5);
					\draw[postaction={decorate}] (0.5,4.5) -- (2,1.5);
					\draw[postaction={decorate}] (0.5,3) 	-- (2,1.5);
					\draw[postaction={decorate}] (0.5,1.5) -- (2,1.5);
					\draw[postaction={decorate}] (0.5,0)	-- (2,1.5);
					
					
					
					\draw (5,7.5) circle (0.5cm);
					\draw[postaction={decorate}] (3,7.5) 	-- (4.5,7.5);
					\draw[postaction={decorate}] (3,6)		-- (4.5,7.5);
					\draw[postaction={decorate}] (3,4.5) -- (4.5,7.5);
					\draw[postaction={decorate}] (3,3) 	-- (4.5,7.5);
					\draw[postaction={decorate}] (3,1.5) -- (4.5,7.5);
					
					\draw (5,6) circle (0.5cm);
					\draw[postaction={decorate}] (3,7.5) 	-- (4.5,6);
					\draw[postaction={decorate}] (3,6)		-- (4.5,6);
					\draw[postaction={decorate}] (3,4.5) -- (4.5,6);
					\draw[postaction={decorate}] (3,3) 	-- (4.5,6);
					\draw[postaction={decorate}] (3,1.5) -- (4.5,6);
					
					\draw (5,4.5) circle (0.5cm);
					\draw[postaction={decorate}] (3,7.5) 	-- (4.5,4.5);
					\draw[postaction={decorate}] (3,6)		-- (4.5,4.5);
					\draw[postaction={decorate}] (3,4.5) -- (4.5,4.5);
					\draw[postaction={decorate}] (3,3) 	-- (4.5,4.5);
					\draw[postaction={decorate}] (3,1.5) -- (4.5,4.5);
					
					\draw (5,3) circle (0.5cm);
					\draw[postaction={decorate}] (3,7.5) 	-- (4.5,3);
					\draw[postaction={decorate}] (3,6)		-- (4.5,3);
					\draw[postaction={decorate}] (3,4.5) -- (4.5,3);
					\draw[postaction={decorate}] (3,3) 	-- (4.5,3);
					\draw[postaction={decorate}] (3,1.5) -- (4.5,3);
					
					\draw (5,1.5) circle (0.5cm);
					\draw[postaction={decorate}] (3,7.5) 	-- (4.5,1.5);
					\draw[postaction={decorate}] (3,6)		-- (4.5,1.5);
					\draw[postaction={decorate}] (3,4.5) -- (4.5,1.5);
					\draw[postaction={decorate}] (3,3) 	-- (4.5,1.5);
					\draw[postaction={decorate}] (3,1.5) -- (4.5,1.5);
					
					
					\draw (7.5,7) circle (0.5cm);
					\draw[postaction={decorate}] (5.5,7.5) -- (7,7);
					\draw[postaction={decorate}] (5.5,6)	-- (7,7);
					\draw[postaction={decorate}] (5.5,4.5) -- (7,7);
					\draw[postaction={decorate}] (5.5,3) 	-- (7,7);
					\draw[postaction={decorate}] (5.5,1.5) -- (7,7);
					
					\draw (7.5,5.5) circle (0.5cm);
					\draw[postaction={decorate}] (5.5,7.5) -- (7,5.5);
					\draw[postaction={decorate}] (5.5,6)	-- (7,5.5);
					\draw[postaction={decorate}] (5.5,4.5) -- (7,5.5);
					\draw[postaction={decorate}] (5.5,3) 	-- (7,5.5);
					\draw[postaction={decorate}] (5.5,1.5) -- (7,5.5);
					
					\draw (7.5,4) circle (0.5cm);
					\draw[postaction={decorate}] (5.5,7.5) -- (7,4);
					\draw[postaction={decorate}] (5.5,6)	-- (7,4);
					\draw[postaction={decorate}] (5.5,4.5) -- (7,4);
					\draw[postaction={decorate}] (5.5,3) 	-- (7,4);
					\draw[postaction={decorate}] (5.5,1.5) -- (7,4);
					
					\draw (7.5,2.5) circle (0.5cm);
					\draw[postaction={decorate}] (5.5,7.5) -- (7,2.5);
					\draw[postaction={decorate}] (5.5,6)	-- (7,2.5);
					\draw[postaction={decorate}] (5.5,4.5) -- (7,2.5);
					\draw[postaction={decorate}] (5.5,3) 	-- (7,2.5);
					\draw[postaction={decorate}] (5.5,1.5) -- (7,2.5);
					
					
					\draw (10,6) circle (0.5cm);
					\draw[postaction={decorate}] (8,7) 	-- (9.5,6);
					\draw[postaction={decorate}] (8,5.5)	-- (9.5,6);
					\draw[postaction={decorate}] (8,4) 	-- (9.5,6);
					\draw[postaction={decorate}] (8,2.5) 	-- (9.5,6);
					
					\draw (10,4.5) circle (0.5cm);
					\draw[postaction={decorate}] (8,7) 	-- (9.5,4.5);
					\draw[postaction={decorate}] (8,5.5)	-- (9.5,4.5);
					\draw[postaction={decorate}] (8,4) 	-- (9.5,4.5);
					\draw[postaction={decorate}] (8,2.5) 	-- (9.5,4.5);
					
					\draw (10,3) circle (0.5cm);
					\draw[postaction={decorate}] (8,7) 	-- (9.5,3);
					\draw[postaction={decorate}] (8,5.5)	-- (9.5,3);
					\draw[postaction={decorate}] (8,4) 	-- (9.5,3);
					\draw[postaction={decorate}] (8,2.5) 	-- (9.5,3);
					
					\draw [decorate,decoration={brace,amplitude=5pt},xshift=0pt,yshift=0pt]
					(-0.5,-0.5) -- (-0.5,9.5) node [black,midway,xshift=-0.4cm,rotate=90] 
					{\footnotesize Input Layer};
					
					\draw [decorate,decoration={brace,amplitude=5pt},xshift=0pt,yshift=0pt]
					(2,8) -- (8,8) node [black,midway,yshift=0.4cm] 
					{\footnotesize Hidden Layers};
					
					\draw [decorate,decoration={brace,amplitude=5pt},xshift=0pt,yshift=0pt]
					(10.5,6.5) -- (10.5,2.5) node [black,midway,xshift=0.4cm,rotate=270] 
					{\footnotesize Output Layer};
				\end{tikzpicture}
				\caption{Ein Beispiel für ein KNN: Der Input Layer hat sieben Neurone, es gibt drei Hidden Layer mit je fünf, fünf und vier Neuronen. Der Output Layer hat drei Neurone. (Quelle: E. D.)}
			\end{wrapfigure}
			
			\subsection{Input Layer}
				Der Input Layer nimmt die dem KNN übergebenen Daten an und leitet diese an den ersten Hidden Layer weiter. Die Anzahl der Neurone in diesem Layer ist oft gleich der Anzahl der Daten einer Eingabe.\footnote{vgl. Gibson \& Patterson, 2017, S. 55} Werden einem KNN beispielsweise Bilder mit einer Auflösung von 28 mal 28 Pixeln übergeben, besteht der erste Layer aus 784 (28$^2$) Neuronen. Des Weiteren haben die Neurone des Input Layers keine Parameter und es wird keine Aktivierungsfunktion auf diese angewendet, da sie exakt jene Werte ausgeben sollen, welche dem Netz übergeben wurden. Dies hat keinen genauen Grund und hängt mit der Entwicklungsgeschichte von KNNs zusammen.\footnote{vgl. Rashid, 2017, S. 41}
				% Notiz 5
	
			\subsection{Hidden Layer}
				Jede in dieser Arbeit behandelte Art von KNNs besitzt mindestens einen oder mehr Hidden Layer. Diese Layer sind verantworlich für den Erfolg von KNNs in den letzten Jahren.\footnote{vgl. Gibson \& Patterson, 2017, S. 55} Der Name dieser Layer hat keine besondere Bedeutung und bedeutet nur, dass die Ausgabewerte ihrer Neurone nicht die finalen Ausgabewerte des Netzes sind.\footnote{vgl. Bengio, Courville \& Goodfellow, 2016, S. 165} Der Aufbau der Hidden Layer ist im Gegensatz zu denen der Input und Output Layer nicht so einfach zu entwickeln. Die Anzahl der Neurone in diesen Layern ist meistens durch die Art von Daten gegeben, zudem gibt es auch nur je einen Layer von beiden.\footnote{vgl. Nielsen, 2015, Kapitel 1/The architecture of neural networks}
				
			\subsection{Output Layer}\label{OutputLayer}
				Der Output Layer gibt die endgültige Antwort des KNNs aus, welche, je nach Aufgabe des Netzes (Regression\footnote{Regression modelliert den Zusammenhang zwischen Eingabe und Ausgabe und versucht, für eine gegebene Eingabe die Ausgabe zu ermitteln. (vgl. Gibson \& Patterson, 2017, S. 23)} oder Classification\footnote{Classification kategorisiert die Eingabe in zwei oder mehr Klassen. Bei zwei Klassen spricht man von Binary Classification. In diesem Fall hat der Output Layer ein Neuron, bei dem der Ausgabewert, welcher oft zwischen 0 und 1 liegt, mit einem Schwellenwert aufgeteilt wird. Für den Fall von $x$ Klassen ($x > 2$), Multiclass Classification genannt, gibt es $x$ Neurone. Die Klasse dessen entsprechendes Neuron den höchsten Wert hat ist die Antwort des KNN. (vgl. ebd., S. 25f)}), eine bestimmte Dimension hat. Abhängig von der in diesem Layer benutzten Aktivierungsfunktion und der Anzahl der Neuronen handelt es sich bei der Ausgabe meistens um entweder einen reellen Wert (Regression) oder einer (Menge von) Wahrscheinlichkeit(en) (Classification).\footnote{vgl. Gibson \& Patterson, 2017, S. 55 \& S. 95} Da der Schwerpunkt dieser Arbeit der MNIST-Datensatz ist (Siehe \ref{MNIST}) und es sich bei diesem um eine Multiclass Classifications-Aufgabe handelt, wird nur auf diese Kategorie von Aufgaben Rücksicht genommen. 
		
		
		\section{Arten von Künstlichen Neuronalen Netzwerken}\label{Arten}
			In den folgenden Kapiteln werden drei Netzwerktypen behandelt und erklärt. Ausgewählt wurden dafür das Feedforward Neural Network (FFNN), das Convolutional Neural Network (CNN) und das Long Short-Term Memory Neural Network (LSTM). Alle drei Netzwerktypen gehören zum Supervised Learning (engl. für überwachtes Lernen), d.h. sie lernen mithilfe von Trainingsdaten, bei denen Eingabe und Ausgabe gegeben sind.\footnote{vgl. Wartala, 2018, S. 23ff} Das FFNN wurde ausgewählt, da es im Vergleich zu anderen Netztypen sehr einfach aufgebaut ist, das CNN, weil es v. a. bei Bilderkennung sehr erfolgreich ist, und das LSTM, weil es durch die Rückkopplung von Daten interessant ist. \footnote{vgl. ebd., S. 26 \& S. 29}
			%, einem Teilgebiet des Deep Learning\footnote{Deep Learning ist ein Teilgebiet der Künstlichen Neuronalen Netzen, welche ein Teilgebiet von Machine Learning sind. (vgl. Wartala, 2018, S. 23) Zu Deep Learning zählen alle KNNs mit mehr als einem Hidden Layer. (vgl. Hurwitz \& Kirsch, 2018, S. 31) Unter Machine Learning versteht man das Erkennen von Strukturen in Beispielen von Daten durch Algorithmen. (vgl. Gibson \& Patterson, 2017, S. 2)}
			
			% In folgenden Kapiteln 3 verschiedene Arten erläutert; Leser soll wissen, 
			% dass es natürlich noch viel mehr gibt und hier nur Ausblick
			% Begründung warum gerade diese 3 Arten (MLP weil einfach, CNN wegen erfolge und RNN weil...?)
			% KEINE genau detaillierte Beschreibung des Stammbaums von KNNS
			% Alle Arten -> Supervised Learning
			% MLP weil (relativ) einfach
			% CNN weil tolle Erfolge in Bilderkennung
			% RNN/LSTM weil interessant bezüglich des Zeit Begriffs
			
		\section{Der MNIST-Datensatz}\label{MNIST}
			Der MNIST-Datensatz ist eine \textbf{m}odifizierte Version zweier Datensätze des \textbf{N}ational \textbf{I}nstitute of \textbf{S}tandards and \textbf{T}echnology der USA. Das Urheberrecht für den MNIST-Datensatz liegt bei Yann LeCun (Courant Institute, NYU) und  Corinna Cortes (Google Labs, New York), welche diesen unter der Creative Commons Attribution-Share Alike 3.0 Lizenz zum freien Gebrauch zur Verfügung stellen. Der Datensatz besteht aus zwei Teilen: Der erste Teil dient zum Trainieren des KNNs und besteht aus 60.000 Ziffern, welche von einer Gruppe, bestehend aus 250 Personen, handgeschrieben wurden. Diese Gruppe setzt sich zusammen aus 125 MitarbeiterInnen des US Census Bureau und 125 High School SchülerInnen. Der zweite Teil besteht aus 10.000 Ziffern, welche von einer zweiten Gruppe (Größe und Zusammensetzung gleich der ersten Gruppe) geschrieben wurden, um das KNN auf Daten zu testen, die es davor noch nie gesehen hat. Die handgeschriebenen Ziffern wurden mit einer Auflösung von 28 mal 28 Pixel in 256 Graustufen digitalisiert und in CSV-Dateien, welche die einzelnen Helligkeitswerte beinhalten, konvertiert.\footnote{vgl. Nielsen, 2015, Kapitel 1/Learning with gradient descent} Abb. 6 zeigt Beispiele für die Ziffern Null bis Neun aus dem zweiten Teil von MNIST. 
			
			%Die CSV-Dateien bestehen aus 785 Spalten, die Erste gibt an, um welche Ziffer es sich handelt, die 784 anderen geben die Helligkeitswerte der einzelnen Pixel Spalte für Spalte an. 
			%\footnote{vgl. Nielsen, 2015, Kapitel 1/Learning with gradient descent}
			
			\begin{figure}[h]\label{MNISTbeispiele}
				\vspace{0.0cm} \centering
				\includegraphics[height=1.3cm]{imagesAndGraphData/mnistBeispiele/4080.png}
				\includegraphics[height=1.3cm]{imagesAndGraphData/mnistBeispiele/6329.png}
				\includegraphics[height=1.3cm]{imagesAndGraphData/mnistBeispiele/922.png}
				\includegraphics[height=1.3cm]{imagesAndGraphData/mnistBeispiele/270.png}
				\includegraphics[height=1.3cm]{imagesAndGraphData/mnistBeispiele/56.png}
				\includegraphics[height=1.3cm]{imagesAndGraphData/mnistBeispiele/253.png}
				\includegraphics[height=1.3cm]{imagesAndGraphData/mnistBeispiele/21.png}
				\includegraphics[height=1.3cm]{imagesAndGraphData/mnistBeispiele/4225.png}
				\includegraphics[height=1.3cm]{imagesAndGraphData/mnistBeispiele/6275.png}
				\includegraphics[height=1.3cm]{imagesAndGraphData/mnistBeispiele/560.png}
				\caption{Beispiele für MNIST-Ziffern. Anmerkung: Alle Darstellungen von Ziffern des MNIST-Datensatzes wurden aus den durch die Library Keras (Siehe \ref{Python&Keras}) bereitgestellten Tabellen generiert. (Quelle: E. D.)}
			\end{figure}
		
	\chapter{Feedforward Neural Networks}\label{FFNN Kapitel}
		Das Feedforward Neural Network (auch Multilayer Perceptron Network\footnote{Diese Bezeichnung ist irreführend (und wird deshalb in dieser Arbeit auch nicht verwendet), da MLPs meistens nicht aus den im Namen stehenden Perceptrons, welche die Heaviside-Funktion als Aktivierungsfunktion verwenden, bestehen. (vgl. Nielsen, 2015, Kapitel 1/The architecture of neural networks)} oder Multilayer Feed-Forward Network genannt) ist trotz seiner Einfachheit ein schon relativ leistungsstarkes Netzwerk, da mit diesem jede stetige Funktion approximiert werden kann.\footnote{vgl. Wartala, 2018, S. 17; Gibson \& Patterson, 2017, S. 50}

		\section{Aufbau}
			Ein FFNN besteht aus einem Input- und einem Output-Layer und einem oder mehreren Hidden-Layern.\footnote{\practitioner{50}} Diese Layer sind bei diesem Netztypus fully connected (engl. für komplett verbunden), was bedeutet, dass jedes Neuron einer Schicht mit jedem Neuron der benachbarten Schichten verbunden ist.\footnote{\ebd{54}} Jeder Layer besteht aus einem oder mehreren Neuronen, wobei es nicht empfehlenswert ist, aufeinanderfolgende Layer mit gleicher Anzahl an Neuronen zu verwenden.\footnote{vgl. Gibson \& Patterson, 2017, S. 50; Buduma, 2017, S. 11} \\
			
			% Dies kann insofern ein Nachteil sein, da bei beispielsweise Bilderkennung mit steigender Auflösung die Anzahl Neuronen allein in der 1. Schicht quadratisch zunimmt.\footnote{\fundamentals{89}} 
			% Dies kann insofern ein Nachteil sein, da bei beispielsweise Bilderkennung mit durch Verdoppelung der Höhe und Breite eines Bildes die Anzahl der Neuronen alleine in der 1. Schicht (und somit auch die Anzahl der Parameter des 1. Hidden Layer) vervierfacht wird.\footnote{\fundamentals{89}}
	
			Des Weiteren ist der Begriff Feedforward in der Bezeichnung wichtig. Er bedeutet, dass Information nur in eine Richtung, nämlich vom Input- zum Output-Layer fließt. Dabei gibt es keine Verbindungen zwischen Neuronen im selben Layer oder in eine vorherige Schicht, da sich sonst Schleifen bilden, sodass der Input in ein Neuron von seinem Output abhängt. Solche Schleifen kommen bei LSTMs in Kapitel \ref{LSTM Kapitel} vor.\footnote{vgl. Buduma, 2017, S. 11; Nielsen, 2015, Kapitel 1/The architecture of neural networks}
	
	
			% Erwähnung von Softmax -> Auf CNN verschoben zu den ganzen anderen Arten von Layern. 																										Aufbau										
			% XOR Problem in Verbindung mit "jede stetige Funktion approximieren" 																														"Einleitung"								NICHT MEHR NÖTIG
			% Def von Feedforward: Fundamentals, S. 11																																					Definition/Aufbau							ERLEDIGT
			% Anzahl der Neurone sollte in Layern unterschiedlich sein: Fundamentals, S. 11																												Aufbau										ERLEDIGT
			% Problem von FFNNs bei größeren bildern -> Anzahl der Neurone in nimmt exponential zu: Fundamentals, S. 89																					"Einleitung"/Weiterführend (Kapitel 2.0)	ERLEDIGT
			% Mit FFNNs kann jede stetige Funktion approximiert werden: Wartala, S. 17	(Gegenteil zu Single Layer Perceptron)																			Einleitung									ERLEDIGT
			% Fully connected, siehe Practitioner, S. 54																																					Aufbau									ERLEDIGT
			% Hinweis, dass verschiedene Anzahl von Layern und Neurone Teil der Experimente ist																											Weiterführend								ERLEDIGT
			% Aufbau der Schichten eines MLP so wie in 1.4 beschrieben, wobei alle Layer fully connected sind; meistens Softmax für letzten Layer; 2 oder mehr Hidden layer -> deep learning				Aufbau										
			% Weiterführend zu Kapitel 1 ließe sich eine Funktion aufschreiben, welche den Output-Layer berechnet. -> Vielleicht auch noch was zum Input-Layer $x_(0)$									Funktionsweise								ERLEDIGT
			% Erwähnung, dass in späteren kapitel erklärt wird, wie ein KNN am Beispiel von diesem Netztyp trainiert wird. 																				Weiterführend 								ERLEDIGT	
			% Beweis, dass mehrere Aktivierungsfunktionen in einem KNN vorkommen können siehe Practitoner S. 50																							-> KAPITEL 1								ERLEDIGT
			
		\section{Funktionsweise}
			Bisher gab es nur eine Formel für die Berechnung der Werte eines Layers in Abhängigkeit der Ausgabewerte der vorherigen Schicht. Diese lässt sich durch Einsetzen in sich selbst zu Formel \ref{FormelGesamtesNetz} erweitern, sodass der Vektor des Output-Layers in Abhängigkeit des Input-Layers berechnet werden kann. 
			\begin{equation}\label{FormelGesamtesNetz}
				\vec{x}_{(L)} = f_{(L)} \left( W_{(L-1),(L)} \cdot f_{(L-1)} \left(…\cdot f_{(2)} \left(W_{(1),(2)}\cdot\vec{x}_{(1)}+\vec{b}_{(1)}\right)+…\right)+\vec{b}_{(L)}\right)
			\end{equation}
			
			Dies erklärt jedoch nicht, wie die einzelnen Parameter so angepasst werden, dass $\vec{x}_{(L)}$ ein brauchbares Ergebnis liefert. Dieser Vorgang wird Training oder auch Lernen genannt und in Kapitel \ref{Lernen} beschrieben. \footnote{\fundamentals{17; ebd., S. 5}}


	
	\chapter{Convolutional Neural Networks}\label{CNN Kapitel}
		KNNs, wie das FFNN, sind für Bilderkennung ungeeignet, da sich mit der Verdopplung der Auflösung entlang der Kanten die Anzahl der benötigten Weights vervierfacht. Handelt es sich dabei noch zusätzlich um Farbfotos, verdreifacht sich nochmal die Anzahl der Gewichte\footnote{Aufgrund der geringen Auflösung und fehlender Farbe sind FFNNs trotzdem gut für MNIST und ein Vergleich mit anderen Netzwerktypen geeignet. (\cnnKlein{3})}.\footnote{\fundamentals{89}} Je mehr Parameter ein KNN hat, umso mehr Rechenleistung benötigt es. Deshalb gibt es CNNs, welche auf die Klassifizierung von Bildern spezialisiert sind (und sich daher für MNIST sehr gut eignen).\footnote{vgl. Nielsen, 2018, Kapitel 6/Introducing convolutional networks}
		%(und nicht wie bei MNIST um Schwarz-Weiß-Bilder)
	
		% Fundamentals S. 89												ERLEDIGT
		% Introduction to CNNs (kurz), Seite 3, Absatz vor Overfitting		ERLEDIGT
		
		\section{Aufbau}
			Der Aufbau von CNNs basiert auf dem von FFNNs aus Kapitel \ref{FFNN Kapitel}. Die beiden größten Unterschiede zu FFNNs sind, dass erstens die Neurone mancher Layer in drei Dimensionen angeordnet sind, und zweitens nicht jedes Neuron mit allen Neuronen der benachbarten Schichten verbunden ist. Des Weiteren werden für unterschiedliche Verbindungen die gleichen Gewichte genutzt.\footnote{\cnnKlein{4 \& 7}}
			%, verändert allerdings auch manche Eigenschaften
		
			% Nicht immer jedes Neuron mit allen Neuronen der benachbarten Schchichten verbunden
			% Tensoren 3. Ordnung (oder gar höher!)
			% "Regeln" zum Strukturieren der Schichten / Wie sollte ein gutes CNN ausschauen?
			
		\section{Arten von Schichten}
			Ein CNN besteht grundsätzlich aus drei verschiedenen Arten von Schichten: Convolutional Layer, Pooling Layer, und Fully-Connected Layer. Außerdem wird eine häufig als Output Layer verwendete Art von Schicht betrachtet, der Softmax Layer\footnote{Dieser kommt auch in anderen Netztypen vor. (\practitioner{55})}.\footnote{\cnnKlein{4}}
		
			\subsection{Convolutional Layer}
				Besonders am Convolutional Layer ist, dass bei diesem die Neurone nicht mit allen Neuronen der vorherigen Schicht verbunden sind. Stattdessen ist jedes Neuron mit Neuronen eines Ausschnitts, welcher eine bestimmte Größe hat, der vorherigen Schicht verbunden, dem Local Receptive Field $R_{(l,d,n)}$, kurz LRF (Siehe Abb. \ref{LRF}). 
				\begin{wrapfigure}[13]{r}{0cm}
					%Vorlage: Nielsen/Kapitel 6/Introducing convolutional networks
					\begin{tikzpicture}[scale=0.5, decoration={markings, mark=at position 1 with {\arrow[scale=2]{>}}}]
						\draw[opacity=0.3] (0,0) circle (0.4cm);
						\draw[opacity=0.3] (0,1) circle (0.4cm);
						\draw (0,2) circle (0.4cm);
						\draw (0,3) circle (0.4cm);
						\draw (0,4) circle (0.4cm);
						
						\draw[opacity=0.3] (1,0) circle (0.4cm);
						\draw[opacity=0.3] (1,1) circle (0.4cm);
						\draw (1,2) circle (0.4cm);
						\draw (1,3) circle (0.4cm);
						\draw (1,4) circle (0.4cm);
						
						\draw[opacity=0.3] (2,0) circle (0.4cm);
						\draw[opacity=0.3] (2,1) circle (0.4cm);
						\draw (2,2) circle (0.4cm);
						\draw (2,3) circle (0.4cm);
						\draw (2,4) circle (0.4cm);
						
						\draw[opacity=0.3] (3,0) circle (0.4cm);
						\draw[opacity=0.3] (3,1) circle (0.4cm);
						\draw[opacity=0.3] (3,2) circle (0.4cm);
						\draw[opacity=0.3] (3,3) circle (0.4cm);
						\draw[opacity=0.3] (3,4) circle (0.4cm);
						
						\draw[opacity=0.3] (4,0) circle (0.4cm);
						\draw[opacity=0.3] (4,1) circle (0.4cm);
						\draw[opacity=0.3] (4,2) circle (0.4cm);
						\draw[opacity=0.3] (4,3) circle (0.4cm);
						\draw[opacity=0.3] (4,4) circle (0.4cm);
						
						
						\draw[opacity=0.3] (7,1) circle (0.4cm);
						\draw[opacity=0.3] (7,2) circle (0.4cm);
						\draw (7,3) circle (0.4cm);
						
						\draw[opacity=0.3] (8,1) circle (0.4cm);
						\draw[opacity=0.3] (8,2) circle (0.4cm);
						\draw[opacity=0.3] (8,3) circle (0.4cm);
						
						\draw[opacity=0.3] (9,1) circle (0.4cm);
						\draw[opacity=0.3] (9,2) circle (0.4cm);
						\draw[opacity=0.3] (9,3) circle (0.4cm);
						
						\draw (0,2) -- (7,3);
						\draw (0,3) -- (7,3);
						\draw (0,4) -- (7,3);
						
						\draw (1,2) -- (7,3);
						\draw[postaction={decorate}] (1,3) -- (7,3);
						\draw (1,4) -- (7,3);
						
						\draw (2,2) -- (7,3);
						\draw (2,3) -- (7,3);
						\draw (2,4) -- (7,3);
						
						\draw (-0.5,1.5) -- (-0.5,4.5) -- (2.5,4.5) -- (2.5,1.5) -- (-0.5,1.5);
						
						\draw [decorate,decoration={brace,amplitude=5pt},xshift=0pt,yshift=0pt] (-0.5,4.5) -- (2.5,4.5) node [black,midway,yshift=0.35cm,] {\footnotesize Local Receptive Field};
						\draw [decorate,decoration={brace,amplitude=5pt},xshift=0pt,yshift=0pt] (-0.5,6) -- (4.5,6) node [black,midway,yshift=0.35cm,] {\footnotesize Input};
						\draw [decorate,decoration={brace,amplitude=5pt},xshift=0pt,yshift=0pt] (6.5,3.5) -- (9.5,3.5) node [black,midway,yshift=0.35cm,] {\footnotesize Convolutional Layer};
						
					\end{tikzpicture}
					\caption{Visualisierung des Convolutional Layers. Es hilft, sich den Input Layer nicht wie bisher als Vektor, sondern als Matrix vorzustellen. (Quelle: E. D.)}\label{LRF}
				\end{wrapfigure}
				
				Die Gewichte zwischen den Neuronen des LRFs und dem zugehörigen Neuron des Convolutional Layers befinden sich in einer Matrix, dem Kernel $K_{(l,d)}$.\footnote{vgl. Nielsen, 2015, Kapitel 6/Introducing convolutional networks} Es wird das Skalarprodukt des Kernels und des zum Neuron zugehörige LRF gebildet, zu diesem wird noch ein Bias $b_{(l,d)}$ addiert. Auf die Summe wird wieder eine Aktivierungsfunktion $f_{(l)}$ angewandt.\footnote{\fundamentals{93}} Dieser Ausschnitt wird über den gesamten Input um einen Wert, den Stride, verschoben. Daraus entsteht eine sogenannte Feature Map, eine zweidimensionale Matrix bestehend aus Neuronen. Zu dieser können noch weitere Feature Maps hinzukommen, wodurch der Convolutional Layer dreidimensional wird. Dabei verwenden alle Neurone innerhalb einer Feature Map den gleichen Kernel und den gleichen Bias.\footnote{\cnnKlein{7}} Formel \ref{ConvolutionFormel} berechnet den Ausgabewert $x_{(l,d,n)}$ des Neurons $n$ in Feature Map $d$ von Layer $l$.\footnote{vgl. Nielsen, 2015, Kapitel 6/Introducing convolutional networks}
		
				\begin{equation}\label{ConvolutionFormel}
					x_{(l,d,n)} = f_{(l)} \left( \langle K_{(l,d)},R_{(l,d,n)} \rangle_F + b_{(l,d)} \right)
				\end{equation}
		
				\begin{figure}[htb]
					\centering
					\begin{minipage}[t]{.6\linewidth}
						\centering
						%Vorlage: Nielsen/Kapitel 6/Introducing convolutional networks
						\begin{tikzpicture}[scale=0.45, decoration={markings, mark=at position 1 with {\arrow[scale=2]{>}}}]
							\draw (0,0) circle (0.4cm);
							\draw (0,1) circle (0.4cm);
							\draw (0,2) circle (0.4cm);
							\draw (0,3) circle (0.4cm);
							\draw (0,4) circle (0.4cm);
							
							\draw (1,0) circle (0.4cm);
							\draw (1,1) circle (0.4cm);
							\draw (1,2) circle (0.4cm);
							\draw (1,3) circle (0.4cm);
							\draw (1,4) circle (0.4cm);
							
							\draw[line width=0.5mm] (2,2) circle (0.05cm);
							\draw (2,3) circle (0.4cm);
							\draw (2,4) circle (0.4cm);
							
							\draw[line width=0.5mm] (3,1) circle (0.05cm);
							\draw (3,3) circle (0.4cm);
							\draw (3,4) circle (0.4cm);
							
							\draw[line width=0.5mm] (4,0) circle (0.05cm);
							\draw (4,3) circle (0.4cm);
							\draw (4,4) circle (0.4cm);
							
							
							
							\draw (7,0) circle (0.4cm);
							\draw (7,1) circle (0.4cm);
							\draw (7,2) circle (0.4cm);
							\draw (7,3) circle (0.4cm);
							\draw (7,4) circle (0.4cm);
							
							\draw (8,0) circle (0.4cm);
							\draw (8,1) circle (0.4cm);
							\draw (8,2) circle (0.4cm);
							\draw (8,3) circle (0.4cm);
							\draw (8,4) circle (0.4cm);
							
							\draw[line width=0.5mm] (9,2) circle (0.05cm);
							\draw (9,3) circle (0.4cm);
							\draw (9,4) circle (0.4cm);
							
							\draw[line width=0.5mm] (10,1) circle (0.05cm);
							\draw (10,3) circle (0.4cm);
							\draw (10,4) circle (0.4cm);
							
							\draw[line width=0.5mm] (11,0) circle (0.05cm);
							\draw (11,3) circle (0.4cm);
							\draw (11,4) circle (0.4cm);
							
							
							
							\draw (14,0) circle (0.4cm);
							\draw (14,1) circle (0.4cm);
							\draw (14,2) circle (0.4cm);
							\draw (14,3) circle (0.4cm);
							\draw (14,4) circle (0.4cm);
							
							\draw (15,0) circle (0.4cm);
							\draw (15,1) circle (0.4cm);
							\draw (15,2) circle (0.4cm);
							\draw (15,3) circle (0.4cm);
							\draw (15,4) circle (0.4cm);
							
							\draw[line width=0.5mm] (16,2) circle (0.05cm);
							\draw (16,3) circle (0.4cm);
							\draw (16,4) circle (0.4cm);
							
							\draw[line width=0.5mm] (17,1) circle (0.05cm);
							\draw (17,3) circle (0.4cm);
							\draw (17,4) circle (0.4cm);
							
							\draw[line width=0.5mm] (18,0) circle (0.05cm);
							\draw (18,3) circle (0.4cm);
							\draw (18,4) circle (0.4cm);
							
							
							\draw (-0.5,2.5) -- (-0.5,4.5) -- (1.5,4.5) -- (1.5,2.5) -- (-0.5,2.5);
							
							\draw (-0.5,1.5) -- (-0.5,3.5) -- (1.5,3.5) -- (1.5,1.5) -- (-0.5,1.5);
							\draw (-0.5,0.5) -- (-0.5,2.5) -- (1.5,2.5) -- (1.5,0.5) -- (-0.5,0.5);
							\draw (-0.5,-0.5) -- (-0.5,1.5) -- (1.5,1.5) -- (1.5,-0.5) -- (-0.5,-0.5);
							
							\draw (0.5,2.5) -- (0.5,4.5) -- (2.5,4.5) -- (2.5,2.5) -- (0.5,2.5);
							\draw (1.5,2.5) -- (1.5,4.5) -- (3.5,4.5) -- (3.5,2.5) -- (1.5,2.5);
							\draw (2.5,2.5) -- (2.5,4.5) -- (4.5,4.5) -- (4.5,2.5) -- (2.5,2.5);
							
							
							
							\draw (6.5,2.5) -- (6.5,4.5) -- (8.5,4.5) -- (8.5,2.5) -- (6.5,2.5);
							
							\draw (6.5,1.5) -- (6.5,3.5) -- (8.5,3.5) -- (8.5,1.5) -- (6.5,1.5);
							\draw (6.5,0.5) -- (6.5,2.5) -- (8.5,2.5) -- (8.5,0.5) -- (6.5,0.5);
							\draw (6.5,-0.5) -- (6.5,1.5) -- (8.5,1.5) -- (8.5,-0.5) -- (6.5,-0.5);
							
							\draw (9.5,2.5) -- (9.5,4.5) -- (11.5,4.5) -- (11.5,2.5) -- (9.5,2.5);
							
							
							
							\draw (13.5,2.5) -- (13.5,4.5) -- (15.5,4.5) -- (15.5,2.5) -- (13.5,2.5);
							
							\draw (13.5,-0.5) -- (13.5,1.5) -- (15.5,1.5) -- (15.5,-0.5) -- (13.5,-0.5);
							
							\draw (16.5,2.5) -- (16.5,4.5) -- (18.5,4.5) -- (18.5,2.5) -- (16.5,2.5);
							
						\end{tikzpicture}
						\caption{Visualisierung verschiedener Stride-Werte - Links: $s_{(l)} = (1,1)$; Mitte: $s_{(l)} = (3,1)$; Rechts: $s_{(l)} = (3,3)$ (Quelle: E. D.)}\label{StrideAbbildung}
		
					\end{minipage}
					\hfill
					\begin{minipage}[t]{.3\linewidth}
						\centering
						% Vorlage: Nielsen/Kapitel 6/Introducing convolutional networks
						\begin{tikzpicture}[scale=0.45, decoration={markings, mark=at position 1 with {\arrow[scale=2]{>}}}]
							\draw (2,1) circle (0.4cm) node [black,midway,xshift=2cm,yshift=1cm,] {1};
							\draw (2,2) circle (0.4cm) node [black,midway,xshift=2cm,yshift=2cm,] {2};
							\draw (2,3) circle (0.4cm) node [black,midway,xshift=2cm,yshift=3cm,] {1};
							
							\draw (3,1) circle (0.4cm) node [black,midway,xshift=3cm,yshift=1cm,] {3};
							\draw (3,2) circle (0.4cm) node [black,midway,xshift=3cm,yshift=2cm,] {0};
							\draw (3,3) circle (0.4cm) node [black,midway,xshift=3cm,yshift=3cm,] {2};
							
							\draw (4,1) circle (0.4cm) node [black,midway,xshift=4cm,yshift=1cm,] {2};
							\draw (4,2) circle (0.4cm) node [black,midway,xshift=4cm,yshift=2cm,] {1};
							\draw (4,3) circle (0.4cm) node [black,midway,xshift=4cm,yshift=3cm,] {3};
							
							
							\draw (7,0) circle (0.4cm) node [black,midway,xshift=7cm,yshift=0cm,] {0};
							\draw (7,1) circle (0.4cm) node [black,midway,xshift=7cm,yshift=1cm,] {0};
							\draw (7,2) circle (0.4cm) node [black,midway,xshift=7cm,yshift=2cm,] {0};
							\draw (7,3) circle (0.4cm) node [black,midway,xshift=7cm,yshift=3cm,] {0};
							\draw (7,4) circle (0.4cm) node [black,midway,xshift=7cm,yshift=4cm,] {0};
							
							\draw (8,0) circle (0.4cm) node [black,midway,xshift=8cm,yshift=0cm,] {0};
							\draw (8,1) circle (0.4cm) node [black,midway,xshift=8cm,yshift=1cm,] {1};
							\draw (8,2) circle (0.4cm) node [black,midway,xshift=8cm,yshift=2cm,] {2};
							\draw (8,3) circle (0.4cm) node [black,midway,xshift=8cm,yshift=3cm,] {1};
							\draw (8,4) circle (0.4cm) node [black,midway,xshift=8cm,yshift=4cm,] {0};
							
							\draw (9,0) circle (0.4cm) node [black,midway,xshift=9cm,yshift=0cm,] {0};
							\draw (9,1) circle (0.4cm) node [black,midway,xshift=9cm,yshift=1cm,] {3};
							\draw (9,2) circle (0.4cm) node [black,midway,xshift=9cm,yshift=2cm,] {0};
							\draw (9,3) circle (0.4cm) node [black,midway,xshift=9cm,yshift=3cm,] {2};
							\draw (9,4) circle (0.4cm) node [black,midway,xshift=9cm,yshift=4cm,] {0};
							
							\draw (10,0) circle (0.4cm) node [black,midway,xshift=10cm,yshift=0cm,] {0};
							\draw (10,1) circle (0.4cm) node [black,midway,xshift=10cm,yshift=1cm,] {2};
							\draw (10,2) circle (0.4cm) node [black,midway,xshift=10cm,yshift=2cm,] {1};
							\draw (10,3) circle (0.4cm) node [black,midway,xshift=10cm,yshift=3cm,] {3};
							\draw (10,4) circle (0.4cm) node [black,midway,xshift=10cm,yshift=4cm,] {0};
							
							\draw (11,0) circle (0.4cm) node [black,midway,xshift=11cm,yshift=0cm,] {0};
							\draw (11,1) circle (0.4cm) node [black,midway,xshift=11cm,yshift=1cm,] {0};
							\draw (11,2) circle (0.4cm) node [black,midway,xshift=11cm,yshift=2cm,] {0};
							\draw (11,3) circle (0.4cm) node [black,midway,xshift=11cm,yshift=3cm,] {0};
							\draw (11,4) circle (0.4cm) node [black,midway,xshift=11cm,yshift=4cm,] {0};
						\end{tikzpicture}
						\caption{Beispiel für unterschiedliche Zero Padding Werte - Links: $p_{(l)}=0$; Rechts: $p_{(l)}=1$ (Quelle: E. D.)}\label{PaddingAbbildung}
						
					\end{minipage}
				\end{figure}
		
				\enlargethispage{0.2\baselineskip}
		
				Der Stride $s_{(l)}$ bestimmt, wie sehr sich die einzelnen LRFs überlappen (Siehe Abb. \ref{StrideAbbildung}). Stride und Größe der LRFs beeinflussen die Größe der Feature Maps.\footnote{\cnnKlein{7}} Diese lässt sich zusätzlich durch den Wert $p_{(l)}$ des Zero Paddings steuern. Indem um die Input Matrix herum Zeilen und Spalten hinzugefügt werden (Siehe Abb. \ref{PaddingAbbildung}), können die Feature Maps vergrößert werden, was v. a. dann angewendet wird, wenn die Feature Maps die gleiche Größe wie die vorherige Schicht haben sollen. Diese Zeilen und Spalten werden meistens mit dem Wert 0 gefüllt.\footnote{vgl. Wu, 2017, S. 12f}
				% (ohne Zero Padding)
		
				Die Dimensionen des Convolutional Layer lassen sich wie folgt berechnen: Hat $X_{(l-1)}$ die Form $\textrm{H}({X_{(l-1)}}) \times \textrm{W}({X_{(l-1)}}) \times \textrm{D}({X_{(l-1)}})$ (der vorangehende Layer kann auch dreidimensional sein, wenn es sich dabei z.B. ebenfalls um einen Convolutional Layer handelt) und $K_{(l)}$ die Form $\textrm{H}({K_{(l)}}) \times \textrm{W}({K_{(l)}}) \times \textrm{D}({X_{(l-1)}}) \times \textrm{D}({K_{(l)}})$ (wobei $K_{(l)}$ die Menge aller Kernel des Layer $l$ und $\textrm{D}({K_{(l)}})$ die Anzahl aller Kernel ist), dann hat $X_{(l)}$ die folgende Form:\footnote{vgl. Wu, 2017, S. 12f; Nash \& O'Shea, 2015, S.7}
				\begin{equation}
					\frac{ \textrm{H}(X_{(l-1)}) - \textrm{H}(K_{(l)}) + 2 \cdot p_{(l)}}{\textrm{H}(s_{(l)})+1} \times \frac{ \textrm{W}(X_{(l-1)}) - \textrm{W}(K_{(l)}) + 2 \cdot p_{(l)}}{\textrm{W}(s_{(l)})+1} \times \textrm{D}({K_{(l)}})
				\end{equation}
		
				% Für jedes Neuron des Convolutional Layers wird dieser Ausschnitt über den vorherigen Layer verschoben. Um wie viel dieser Ausschnitt verschoben wird, legt der Stride-Wert fest.  Größe des Ausschnitts und Stride sind ebenefalls Hyperparameter (?).																	ERLEDIGT
				% Zusätzlich kommt auch noch die Tiefe des Convolutional Layer												ERLEDIGT
				% Die Werte einer Tiefe d des Convolutional Layers werden auch als Feature Map bezeichnet. 					ERLEDIGT
				% Alle Neurone innerhalb einer Feature Map eines Layers haben den gleichen Bias! (Fundamentals, S. 93)		ERLEDIGT
				% Zero Padding																								ERLEDIGT
				% Formel zur Berechnung der Form der Feature Map															
				% Formel zur Berechnung der Werte einer Featur Map
				% Diagramm mit stride = (1,1), (2,1), (2,2)																	ERLEDIGT
				
				
			\subsection{Pooling Layer}
				Das Ziel eines Pooling Layers ist es, die Größe der Feature Maps zu verkleinern, wodurch gleichzeitig die Komplexität des Models verringert wird. Ähnlich wie die Convolutional Layer haben auch die Pooling Layer drei Dimensionen und ein LRF mit Stride Wert, allerdings keine Kernel oder Biases. Auf das LRF wird eine Pooling-Funktion, wie z. B. Max-Pooling oder Average-Pooling angewandt. Im Fall von Max-Pooling erhält das Neuron des Pooling Layers den höchsten Wert innerhalb des LRF, bei Average-Pooling das arithmetische Mittel der Neurone im LRF.\footnote{\cnnKlein{8}}
				
			\subsection{Fully-Connected Layer}
				Die Fully-Connected Layer sind gleich den Layern eines FFNN. Jedes ihrer Neurone ist mit jedem Neuron der vorherigen Schicht verbunden und hat seinen eigenen Bias, jede Verbindung hat ihr eigenes Gewicht.\footnote{\ebd{8}}
				
			\subsection{Softmax Layer}
				Der Softmax Layer ist ein Output Layer, der sich bis auf die Aktivierungsfunktion von bisherigen Output Layern nicht unterscheidet und eine besondere Eigenschaft hat: Die Summe der Ausgabewerte aller Neurone dieses Layers hat den Wert 1. Dies ist insofern sinnvoll, da die Ausgabewerte als eine Wahrscheinlichkeitsverteilung betrachten werden können. Deshalb wird der Softmax Layer v. a. bei Classification-Aufgaben als Output Layer eingesetzt, da der Wert $x_{(L,n)}$ als Wahrscheinlichkeit dafür betrachtet werden kann, dass die korrekte Klasse jene des Neurons $n$ ist.~Formel \ref{SoftmaxFormel} gibt die Berechnung der Aktivierung eines Neurons eines Softmax Layers an.\footnote{vgl. Nielsen, 2015, Kapitel 3/Softmax}
				\vspace*{-0.3cm}
				\begin{equation}\label{SoftmaxFormel}
					x_{(L,n)} = \frac{ \textrm{e}^{ z_{(L,n)} } }{ \sum\limits_{ n=1 }^{ \textrm{len}(L) } \left( \textrm{e}^{ z_{ (L,n) } } \right)} 
				\end{equation}
				
		\section{Anordnung der Schichten}
			Es gibt keine genauen Regeln, wie die verschiedenen Layer kombiniert werden sollen, allerdings kann man nicht einfach ein paar Schichten miteinander kombinieren und brauchbare Ergebnisse erwarten. Es gibt jedoch Reihenfolgen für die Layer, welche sich in der Literatur durchgesetzt haben. So folgt meistens auf einen Convolutional Layer ein Pooling Layer. Diese Abwechslung zwischen den beiden kann mehrmals wiederholt werden, bevor zum Schluss ein oder mehrere Fully-Connected Layer folgen. Des Weiteren hat es sich bewährt, mehrere Convolutional Layer vor einen Pooling Layer zu stapeln, da dadurch die Komplexität des Modells gesteigert werden kann.\footnote{\cnnKlein{8f}}
			% Erklären, dass z. B. ein CNN oft mit C Layer beginnt, und auf einen oder mehreren C Layer ein P Layer folgt. 		ERLEDIGT



	\chapter{Long Short-Term Memory Neural Networks}\label{LSTM Kapitel}
		Bisher wurden in der Arbeit nur KNNs betrachtet, welche für viele Aufgaben geeignet sind, jedoch die einzelnen Eingaben voneinander getrennt behandeln. Es gibt allerdings Aufgabenstellungen, bei denen die Eingaben in einem Zusammenhang stehen, wie z. B. bei der Klassifizierung, ob die Aussage eines Satzes positiv oder negativ ist\footnote{"`Many-To-One"': Eine Sequenz von Eingaben (Wörter) erzeugt eine Ausgabe (positive/negative Aussage).}, die Beschreibung eines Bildes\footnote{"`One-To-Many"': Eine Eingabe (Bild) erzeugt eine Sequenz von Ausgaben (mehrere Wörter, die das Bild beschreiben).} oder die Übersetzung von Sprachen\footnote{"'Many-To-Many"': Eine Sequenz von Eingaben (Wörter einer Sprache) erzeugt eine Sequenz von Ausgaben (Wörter einer anderen Sprache).}.\footnote{vgl. Gupta, 2017} Dafür gibt es die Recurrent Neural Networks (kurz RNNs), welche die Besonderheit haben, dass sie zusätzlich die zeitliche Dimension, in der die Daten eingegeben werden, modellieren, wodurch Eingabedaten die Ausgaben späterer Eingabedaten beeinflussen können\footnote{Da diese Modelle mehrere Eingaben pro Trainingsbeispiel benötigen, werden im Beispiel von MNIST die einzelnen Spalten eines Bildes dem Netz übergeben.}. Dies wird erreicht indem die Ausgabe eines Neurons diesem als zusätzliche Eingabe zu einem späteren Zeitpunkt übergeben wird.\footnote{\practitioner{143-146; Gupta, 2017}} \\
		
		\begin{wrapfigure}[7]{r}{0cm}	
			%Vorlage: Olah
			\begin{tikzpicture}[scale=0.45, decoration={markings, mark=at position 1 with {\arrow[scale=2]{>}}}]
				\draw[fill=white] (1.8,0) circle (0.44cm);
				\draw[fill=white] (1.5,0) circle (0.45cm);
				\draw[fill=white] (1.2,0) circle (0.46cm);
				\draw[fill=white] (0.9,0) circle (0.47cm);
				\draw[fill=white] (0.6,0) circle (0.48cm);
				\draw[fill=white] (0.3,0) circle (0.49cm);
				\draw[fill=white] (0,0) circle (0.5cm);
				
				\draw (-0.6,-0.6) -- (-0.6,0.6) -- (2.4,0.6) -- (2.4,-0.6) -- (-0.6,-0.6);
				\draw[postaction={decorate}] (0.9,0.6) -- (0.9,1.4);
				
				
				\draw[fill=white] (1.5,2) circle (0.46cm);
				\draw[fill=white] (1.2,2) circle (0.47cm);
				\draw[fill=white] (0.9,2) circle (0.48cm);
				\draw[fill=white] (0.6,2) circle (0.49cm);
				\draw[fill=white] (0.3,2) circle (0.5cm);
				
				\draw (-0.3,1.4) -- (-0.3,2.6) -- (2.1,2.6) -- (2.1,1.4) -- (-0.3,1.4);
				\draw[postaction={decorate}] (0.9,2.6) -- (0.9,3.4);
				
				\draw (2.1,2) -- (2.5,2);
				\draw (2.5,2) -- (2.5,3);
				\draw (2.5,3) -- (1,3);
				\draw (0.8,3) -- (-0.7,3);
				\draw (-0.7,3) -- (-0.7,2);
				\draw[postaction={decorate}] (-0.7,2) -- (-0.3,2);
				
				
				\draw[fill=white] (1.2,4) circle (0.48cm);
				\draw[fill=white] (0.9,4) circle (0.49cm);
				\draw[fill=white] (0.6,4) circle (0.5cm);
				
				\draw (0,3.4) -- (0,4.6) -- (1.8,4.6) -- (1.8,3.4) -- (0,3.4);
				
				
				
				\draw node[black,midway,yshift=2cm,xshift=4cm] at (0,0) {\Huge $=$};
				\draw node[black,midway,yshift=2cm,xshift=15cm] at (0,0) {\Huge …};
				\draw node[black,midway,yshift=-1.4cm,xshift=6.9cm] at (0,0) {\LARGE $t=1$};
				\draw node[black,midway,yshift=-1.4cm,xshift=10.9cm] at (0,0) {\LARGE $t=2$};
				
				
				
				
				
				
				\draw[fill=white] (1.8+6,0) circle (0.44cm);
				\draw[fill=white] (1.5+6,0) circle (0.45cm);
				\draw[fill=white] (1.2+6,0) circle (0.46cm);
				\draw[fill=white] (0.9+6,0) circle (0.47cm);
				\draw[fill=white] (0.6+6,0) circle (0.48cm);
				\draw[fill=white] (0.3+6,0) circle (0.49cm);
				\draw[fill=white] (0+6,0) circle (0.5cm);
				
				\draw (-0.6+6,-0.6) -- (-0.6+6,0.6) -- (2.4+6,0.6) -- (2.4+6,-0.6) -- (-0.6+6,-0.6);
				\draw[postaction={decorate}] (0.9+6,0.6) -- (0.9+6,1.4);
				
				
				\draw[fill=white] (1.5+6,2) circle (0.46cm);
				\draw[fill=white] (1.2+6,2) circle (0.47cm);
				\draw[fill=white] (0.9+6,2) circle (0.48cm);
				\draw[fill=white] (0.6+6,2) circle (0.49cm);
				\draw[fill=white] (0.3+6,2) circle (0.5cm);
				
				\draw (-0.3+6,1.4) -- (-0.3+6,2.6) -- (2.1+6,2.6) -- (2.1+6,1.4) -- (-0.3+6,1.4);
				\draw[postaction={decorate}] (0.9+6,2.6) -- (0.9+6,3.4);
				
				\draw[postaction={decorate}] (2.1+6,2) -- (-0.3+10,2);
				
				
				\draw[fill=white] (1.2+6,4) circle (0.48cm);
				\draw[fill=white] (0.9+6,4) circle (0.49cm);
				\draw[fill=white] (0.6+6,4) circle (0.5cm);
				
				\draw (0+6,3.4) -- (0+6,4.6) -- (1.8+6,4.6) -- (1.8+6,3.4) -- (0+6,3.4);
				
				
				
				
				
				
				\draw[fill=white] (1.8+10,0) circle (0.44cm);
				\draw[fill=white] (1.5+10,0) circle (0.45cm);
				\draw[fill=white] (1.2+10,0) circle (0.46cm);
				\draw[fill=white] (0.9+10,0) circle (0.47cm);
				\draw[fill=white] (0.6+10,0) circle (0.48cm);
				\draw[fill=white] (0.3+10,0) circle (0.49cm);
				\draw[fill=white] (0+10,0) circle (0.5cm);
				
				\draw (-0.6+10,-0.6) -- (-0.6+10,0.6) -- (2.4+10,0.6) -- (2.4+10,-0.6) -- (-0.6+10,-0.6);
				\draw[postaction={decorate}] (0.9+10,0.6) -- (0.9+10,1.4);
				
				
				\draw[fill=white] (1.5+10,2) circle (0.46cm);
				\draw[fill=white] (1.2+10,2) circle (0.47cm);
				\draw[fill=white] (0.9+10,2) circle (0.48cm);
				\draw[fill=white] (0.6+10,2) circle (0.49cm);
				\draw[fill=white] (0.3+10,2) circle (0.5cm);
				
				\draw (-0.3+10,1.4) -- (-0.3+10,2.6) -- (2.1+10,2.6) -- (2.1+10,1.4) -- (-0.3+10,1.4);
				\draw[postaction={decorate}] (0.9+10,2.6) -- (0.9+10,3.4);
				
				\draw[postaction={decorate}] (2.1+10,2) -- (-0.3+14,2);
				
				
				\draw[fill=white] (1.2+10,4) circle (0.48cm);
				\draw[fill=white] (0.9+10,4) circle (0.49cm);
				\draw[fill=white] (0.6+10,4) circle (0.5cm);
				
				\draw (0+10,3.4) -- (0+10,4.6) -- (1.8+10,4.6) -- (1.8+10,3.4) -- (0+10,3.4);
			\end{tikzpicture}
			\caption{Visualisierung der Rückkopplung eines RNNs. (Quelle: E. D.)}\label{RNNunfolded}
		\end{wrapfigure}
				
		Diese Arbeit befasst sich genauer nur mit dem Long Short-Term Memory Neural Network, kurz LSTM Netz, welches eines der häufigsten Typen von RNNs ist, da es im Gegensatz zum allgemeinen Modell nicht vom Vanishing Gradient (Siehe \ref{VanishingGradient}) betroffen ist.\footnote{\practitioner{149-150}}
				
				
		% Arbeit betrachtet genauer LSTM Netz (-> Wäre es nicht sinnvoll zu sagen, dass dieses Kapitel (auch den Namen LSTM haben sollte) insgesamt von LSTMs handelt und nur zu Beginn RNNs allgemein angesprochen werden?)
		% Wichtig zu sagen, warum gerade LSTM Netzwerke so interessant sind. 
		% Für MNIST genauso geeignet, da man die Bilder beispielsweise in 28 Vektoren zerlegen könnte, die nacheinander eingespeist werden.
		% Gründe für LSTM: Beseitigt Problem des Vanishing Gradients und eine der häufigsten Varianten von Recurrent Neural Networks. (practitioner 149-150)
				
		\section{RNNs allgemein}
			RNNs gehen, wie CNNs, von den FFNNs aus, und wandeln diese ab.\footnote{\practitioner{149}} Der Unterschied liegt darin, dass die Ausgaben von Neuronen der Hidden Layer $\vec{x}_{(l)}^{(t)}$ im nächsten Zeitschritt $t+1$ als zusätzliche Eingaben, neben der Ausgabe $\vec{x}_{(l-1)}^{(t+1)}$ der Neuronen der vorherigen Schicht, verwendet werden. Berechnen lassen sich die Ausgabewerte des Layer $l$ zum Zeitpunkt $t$ mit Formel \ref{RNNallgemeinFormel}\footnote{Hier widersprechen sich verschiedene Autoren. Während Gupta eine $1 \times 1$ Matrix als Gewicht für $\vec{x}_{(l)}^{(t-1)}$ verwendet, benutzt Karpathy keinen Bias in seinen Berechnungen. Daher behandelt diese Arbeit diesen Vektor genau gleich wie $\vec{x}_{(l-1)}^{(t)}$ und verwendet einen Bias. (vgl. Gupta, 2017; Karpathy, 2015)}. Abb. \ref{RNNunfolded} visualisiert, wie die Berechnungen zum Zeitpunkte $t$ von den vorherigen Zeitpunkten abhängig sind.\footnote{vgl. Gupta, 2017}
				
			\begin{equation}\label{RNNallgemeinFormel}
				\vec{x}_{(l)}^{(t)} = f_{(l)} ( W_{(l-1), (l)}^{(t),(t)} \cdot \vec{x}_{(l-1)}^{(t)} + W_{(l),(l)}^{(t-1),(t)} \cdot \vec{x}_{(l)}^{(t-1)} + \vec{b}_{(l)})
			\end{equation}
		
				
			% Sequentielle Datenverarbeitung
			% Problem des Vanishing Gradient -> Fundamentals
			% One to Many, Many to One, Many to Many (practitioner S. 146)
			% Keine 100% eindeutige Definition, während Gupta 1*1 Matrizen als Weights für h^t verwendet, benutzt Karpathy keinen Bias -> wir verwenden wie bisher gelernt 1*irgendwas Matrizen als Weights mit Bias.
			% ERLEDIGT
				
				
		\section{LSTM Netzwerke}
			LSTM Netze unterscheiden sich in der Funktionsweise ihrer Neurone, den LSTM Zellen, von anderen Netzen. Diese lösen auch das Problem des Vanishing Gradients.\footnote{vgl. ebd.}
			% Lösen das Problem des Vanishing Gradient
			% Erklärung der Abkürzung
			% Haben einen Long-Term (Zell-Status) und Short-Time (vorherige Hidden-Status) Speicher
			% Das wichtige sind die Memory Cells und die Gates welche den Inhalt der Memory Cells steuern können (Practitioner 150) 
			% Cell Status
			% Hier keine Leerzeilen zwischen section Überschrift und wrapfigure einfügen!
			\begin{wrapfigure}[12]{r}{0cm}	
				%Vorlage: Olah
				\begin{tikzpicture}[scale=0.5, decoration={markings, mark=at position 1 with {\arrow[scale=2]{>}}}]
					\draw[rectangle,rounded corners,] (-0.5,-0.6) rectangle (14,7.2);
					
					\draw[fill=white, opacity=0.3] (1.2,1.5) circle (0.48cm);
					\draw[fill=white, white] (0.9,1.5) circle (0.49cm);
					\draw[fill=white, opacity=0.3] (0.9,1.5) circle (0.49cm);
					\draw[fill=white, white] (0.6,1.5) circle (0.5cm);
					\draw[fill=white, opacity=0.3] (0.6,1.5) circle (0.5cm);
					\draw (0,0.9) rectangle (1.8,2.1);
					\draw node[black,midway,yshift=1.5cm,xshift=1cm] at (0,0) {\Huge $\sigma$};
					
					
					\draw[fill=white, opacity=0.3] (1.2+3,1.5) circle (0.48cm);
					\draw[fill=white, white] (0.9+3,1.5) circle (0.49cm);
					\draw[fill=white, opacity=0.3] (0.9+3,1.5) circle (0.49cm);
					\draw[fill=white, white] (0.6+3,1.5) circle (0.5cm);
					\draw[fill=white, opacity=0.3] (0.6+3,1.5) circle (0.5cm);
					\draw (0+3,0.9) rectangle (1.8+3,2.1);
					\draw node[black,midway,yshift=1.5cm,xshift=4cm] at (0,0) {\Huge $\sigma$};
					
					
					\draw[fill=white, opacity=0.3] (1.2+6,1.5) circle (0.48cm);
					\draw[fill=white, white] (0.9+6,1.5) circle (0.49cm);
					\draw[fill=white, opacity=0.3] (0.9+6,1.5) circle (0.49cm);
					\draw[fill=white, white] (0.6+6,1.5) circle (0.5cm);
					\draw[fill=white, opacity=0.3] (0.6+6,1.5) circle (0.5cm);
					\draw (0+6,0.9) rectangle (1.8+6,2.1);
					\draw node[black,midway,yshift=1.5cm,xshift=7cm] at (0,0) {\LARGE $f_{z; (l)}$};

					
					\draw[fill=white, opacity=0.3] (1.2+3+6,1.5) circle (0.48cm);
					\draw[fill=white, white] (0.9+3+6,1.5) circle (0.49cm);
					\draw[fill=white, opacity=0.3] (0.9+3+6,1.5) circle (0.49cm);
					\draw[fill=white, white] (0.6+3+6,1.5) circle (0.5cm);
					\draw[fill=white, opacity=0.3] (0.6+3+6,1.5) circle (0.5cm);
					\draw (0+3+6,0.9) rectangle (1.8+3+6,2.1);
					\draw node[black,midway,yshift=1.5cm,xshift=10cm] at (0,0) {\Huge $\sigma$};
					
					
					
					\draw (-1,0.2) -- (9.6,0.2);
					\draw[postaction={decorate}] (0.6,0.2) -- (0.6,0.9);
					\draw[postaction={decorate}] (0.6+3,0.2) -- (0.6+3,0.9);
					\draw[postaction={decorate}] (0.6+3+3,0.2) -- (0.6+3+3,0.9);
					\draw[postaction={decorate}] (0.6+3+3+3,0.2) -- (0.6+3+3+3,0.9);
					
					
					\draw (0,-1.2) -- (0,-0.2);
					\draw (0,-0.2) -- (10.2,-0.2);
					\draw[postaction={decorate}] (0.6+0.6,0.3) -- (0.6+0.6,0.9);
					\draw[postaction={decorate}] (0.6+3+0.6,0.3) -- (0.6+3+0.6,0.9);
					\draw[postaction={decorate}] (0.6+3+3+0.6,0.3) -- (0.6+3+3+0.6,0.9);
					\draw[postaction={decorate}] (0.6+3+3+3+0.6,-0.2) -- (0.6+3+3+3+0.6,0.9);
					\draw (0.6+0.6,-0.2) -- (0.6+0.6,0.1);
					\draw (0.6+3+0.6,-0.2) -- (0.6+3+0.6,0.1);
					\draw (0.6+3+3+0.6,-0.2) -- (0.6+3+3+0.6,0.1);
					
					
					
					\draw[postaction={decorate}] (-1,6.5) -- (0.5,6.5);
					\draw[postaction={decorate}] (0.9,2.1) -- (0.9,6.1);
					\draw[postaction={decorate}] (1.3,6.5) -- (0.5+6,6.5);
					\draw[postaction={decorate}] (0.9+6,2.1) -- (0.9+6,2.9);
					\draw[postaction={decorate}] (0.9+6,3.7) -- (0.9+6,6.1);
					
					\draw (0.9+3,2.1) -- (0.9+3,3.3);
					\draw[postaction={decorate}] (0.9+3,3.3) -- (6.5,3.3);
					
					\draw (0.9+3+6,2.1) -- (0.9+3+6,3.3);
					\draw[postaction={decorate}] (0.9+3+6,3.3) -- (6.5+5,3.3);
					
					\draw[postaction={decorate}] (7.3,6.5) -- (15,6.5);
					
					\draw[postaction={decorate}] (11.9,6.5) -- (11.9,5.4);
					\draw[postaction={decorate}] (11.9,4.6) -- (11.9,3.7);
					\draw (11.9,2.9) -- (11.9,0.2);
					\draw[postaction={decorate}] (11.9,0.2) -- (15,0.2);
					\draw (13,0.2) -- (13,6.4);
					\draw[postaction={decorate}] (13,6.6) -- (13,8);
					
					
					\draw node[black,midway,yshift=6.5cm,xshift=0.9cm] at (0,0) {\Huge $\odot$};
					\draw node[black,midway,yshift=6.5cm,xshift=6.9cm] at (0,0) {\Huge $+$};
					\draw node[black,midway,yshift=3.3cm,xshift=6.9cm] at (0,0) {\Huge $\odot$};
					\draw node[black,midway,yshift=3.3cm,xshift=11.9cm] at (0,0) {\Huge $\odot$};
					\draw node[black,midway,yshift=5cm,xshift=11.9cm] at (0,0) {\LARGE $f_{x;(l)}$};
					
					
					\draw node[black,midway,yshift=-2cm,xshift=0.3cm] at (0,0) {\huge $\vec{x}_{(l-1)}^{(t)}$};
					\draw node[black,midway,yshift=0.3cm,xshift=-1.7cm] at (0,0) {\huge $\vec{x}_{(l)}^{(t-1)}$};
					\draw node[black,midway,yshift=6.6cm,xshift=-1.7cm] at (0,0) {\huge $\vec{c}_{(l)}^{(t-1)}$};
					\draw node[black,midway,yshift=6.6cm,xshift=16cm] at (0,0) {\huge $\vec{c}_{(l)}^{(t)}$};
					\draw node[black,midway,yshift=0.3cm,xshift=16cm] at (0,0) {\huge $\vec{x}_{(l)}^{(t)}$};
					\draw node[black,midway,yshift=8.5cm,xshift=13.4cm] at (0,0) {\huge $\vec{x}_{(l)}^{(t)}$};
					
					
					\draw[dashed] (-0.25, 0.4) rectangle (2.05, 6.95);
					\draw[dashed] (2.75, 0.4) rectangle (8.05, 6.95);
					\draw[dashed] (8.75, 0.4) rectangle (12.8, 6);
					
					
					\draw node[black,midway,yshift=5cm,xshift=0.3cm] at (0,0) {\Huge $I.$};
					\draw node[black,midway,yshift=5cm,xshift=3.5cm] at (0,0) {\Huge $II.$};
					\draw node[black,midway,yshift=5cm,xshift=9.7cm] at (0,0) {\Huge $III.$};
				\end{tikzpicture}
				\caption{Visualisierung der Datenströme einer LSTM Zelle. (Quelle: E. D.)}\label{LSTMzellDarstellung}
			\end{wrapfigure}
				
			\subsection{LSTM Zellen}
				Das Besondere an LSTM Zellen ist der Zell Status, welcher sich über sogenannte Gates (auch Gatter genannt) verändern lässt. Ein LSTM hat normalerweise drei Gates, welche in Abb. \ref{LSTMzellDarstellung} mit strichlierten Boxen gekennzeichnet sind: Ein Forget Gate ($I.$), ein Input Gate ($II.$) und ein Output Gate ($III.$). Das Forget Gate bestimmt, welche Informationen vom Zell Status des Zeitpunktes $t-1$ übernommen werden sollen. Dies geschieht in Form des Vektors $\vec{u}_{(l)}^{(t)}$, welcher mit Formel \ref{uFormel} berechnet wird, dessen Elemente einen Wert zwischen 0 und 1 haben, wobei 0 bedeutet, dass diese Information vergessen, und 1, dass sie weiterhin gespeichert werden soll.\footnote{vgl. Olah, 2015}
				
				\begin{equation}\label{uFormel}
					\vec{u}_{(l)}^{(t)} = f_{u;(l)} \left( W_{u; (l-1),(l)}^{(t),(t)} \cdot \vec{x}_{(l-1)}^{(t)} + W_{u; (l),(l)}^{(t-1),(t)} \cdot \vec{x}_{(l)}^{(t-1)} + \vec{b}_{u; (l)} \right)
				\end{equation}
				
				Das Input Gate entscheidet, welche neuen Informationen zum Zell Status hinzugefügt werden sollen. Der Vektor $\vec{i}_{(l)}^{(t)}$ (Formel \ref{iFormel}) hat für den Vektor $\vec{z}_{(l)}^{(t)}$ (Formel \ref{zFormel}) dabei die gleiche Funktion, wie $\vec{u}_{(l)}^{(t)}$ für $\vec{c}_{(l)}^{(t-1)}$: $\vec{z}_{(l)}^{(t)}$ gibt dabei die neuen Informationen an, $\vec{i}_{(l)}^{(t)}$ welche der neuen Daten übernommen werden sollen. Dabei ist $f_{z;(l)}$ die Aktivierungsfunktion für die neuen Daten, wobei hierfür meistens die TanH-Funktion eingesetzt wird. Mit Formel \ref{ZFormel} wird der Zell Status anschließend aktualisiert.
				
				% Gatter eingezeichnet in strichlierten Boxen
				% Bestehen aus einem Layer und einer Operation mit  
				% Beschreibung der Gatter
				% für f_{z; (l)} und f_{x; (l)} meistens tanh-Funktion verwendet 
				
				\begin{equation}\label{iFormel}
					\vec{i}_{(l)}^{(t)} = f_{i;(l)} \left( W_{i; (l-1),(l)}^{(t),(t)} \cdot \vec{x}_{(l-1)}^{(t)} + W_{i; (l),(l)}^{(t-1),(t)} \cdot \vec{x}_{(l)}^{(t-1)} + \vec{b}_{i; (l)} \right)
				\end{equation}
				
				\begin{equation}\label{zFormel}
					\vec{z}_{(l)}^{(t)} = f_{z; (l)} \left( W_{z; (l-1),(l)}^{(t),(t)} \cdot \vec{x}_{(l-1)}^{(t)} + W_{z; (l),(l)}^{(t-1),(t)} \cdot \vec{x}_{(l)}^{(t-1)} + \vec{b}_{z; (l)} \right)
				\end{equation}
				
				\begin{equation}\label{ZFormel}
					\vec{c}_{(l)}^{(t)} = \vec{u}_{(l)}^{(t)} \odot \vec{c}_{(l)}^{(t-1)} + \vec{i}_{(l)}^{(t)} \odot \vec{z}_{(l)}^{(t)}
				\end{equation}
				
				Schlussendlich wird der Output berechnet, wobei $\vec{o}_{(l)}^{(t)}$ (Formel \ref{oFormel}) ebenfalls ein Vektor ist, welcher entscheidet, welche Einträge eines Vektors, in diesem Fall des Vektors $\vec{c}_{(l)}^{(t)}$, verwendet werden. Die Endgültige Ausgabe zum Zeitpunkt $t$ wird mit Formel \ref{xFormel} berechnet, wobei $f_{x;(l)}$ eine Aktivierungsfunktion ist, für die auch meistens die TanH-Funktion verwendet wird.\footnote{vgl. Olah, 2015; Gibson \& Patterson, 2017, S. 154f} 
				
				\begin{equation}\label{oFormel}
					\vec{o}_{(l)}^{(t)} = f_{o;(l)} \left( W_{o; (l-1),(l)}^{(t),(t)} \cdot \vec{x}_{(l-1)}^{(t)} + W_{o; (l),(l)}^{(t-1),(t)} \cdot \vec{x}_{(l)}^{(t-1)} + \vec{b}_{o; (l)} \right)
				\end{equation}
				
				\begin{equation}\label{xFormel}
					\vec{x}_{(l)}^{(t)} = \vec{o}_{(l)}^{(t)} \odot f_{x; (l)}(\vec{c}_{(l)}^{(t)})
				\end{equation}
				
				Es gibt verschiedene Varianten von LSTM Zellen, welche beispielsweise den vorherigen Zell Status $\vec{c}_{(l)}^{(t-1)}$ in die Berechnung von $\vec{u}_{(l)}^{(t)}$, $\vec{i}_{(l)}^{(t)}$ und $\vec{o}_{(l)}^{(t)}$ miteinbeziehen, allerdings in dieser Arbeit nicht untersucht werden.\footnote{vgl. Olah, 2015}
				
				% Grafik mit Aufbau der Gates
				% Unterschiedliche Wege der Berechnung in der Literatur, ich halte mich an Christopher Olah und practitioner
				% Erwähnen, dass andere Zellenarten gibt, aber nicht genauer auf diese eingehen.
				
			\subsection{Aufbau}
				Ein LSTM Layer besteht aus einer solchen Zelle, wobei die Anzahl der Dimensionen von $\vec{x}_{(l)}^{(t)}$ (bzw. auch von $\vec{c}_{(l)}^{(t)}$) vergleichbar mit der Anzahl der Neuronen eines Fully-Connected Layers ist. Dabei kann es einen oder mehrere LSTM Layer geben.\footnote{vgl. Olah, 2015; Gibson \& Patterson, 2017, S. 156}
				% Erwähnen, dass nur eine LSTM Zelle pro Layer und Dimension von \vec{x}_{(l)}^{(t-1)} vergleichbar ist mit der Anzahl der Neurone in einem Fully-Connected Layer. (Eigene Erkenntnis? Komisches Paper in Practitioner?)
				% Wie viele Layer von LSTMs
				% andere Layer
				
				
	\chapter{Lernen eines KNNs am Beispiel des FFNN}\label{Lernen}
		Bisher wurden in der Arbeit nur verschiedene Arten von KNNs, ihr Aufbau und ihre Funktionsweise behandelt. Jedoch fehlt noch ein wichtiger Teil für die Entwicklung von KNNs, das Training. Unter Training versteht man das Bestimmen der passenden Werte für die Parameter des KNNs für eine bestimmte Aufgabe mithilfe von Trainingsdaten.\footnote{vgl. Buduma, 2017, S. 17; Gibson \& Patterson, 2017, S. 27} Erklärt wird der Vorgang des Trainings in dieser Arbeit nur anhand des FFNN, und zwar aus dem Grund, dass dieser Prozess bei diesem Netztypus am verständlichsten ist. \\

		%Zweitens funktioniert das Trainieren von CNNs und RNNs ähnlich wie beim FFNN. Drittens ist das Verstehen dieses Prozesses für die Implementierung von KNNs dank vieler moderner Libraries nicht mehr nötig. \\
		
		Es gibt mehrere Optimierungs-Methoden, d. h. Möglichkeiten, wie das Trainieren umgesetzt wird. Diese Arbeit befasst sich mit dem Gradient Descent (GD) und dessen zwei bekanntesten Abwandlungen, dem Stochastic Gradient \mbox{Descent (SGD)} und Mini-Batch Gradient Descent.\footnote{vgl. Gibson \& Patterson, 2017, S. 98f; ebd., S. 30ff; Buduma, 2017, S. 25} Das Problem beim Herausfinden der Werte für diese Parameter ist, dass diese nicht über ein Gleichungssystem gelöst werden können, sondern iterativ berechnet werden müssen.\footnote{vgl. Rashid, 2017, S. 71; Buduma, 2017, S. 18f}
				
		
		% Bisher wurden in der Arbeit nur verschiedene Arten von KNNs, ihr Aufbau und ihre Funktionsweise genannt. Jedoch reicht das nicht aus, damit ein Neuronales Netz beispielsweise Bilder klassifizieren kann. Jedes KNN muss vor seinem Einsatz trainiert werden, d.h. die Weights und Biases so anzupassen, dass das Netz Strukturen in den Daten erkennen kann [BELEG ERFORDERLICH]. Die dafür benötigten Algorithmen sind teilweise erst seit den 1980er Jahren bekannt, aufgrund der mangelnden Rechenleistung haben KNNs erst seit wenigen Jahren Erfolg. [2 BELEGE ERFORDERLICH]. Die zwei häufig miteinander verwendeten Algorithmen sind (Stochastic) Gradient Descent und Backpropagation. 
		% WICHTIG: Zu verstehen, wie Training funktioniert, ist nicht notwendig für die Beantwortung der Forschungsfragen. Jedoch gehört es auch dazu ("der Vollständigkeit wegen"). Außerdem sind die Erkenntnise, was Optimiert werden kann, hilfreich für Experimente (Und somit für Forschungsfragen)
		% Zitat Technologies come and go, but insight is forever -> Notiz 21 			
		% Practitioner S. 92															
		% Bezeichnung Training: Fundamentals, S. 17										
		% Eine Definition von Gradient: Fundamentals, S. 20								
		% Eine Definition von Learningrate: Fundamentals, S. 21							
		% Eine "Definition" von Backpropagation: Fundamentals, S. 23					
		% 3 WICHTIGE OBSERVATION: Fundamentals, S. 30f									
		% Aufbau des Datasets: Fundamentals, S. 31	
		% Notiz 57		Der Prozess, Weights & Biases so zu adjustieren, dass der Fehler kleiner wird, nennt sich Parameter Optimierung (Practitioner S. 27)			ERLEDIGT
		% Notiz 2.5		es wird nicht perfektes Gewicht berechnet, da sonst bisheriger Lernfortschritt zunichte gemacht wird (Rashid S.21)								
		% Notiz 7		Je größer das Gewicht, desto größer die Beteiligung am Fehler (Rashid S. 62)																	
		% Notiz 8		Gefahr, nur lokales und nicht globales Minimum zu erreichen (Rashid S. 77)																		
		% Notiz 9 		Definierung der "klassischen" Loss-Funktion (und warum andere schlecht sind) (Rashid S. 79)														
		%	-> Siehe auch: Loss-Function in Practitioner																												
		% Notiz 13 		Faustregel für Interval für Zufallsinitialisierung der Gewichte (Rashid S. 92)																	
		% Notiz 14		Begründung, warum Gewichte zufällig initiallisiert werden sollten und nicht gleich und oder 0 sein sollten (Rashid S. 93)						
		% Notiz 17		Definition von einem Epoch (Rashid S. 155)																													
		% Notiz 19		Zusammenhang zwischen Lernrate und Epochen (Rashid S. 157)																						
		% Notiz 20 		Zusammenhang zwischen Trefferquote und Anzahl der Hidden Neurone (Rashid S. 158)																
		%	-> eher bei Kapitel zu Experimenten																															
		% Notiz 29		Definiton der Loss-Funktion nach Nielsen (Nielsen Blatt 8)																						
		% Notiz 32		Definiton des Gradienten-Vektor nach Nielsen (Nielsen Blatt 9)																					
		% Notiz 33		Symbol für die Lernrate (Nielsen Blatt 9)																										
		% Notiz 34		Unterschied zwischen GD und SGD nach Nielsen (Nielsen Blatt 10)																					
		% Notiz 36		Über Hyperparameter (Nielsen Blatt 13)																											
		% Notiz 40		Unterschied zwischen SGD und Backpropagation (Nielsen Blatt 16)																					
		% Notiz 43		Ziel von Backpropagation (nach Nielsen) (Nielsen Blatt 16 Rückseite)																			
		% Notiz 44		Error Berechnung bei Backpropagation nach Nielsen (Nielsen Blatt 17)																			
		% Notiz 45		Über BP1 (Nielsen Blatt 18)																														
		% Notiz 46 		Backprop in Worten (Nielsen Blatt 19)																											
		% Notiz 68 		Lernalgorithmen -> keine Garantie effizient oder richtig (Practitioner S. 58)																	
		% Notiz 69		Beschreibung von Backpropagation (Practitioner S. 64)																							
		% Notiz 72		Was genau macht Loss-Funktion (Practitioner S. 71)																								
		% Notiz 79		Wann hört man auf zu Lernen? (Kriesel S. 63)																									
		% Notiz 80		Probleme von Gradientenverfahren (Kriesel S. 65)																								
		% 				Bezeichnung ob Cost oder Loss Funktion (deeplearning S. 80)																						
		%				Backpropagation in Worten (Wartala S. 17)																										
		
	
		\section{Gradient Descent}\label{GradientDescent}
			%Gradient Descent ist ein Algorithmus, welcher die Parameter eines KNNs iterativ, d. h. in mehreren Schritten, verändert. \practitioner{64} 
			Um ein KNN trainieren zu können, muss zunächst herausgefunden werden, wie gut oder schlecht die bisherigen Weights geeignet sind. Dazu wird die Eingabe\break\mbox{$\vec{x}_{(1)}(\textrm{tr}:i)$} \footnote{Im Fall von MNIST ein Vektor mit 784 Dimensionen; eine pro Pixelwert} eines Trainingsbeispiels $i$ in das Netz eingespeist, dessen gewünschte Ausgabe \mbox{$\vec{y}(\textrm{tr}:i)$} \footnote{Im Fall von MNIST ein Vektor mit 10 Dimensionen; eine pro möglicher Ziffer} bekannt ist, und liefert einen Output \mbox{$\vec{x}_{(L)}(\textrm{tr}:i)$}. Wie nahe dieser Output an den Gewünschten herankommt, lässt sich mit einer Cost-Funktion, auch als Loss-Funktion bezeichnet, ermitteln. Es gibt mehrere Cost-Funktionen (Siehe \ref{Optimierung:Cost-Funktion}), eine der einfacheren und gebräuchlicheren ist die mittlere quadratische Abweichung in Formel \ref{MSE}, welche den durchschnittlichen Cost-Wert für alle Trainingsbeispiele berechnet.\footnote{vgl. Nielsen, 2015, Kapitel 1/Learning with gradient descent}
	
			% Sollte da nicht besser Sigmoid als Bsp. verwendet werden?
			\begin{equation}\label{MSE}
				C = \frac{1}{2\cdot\textrm{Tr}} \cdot \left( \sum_{i=1}^{\textrm{Tr}} |(\vec{y}(\textrm{tr}:i) - \vec{x}_{(L)}(\textrm{tr}:i))^2| \right)
			\end{equation} 
	
	
			Der Wert der Cost-Funktion wird kleiner, je kleiner die Differenz zwischen dem gewünschten und dem eigentlichen Output wird, was wiederum eine Folge von an die Trainingsdaten angepassten Parametern ist. Das Ziel des GD Algorithmus ist es, den Wert der Cost-Funktion zu minimieren\footnote{Das ein kleinerer Cost-Wert nicht unbedingt ein besseres KNN zufolge hat, sieht man in \ref{UnderOverFitting} (\fundamentals{31})}.\footnote{vgl. ebd., Kapitel 1/Learning with gradient descent}
	
			Vor dem Training müssen die Weights und Biases initialisiert werden, wofür man Zufallswerte verwendet.\footnote{vgl. Nielsen, 2015, Kapitel 1/Implementing our network to classify digits} 
			%Es ist nämlich nicht ratsam für alle Weights und Biases den gleichen Startwert zu verwenden, vor allem nicht 0.\footnote{Genaueres dazu in \ref{TodsündenWeights}}  (vgl. Rashid, 2017, S. 93) 
			Da der Cost-Wert nur von den Parametern abhängig ist (Input und Output sind bei Trainingsbeispielen fest vorgegeben und lassen sich nicht verändern), lassen sich diese mithilfe der partiellen Ableitung der Cost-Funktion nach diesen verändern. Die Formeln zum Aktualisieren der Parameter sind folgende:
	
			\begin{equation}\label{UpdateRuleWeights}
				w_{(l-1,m),(l,n)} = w_{(l-1,m),(l,n)} - \eta \cdot \frac{\partial C}{\partial w_{(l-1,m),(l,n)}}
			\end{equation}
			\begin{equation}\label{UpdateRuleBiases}
				b_{(l,n)} = b_{(l,n)} - \eta \cdot \frac{\partial C}{\partial b_{(l,n)}}
			\end{equation}
	
			Der Vektor mit den partiellen Ableitungen der Cost-Funktion nach den einzelnen Weights und Biases wird Gradient genannt.\footnote{vgl. Nielsen, 2015, Kapitel 1/Learning with gradient descent; ebd., Kapitel 2/The two assumptions we need about the cost function} Die partielle Ableitung $\frac{\partial C}{\partial w_{(l-1,m),(l,n)}}$ bzw. $\frac{\partial C}{\partial b_{(l,n)}}$ gibt an, wie sich die Cost-Funktion ändert, wenn man diesen Parameter ändert.\footnote{vgl. Rashid, 2017, S. 81} Ist die partielle Ableitung positiv bzw. negativ, muss der Parameter verringert bzw. erhöht werden. Je größer die partielle Ableitung, desto größer der Betrag um den der Parameter verändert werden muss.\footnote{vgl. ebd., S. 76} Der Faktor $\eta$ wird Learning Rate (oder Lernrate) genannt und steuert, wie groß die Veränderungen der Parameter sein sollen. Die Lernrate ist einer der Hyperparameter (Siehe \ref{Hyperparameter}) eines KNNs.\footnote{\practitioner{31}} \newline \break 
			Durch mehrfaches, iteratives Anwenden der beiden Formeln \ref{UpdateRuleWeights} und \ref{UpdateRuleBiases} lassen sich die Parameter so anpassen, dass die Cost-Funktion zu einem Minimum kommt. Dabei wird zuerst der durchschnittliche Gradient aller Trainingsbeispiele berechnet, mit welchem dann die Weights und Biases aktualisiert werden.\footnote{vgl. Nielsen, 2015, Kapitel 1/Learning with gradient descent} Ein solcher Durchgang durch alle Trainingsdaten wird als Epoch (auch Epoche) bezeichnet.\footnote{vgl. Rashid, 2017, S. 155} Die Anzahl der Epochen beim Trainieren ist ebenfalls ein Hyperparameter.\footnote{\practitioner{100}}

			\enlargethispage{0.3\baselineskip}
	
			Es gibt allerdings zwei größere Probleme mit diesem Algorithmus: Erstens gibt es keine Garantie, dass die Cost-Funktion sich dem globalen und nicht nur einem lokalen Minimum annähert. Zweitens dauert das Berechnen eines durchschnittlichen Gradienten bei einer großen Menge von Trainingsdaten relativ lange.\footnote{vgl. Nielsen, 2015, Kapitel 1/Learning with gradient descent} Eine Möglichkeit diese Probleme zu umgehen wäre, die Parameter nach jedem Trainingsbeispiel, d. h. mit den einzelnen Gradienten der Beispiele zu adjustieren, was als Stochastic Gradient Descent (SGD) bekannt ist. Der Nachteil dieser Optimierungs-Methode ist, dass die einzelnen Gradienten möglicherweise keine genügende Annäherung an den durchschnittlichen Gradienten sind, wodurch die Werte der Parameter fluktuieren.\footnote{\fundamentals{33}} Um diese Aufgabe zu lösen, verwendet man den Mini-Batch GD Algorithmus, welcher den Gradienten für eine kleine Teilmenge (Mini-Batch) aller Trainingsbeispiele berechnet und mit diesen die Parameter aktualisiert\footnote{Es kommt in der Literatur auch vor, dass SGD und Mini-Batch GD zu SGD zusammengefasst werden, wobei SGD wie es hier erklärt ist wie Mini-Batch GD mit einer Teilmengen-Größe von 1 gehandhabt wird.}.\footnote{\fundamentals{27}} Zu Beginn eines Epochs werden die Trainingsbeispiele in ihrer Reihenfolge durchgemischt und in die Mini-Batches aufgeteilt, welche dann zum Trainieren verwendet werden.\footnote{vgl. Nielsen, 2015, Kapitel 1/Implementing our network to classify digits}
			\\\
			\\\
			Nach dem Trainieren wird das KNN getest, d. h. es werden Testbeispiele, welche nicht bei den Trainingsbeispielen dabei waren, dem Netz übergeben um zu überprüfen, wie viele zuvor noch nie gesehene Beispiele richtig klassifiziert\footnote{Unter der Annahme, dass es sich um eine Classifications-Aufgabe handelt} werden.\footnote{vgl. Rashid, 2017, S. 148; Buduma, 2017, S. 29} Falls man mit dem Testergebnis zufrieden ist, ist das KNN bereit für seinen Einsatz.\footnote{\fundamentals{33}} Neben den Trainings- und Testdatensätzen gibt es noch den Validationsdatensatz (Siehe \ref{EpochsUndMiniBatch}).\footnote{vgl. Nielsen, 2015, Kapitel 1/Implementing our network to classify digits}

	
		\section{Backpropagation} \label{Backpropagation}
			Zur Berechnung der partiellen Ableitungen wird der Backpropagation (auch Fehlerrückführung genannt) Algorithmus verwendet, welcher das Problem ihrer Berechnung bei mehrschichtigen KNNs gelöst hat.\footnote{vgl. Wartala, 2018, S. 17} Um diese zu berechnen, wird zunächst ein Zwischenwert eingeführt, der Fehler $\delta_{(l,n)}$ eines Neurons, welcher zunächst über $\frac{\partial C}{\partial z_{(l,n)}}$ definiert wird. Das kommt daher, dass diese partielle Ableitung den Faktor angibt, wie stark sich eine Änderung $\Delta z_{(l,n)}$ auf den Wert der Cost-Funktion auswirkt (nämlich um eine Differenz von $\frac{\partial C}{\partial z_{(l,n)}} \cdot \Delta z_{(l,n)}$). Mit Backpropagation lässt sich dieser Fehler für jedes Neuron jeder Schicht berechnen, mit welchem man auch die partiellen Ableitungen $\frac{\partial C}{\partial w_{(l-1,m),(l,n)}}$ und $\frac{\partial C}{\partial b_{(l,n)}}$ berechnen kann.\footnote{vgl. Nielsen, 2015, Kapitel 2/The four fundamental equations behind backpropagation} Der Algorithmus verwendet die folgenden vier Formeln \ref{BP1} bis \ref{BP4} um die Fehler und partiellen Ableitungen zu berechnen: 
	
			\begin{equation}\label{BP1}
				\vec{\delta}_{(L)} = \nabla_{\vec{x}_{(L)}}C \odot f_{(L)}^{\prime} (\vec{z}_{(L)})
			\end{equation}
			
			\begin{equation}\label{BP2}
				\vec{\delta}_{(l)} = ({W_{(l),(l+1)}}^\intercal \cdot \vec{\delta}_{(l+1)}) \odot f_{(l)}^{\prime} (\vec{z}_{(l)})
			\end{equation}
			
			\begin{equation}\label{BP3}
				\frac{\partial C}{\partial b_{(l,n)}} = \delta_{(l,n)}
			\end{equation}
			
			\begin{equation}\label{BP4}
				\frac{\partial C}{\partial w_{(l-1,m), (l,n)}} = \delta_{(l,n)} \cdot x_{(l-1,m)}
			\end{equation}
	
			Die erste Formel berechnet den Fehler in der letzten Schicht des KNNs. Mithilfe der zweiten Formel lässt sich der Fehler in die Schichten $L-1$, $L-2$, ..., $1$ rückführen. Die letzten beiden Formeln dienen der Berechnung der partiellen Ableitungen\footnote{Da diese Formeln für manche Leser nicht einfach verständlich sind, befinden sich in Anhang B Beweise für diese.}.\footnote{vgl. ebd., Kapitel 2/The four fundamental equations behind backpropagation}
	
	
		\section{Hyperparameter} \label{Hyperparameter}
			Unter Hyperparametern versteht man Parameter, welche das Training eines KNN steuern. Diese wären: Anzahl der Neurone pro Layer, Learning Rate, Aktivierungsfunktion, Initialisierung der Gewichte und Biases, Cost-Funktion, Anzahl der Epochs und Größe der Mini-Batches\footnote{Hyperparameter, die nur für bestimmte Netztypen benötigt werden, werden nicht behandelt.}.\footnote{vgl. Gibson \& Patterson, 2017, S. 78; ebd., S. 100}
			% Practitioner S. 100

			\subsection{Learning Rate}
				Die Learning Rate steuert, wie groß die Schritte relativ zu den partiellen Ableitungen sein sollen, mit denen die Parameter verändert werden. Dabei hat die Lernrate Einfluss darauf, wie gut und wie lange das KNN trainiert wird. Eine große Learning Rate (z.B. $\eta = 1$) führt zwar schneller zu einem Minimum als eine kleine Lernrate (z.B. $\eta = 0,000001$), hat allerdings den Nachteil, dass der Wert der Cost-Funktion nicht so nahe ans Minimum herankommt, sondern sich nur in seiner Umgebung bewegt. Die kleinere Lernrate nähert sich dem Minimum zwar besser an, benötigt \newpage 
				dafür aber mehr Epochen. Es gibt zwar die Möglichkeit, die Lernrate in Abhängigkeit zur Nähe am Minimum anzupassen, allerdings ist das außerhalb des Rahmens dieser Arbeit.\footnote{\ebd{100}}

			\subsection{Cost-Funktion}\label{Optimierung:Cost-Funktion}
				Es gibt unterschiedliche Cost-Funktionen, welche je nach Aufgabe eines Netzes verwendet werden. So gibt es beispielsweise eigene Cost-Funktionen für Classification-Aufgaben.\footnote{\ebd{71-78}} In dieser Arbeit wird allerdings nur die Mean Squared Error Funktion eingesetzt, da andere Cost-Funktionen bestimme Voraussetzungen an den Output Layer haben oder nur bei bestimmten Aufgaben eingesetzt werden können.\footnote{vgl. Nielsen, 2015, Kapitel 3/Softmax; Rashid, 2017, S. 79}
				% und auch den Rahmen der Arbeit übersteigen würden
	
			% Gründe für MSE: Rashid S. 79/Liste
			\subsection{Epochs \& Mini-Batch}\label{EpochsUndMiniBatch}
				Die Größe der Mini-Batches hat einen Einfluss darauf, wie effizient die Optimierungs-Methode arbeitet. Zu kleine Mini-Batches (z.B. 1) nutzen oft die Fähigkeiten der Hardware nicht komplett aus, wodurch das Trainieren länger dauert. Zu große Mini-Batches sind ebenfalls ineffizient, da eine ausreichende Annäherung an den durchschnittlichen Gradienten auch mit kleineren Mini-Batches erzielt werden kann.
				% Wie sich die Größe der Mini-Batches auswirkt wird in den späteren Experimenten untersucht. \newline
				\\\
				Die Anzahl der Epochs gibt an, wie oft das KNN die Trainingsbeispiele durcharbeitet. Dabei gibt es einen Zusammenhang zwischen Anzahl der Epochs und der Learning Rate. Je mehr Epochen das KNN beim Training durchläuft, desto kleiner sollte die Learningrate sein, da sich die Optimierungs-Methode in vielen Schritten besser ans Minimum annähern kann, wenn diese kleiner sind. Durchläuft das KNN nur wenige Epochs sollte die Learning Rate größer sein, da die wenigen Aktualisierungen größer sein müssen, um sich dem Minimum zu nähern.\footnote{vgl. Rashid, 2017, S. 157}
	
				Das Training kann allerdings mit Early Stopping schon vor Durchlauf aller Epochs beendet werden. Nach jedem Epoch wird der Validationsdatensatz durchgegangen. Sobald sich die Trefferquote nur noch bei den Trainingsbeispielen, nicht aber bei den Validationsbeispielen verbessert, wird das Training gestoppt, da es sonst zu Overfitting (Siehe \ref{UnderOverFitting}) kommt.\footnote{vgl. Nielsen, 2015, Kapitel 3/Overfitting and regularization}
		
			% Early stopping
			\subsection{Initialisierung der Parameter} \label{TodsündenWeights}
				\enlargethispage{0.2\baselineskip}
				Für die Effizienz der Optimierungs-Methode sind gute Startwerte für die Parameter ausschlaggebend.\footnote{vgl. Nielsen, 2015, Kapitel 3/Weight initialization} Diese werden mit Zufallswerten initialisiert. Der Grund, warum nicht für alle Weights der gleiche Startwert verwendet wird, ist, dass in diesem Fall die Optimierungs-Methode nicht dazu in der Lage wäre, unterschiedliche Gewichte zu erzeugen, d. h. alle Gewichte haben am Ende des Trainings den gleichen Wert (auch wenn dieser nicht gleich dem Startwert sein muss). Wenn alle Gewichte gleich sind, wäre das so, wie wenn alle Hidden Layer nur ein Neuron hätten\footnote{Angenommen alle Biases hätten den Wert 0}. Einem solchen Netz würde es an Lernkapazität mangeln.\footnote{vgl. Rashid, 2017, S. 93; ebd., S. 158}
				%, wie in \ref{GradientDescent} erwähnt,
				%DER BIAS IST DAVON NICHT BETROFFEN
				Daher werden häufig Zufallswerte einer Normalverteilung verwendet. Hier kommt es auf die Werte des Erwartungswerts $\mu$ und der Standardabweichung $\sigma$ der Normalverteilung an. Werden diese beispielsweise auf $\mu = 0$ und $\sigma = 1$ gesetzt, kann es zu einer Sättigung (Siehe \ref{Sättigung}) kommen, da der Betrag von $z_{(l,n)}$ wahrscheinlich hoch ist, da die Standardabweichung von $z_{(l,n)}$ bei beispielsweise 500 Weights und einem Bias $\sqrt{1 \cdot 500 + 1} \approx 22,38$ ist. Eine Möglichkeit wäre nun, für $\mu = 0$ und $\sigma = \frac{1}{\sqrt{\textrm{len}(l-1)}}$ einzusetzen, da die Standardabweichung von $z_{(l,n)}$ im genannten Beispiel nur $\sqrt{\frac{1}{500} \cdot 500 + 1} = \sqrt{2}$ beträgt. Diese Regelung gilt nur für die Weights. Für die Biases werden oft die Zufallswerte einer Normalverteilung mit $\mu = 0$ und $\sigma = 1$ verwendet.\footnote{vgl. ebd., Kapitel 3/Weight initialization}
	
				% Daher werden häufig Zufallswerte einer Normalverteilung verwendet. Hier kommt es auf die Werte des Durchschnitts $\varnothing$ und der Standardabweichung $\sigma$ der Normalverteilung an. Werden diese beispielsweise auf $\varnothing = 0$ und $\sigma = 1$ gesetzt, kann es zu einer Sättigung (Siehe \ref{Sättigung}) kommen, da der Betrag von $z_{(l,n)}$ wahrscheinlich hoch ist, da die Standardabweichung von $z_{(l,n)}$ bei beispielsweise 500 Weights und einem Bias $\sqrt{1 \cdot 500 + 1} \approx 22,38$ ist. Eine Möglichkeit wäre nun, für $\varnothing = 0$ und $\sigma = \frac{1}{\sqrt{\textrm{len}(l-1)}}$ einzusetzen, da die Standardabweichung von $z_{(l,n)}$ im genannten Beispiel nur $\sqrt{\frac{1}{500} \cdot 500 + 1} = \sqrt{2}$ beträgt. Diese Regelung gilt nur für die Weights. Für die Biases werden oft die Zufallswerte einer Normalverteilung mit $\varnothing = 0$ und $\sigma = 1$ verwendet.\footnote{vgl. ebd., Kapitel 3/Weight initialization}
	
				% Werden die Parameter mit Zufallswerten einer Normalverteilung mit beispielsweise $\varnothing = 0$ und $\sigma = 1$ initialisiert, so kann es zur Sättigung (Siehe \ref{Sättigung}) von Neuronen kommen, da die Normalverteilung von $z_{(l,n)}$ eines Neurons mit 99 Eingabewerten von vorherigen Neuronen, welche alle den Ausgabewert 1 haben, eine Standardabweichung von $\sqrt{100}=10$ hat. In diesem Fall lernt das KNN zu Beginn nur langsam und benötigt um einiges länger, als wenn man die Parameter intelligenter initialisiert hätte. Daher werden die Gewichte zu einem Neuron $x_{(l,n)}$ mit Zufallswerten einer Normalverteilung mit $\varnothing = 0$ und $\sigma = \frac{1}{\sqrt{\textrm{len}(l-1)}}$ initialisiert\footnote{Es gibt noch andere Methoden, um dieses Problem zu umgehen, welche allerdings nicht Thema dieser Arbeit sein werden.}. Die Biases werden hingegen oft mit 0 oder Zufallswerten einer Normalverteilung mit $\varnothing = 0$ und $\sigma = 1$ initialisiert.\footnote{vgl. ebd., Kapitel 3/Weight initialization}
				%, wobei \mbox{$\textrm{len}(l-1)$} die Anzahl der Neuronen in der Schicht \mbox{$l-1$}, deren Werte dem Neuron $x_{(l,n)}$ übergeben werden,
	

				% Gleicher Wert für alle Weights bei Initiallisierung
				% 0 als Startwert für Initiallisierung
				% Zu große/kleine Startwerte -> Sättigung
				% Rashid S. 92
				% Siehe Nielsen Kapitel 3 Absatz 2)	
	
	
		\section{Probleme beim Trainieren}
			\begin{wrapfigure}{}{0cm} %Vorlage: Fundamentals S. 29
				\begin{tikzpicture}[scale=0.9]
					\begin{axis}[
						%title={$f(x) = \frac{1}{1 + \mathrm{e}^{-x}}$},
						xtick={-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5}, 
						xticklabels={-5,,,,,0,,,,,5},
						ytick={-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5}, 
						yticklabels={-5,,,,,0,,,,,5},
						x=20,
						y=20,
						ymin=-5.5, 	ymax=5.5, 
						xmin=-5.5, 	xmax=5.5, 
						axis lines=center, 
						hide obscured x ticks=false, 
						xlabel=$x$,
						ylabel={$y$},
						every inner x axis line/.append style={-},
						every inner y axis line/.append style={-},
						every axis x label/.style={at={(ticklabel* cs:0.99)},anchor=west,},
						every axis y label/.style={at={(ticklabel* cs:0.99)},anchor=south,},
						] 
						\addplot[domain=-5:5, samples=100, dashed]{((x-1)^3)+2*((x-1)^2)-x}; 
						\addplot[domain=-5:5, samples=100, thick]{0.45*x+0.25};
						\addplot[domain=-5:5, samples=100, dotted]{-1.5*x-3};
						\addplot[only marks, thick, mark=x, mark options={scale=2}] table[x index=0, y index=1] {imagesAndGraphData/bspTrainingsdaten.dat};
						\addplot[only marks, thin, mark=*] table[x index=0, y index=1] {imagesAndGraphData/bspTestdaten.dat};
					\end{axis};
				\end{tikzpicture}	
				\caption{Die gepunktete Funktion repräsentiert Underfitting, die strichlierte Overfitting und die durchgehende ein für die Aufgabe passendes KNN. Die Kreuze stellen die Trainingsdaten, die Punkte die Testdaten dar. (Quelle: E. D.)}\label{UnderOverFittingGraph}
			\end{wrapfigure}
			Das Trainieren von KNNs ist oft mit zusätzlichen Aufgaben und Problemen verbunden. Vier der verbreiteteren Probleme werden im folgenden Unterkapitel angesprochen. Für viele Spezialfälle von KNNs gibt es eigene Schwierigkeiten, welche das Ausmaß dieser Arbeit allerdings bei Weitem übersteigen würden.\footnote{vgl. Rashid, 2017, S. 156; Nielsen, 2015, Kapitel 5/Other obstacles to deep learning}
			
			\subsection{Under- und Overfitting} \label{UnderOverFitting}
				Beim Trainieren eines KNNs geht es um das Anpassen der Parameter, damit das Netz die Trainingsbeispiele richtig klassifiziert. Je besser allerdings das Netz an diese Beispiele angepasst ist, desto schlechter verhält sich das Netz mit Daten, welche es zuvor nicht gesehen hat. 
				\clearpage
	
				Sobald sich die Cost-Funktion einem Minimum nähert, die Trefferquote bei den Testbeispielen aber nicht mehr steigt, spricht man von Overfitting.\footnote{vgl. Nielsen, 2015, Kapitel 3/Overfitting and regularization}
	
				Underfitting ist das Gegenteil von Overfitting. Es tritt auf, wenn das Netz zu wenig an die Trainingsbeispiele angepasst und die Eingabe nicht in Verbindung mit der Ausgabe setzen kann.\footnote{\practitioner{26-27; Wartala, 2018, S. 195}} Abb. \ref{UnderOverFittingGraph} veranschaulicht den Unterschied anhand einer Regressions-Aufgabe. 
				
				% Siehe Fundamentals, S. 29
				% Siehe Deep Learning
				% Notiz 54, 55
	
			\subsection{Vanishing Gradient}\label{VanishingGradient}
				Das Problem des Vanishing Gradient tritt v. a. bei KNNs mit mehreren Hidden Layern auf. Durch das Rückführen des Fehlers mit Formel \ref{BP2} nähern sich die partiellen Ableitungen für Parameter in den ersten Hidden Layern immer mehr dem Wert 0, wodurch diese Layer langsamer lernen als die Hidden Layer nahe dem Output Layer.\footnote{vgl. Nielsen, 2015, Kapitel 5} Dieses Problem tritt auch bei RNNs auf, da bei diesen mit steigender Länge der Sequenz einer Eingabe die Beträge der partiellen Ableitungen für die Parameter in den ersten Zeitschritten immer kleiner werden.\footnote{vgl. Gupta, 2017}
	
			\subsection{Dying ReLU}\label{DyingReLU}
				Dieses Problem betrifft normale ReLUs\footnote{Als Rectified Linear Units (kurz ReLUs) werden Neurone mit der Rectified Linear-Funktion als Aktivierungsfunktion bezeichnet (\practitioner{70})}. Es besteht die Möglichkeit, dass $z_{l,n}$ einen Wert kleiner 0 annimmt, wodurch die Ausgabe $x_{(l,n)}$ den Wert 0 annimmt. In diesem Fall schafft es der Backpropagation Algorithmus nicht mehr die Gewichte zu diesem Neuron anzupassen, da Formel \ref{BP4} ebenfalls immer den Wert 0 erzeugt. Um dieses Problem zu umgehen gibt es die Leaky ReLUs, welche von diesem Problem nicht betroffen sind.\footnote{\practitioner{243}}
	
			\subsection{Sättigung}\label{Sättigung}
				Ein Neuron gilt dann als gesättigt, wenn sich $f_{(l)}^{\prime}(z_{(l,n)})$ dem Wert 0 annähert. In diesem Fall können die Parameter zu Beginn des Trainings nur in sehr kleinen Schritten verändert werden, da $\delta_{(l,n)}$ durch die geringe Steigung der Aktivierungsfunktion ebenfalls relativ klein ist. Von diesem Problem sind allerdings nur bestimmte Aktivierungsfunktionen, wie die Sigmoid- oder TanH-Funktion, betroffen. Die Rectified Linear-Funktion und ihre Abwandlung haben diesen Nachteil nicht.\footnote{vgl. Nielsen, 2015, Kapitel 2/The four fundamental equations behind backpropagation}
				
			% \subsection{Langsames Lernen durch große Gewichte und falsche Cost-Funktion} % Siehe Nielsen Kapitel 3/Cross entropy 
			% Notiz 8, 11, 12
	
			%IRGENDWO MUSS DIE GRENZE GEZOGEN WERDEN, WAS ALLES BEI DEN EXPERIMENTEN DABEI IST UND WAS NICHT.
	
	
	\chapter{Aufbau und Durchführung der Experimente} \label{Experimente}
		Dieser Teil der Arbeit befasst sich mit praktischen Experimenten zu den in den vorherigen Kapiteln behandelten Typen von KNNs. Ziel der Experimente ist es weniger, neue Rekorde aufzustellen\footnote{Hierfür hätte man sich mit der Optimierung des Trainings auseinandersetzen müssen}, sondern herauszufinden, wie die einzelnen Hyperparameter die Leistung\footnote{Als Leistung wird hier die Trefferquote eines KNNs bei einem vollständigen Durchlauf aller Beispiele des Testdatensatzes bezeichnet} der verschiedenen Typen von KNNs beeinflussen und mit welchen Werten eine besonders hohe Trefferquote\footnote{Die Trefferquote bezieht sich hier nur auf den Testdatensatz, außer es wird explizit von einer anderen Menge gesprochen} erzielt werden kann. 
		
		% Das mit Durchprobieren von mehreren Hyperparametern, etc. Siehe Fundamentals, S. 32
		% Layer Size eher eine Potenz von 2 (Practitioner S. 274)
		% Ich habe nicht das Ziel, irgendwelche Rekorde zu brechen. Meine Forschung geht, wie sich das Verhalten von KNNs je nach Änderung von (Hyper)parametern ändern. Würde ich Rekorde aufstellen wollen, müsste ich mich mehr mit Optimiesierungsmethoden beschäftigen. 
	

		\section{Aufbau und Durchführung}
			\subsection{Untersuchte Hyperparameter}
				Bei den Versuchen werden die Werte verschiedener, allerdings nicht aller, Hyperparameter verändert. So werden für alle Typen die Lernrate und die Größe der Mini-Batches verändert. Des Weiteren wird auch der Aufbau der KNNs verändert, wobei die verschiedenen Modelle in Tabellen in Anhang C zu finden sind. Nicht verändert wird bei den Experimenten u. a. die Anzahl der Epochs oder die Cost-Funktion.\footnote{Das liegt daran, dass ihre Anzahl in Zusammenhang mit der Lernrate und der Größe der Mini-Batches steht (vgl. Rashid, 2017, S. 157)} 
				%, wobei die Mengen der Werte für diese aus Vorversuchen und der Literatur ermittelt wurden und für alle Typen gleich sind
				\\ \ \\ \
				Speziell bei den FFNNs wird der Einfluss verschiedener Aktivierungsfunktionen und das Benutzen von Biases getestet. Bei den Experimenten zu CNNs haben diese Hyperparameter fixe Werte, der Fokus liegt hier v. a. auf der Größe des Kernels und dem Stride-Wert, da sonst durch die Kombination weiterer Hyperparameter die Experimente zu viel Zeit in Anspruch genommen hätten. Des Weiteren ist auch das Zero Padding so eingestellt, dass die Eingaben so gepaddet werden, dass die Feature Maps gleichgroß wie die Eingaben sind. Bei LSTM Netzen wird der Einfluss der Aktivierungsfunktion und der rekurrenten Aktivierungsfunktion\footnote{$f_{i;(l)}$, $f_{o;(l)}$, $f_{u;(l)}$ werden nachfolgend als rekurrente Aktivierungsfunktionen, $f_{x;(l)}$ und $f_{z;(l)}$ als Aktivierungsfunktionen bezeichnet. Dass die einzelnen Funktionen bei den Experimenten nicht getrennt voneinander behandelt werden liegt an einer Limitierung der Programmierung.} erforscht.
				% Des Weiteren wird für Pooling-Funktion nur Max-Pooling und eine fixe Anzahl von zehn Feature Maps verwendet
	
			\subsection{Python und Keras}\label{Python&Keras}
				Umgesetzt werden die Experimente mit der Programmiersprache Python, welche für die Arbeit mit KNNs relativ häufig eingesetzt wird.\footnote{vgl. Rashid, 2017, S. 95} Außer Python wird die Library Keras verwendet, welche eine kompakte Schreibweise ermöglicht und durch Verwendung einer weiteren Library namens Tensorflow die nötigen Berechnungen beschleunigt.\footnote{vgl. Wartala, 2018, S. 124}
				
			\subsection{Aufbau des Programmcodes}
				Der Programmcode für alle Netztypen ist gleich strukturiert. Zunächst wird, neben einigen nebensächlichen Aufgaben, eine Klasse definiert, welche die Arbeit des Erstellens und Trainierens eines KNNs sowie das Niederschreiben der Ergebnisse in CSV-Dateien (die wichtigsten Tabellen sind in Anhang E zu finden) übernimmt. Anschließend wird mit mehreren Schleifen durch die verschiedenen, festgelegten Werte für die Hyperparameter iteriert. Dieses Verfahren wird als Grid Search bezeichnet.\footnote{\fundamentals{32}} 

				%Die aus den Experimenten gewonnen Daten werden in CSV-Dateien gespeichert. Eine davon ist eine Tabelle für jeden Netztypus mit allen möglichen Kombinationen, für welche die Trainingszeit, die Cost-Werte und die Trefferquoten für alle drei Mengen festgehalten werden, welche in Anhang E zu finden sind. Zusätzlich wird in einer weiteren Datei gespeichert, wie viele Netze jede der Ziffern der Testmenge nicht erkannt haben. Des Weiteren wird für jedes KNN in einer eigenen Tabelle gespeichert, bei welchen Ziffern der Testmenge es falsch liegt und wie es diese stattdessen klassifiziert.
				
			\subsection{Durchführung}
				Durchgeführt wurden die Experimente auf einem Heimcomputer mit etwa 190\break GFLOPS\footnote{Messung eines Intel Core i7-7700K Prozessors mit dem Intel MKL Linpack Benchmark, verfügbar unter: https://software.intel.com/en-us/articles/intel-mkl-benchmarks-suite (Zuletzt besucht am 20. 01. 2019)}. Dabei wurden 378 FFNNs, 576 CNNs und 144 LSTMs in etwa 15 Stunden Rechenzeit getestet.
	
	
		\section{Auswertung der Ergebnisse}
			In den folgenden Unterpunkten werden die Auswirkungen der Hyperparameter auf die Trefferquoten der Netze analysiert. Die Boxplots der Abbildungen \ref{ErsterBoxplot} - \ref{LetzterBoxplot} visualisieren die Ergebnisse aus den Experimenten.

			\subsection{Allgemeine Erkenntnisse zu Lernrate und Mini-Batch Größe}
				Die Versuche zu allen drei Netztypen haben bezüglich Lernrate und Größe der Mini-Batches bis auf eine Abweichung die gleichen Ergebnisse geliefert. Die leistungsstärksten KNNs der jeweiligen Netztypen haben alle den Wert 1, den größten der drei möglichen, für die Learning Rate verwendet, wobei mit sinkender Lernrate auch die durchschnittliche Trefferquote sank.
				
				\begin{figure}[h] 
					%Vorlage: Practitioner S. 65-70
					\centering
					\begin{subfigure}[t]{.32\linewidth}
						\centering
						\begin{scriptsize}
							\begin{tikzpicture}
								\begin{axis}
									[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=36mm,width=1.1\textwidth,ytick={1,2,3},yticklabels={1, 0.1, 0.01},]
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.918,
										upper quartile=0.9325,
										lower quartile=0.8943,
										upper whisker=0.9755,
										lower whisker=0.1012,
									},
									] coordinates {};
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.8548,
										upper quartile=0.9075,
										lower quartile=0.4966,
										upper whisker=0.9434,
										lower whisker=0.1033,
									},
									] coordinates {};
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.3675,
										upper quartile=0.6709,
										lower quartile=0.1733,
										upper whisker=0.8948,
										lower whisker=0.0650,
									},
									] 
									coordinates {};
								\end{axis}
							\end{tikzpicture}
						\end{scriptsize}
						\caption{FFNNs}
					\end{subfigure}
					\hfill
					\begin{subfigure}[t]{.32\linewidth}
						\centering
						\begin{scriptsize}
							\begin{tikzpicture}
								\begin{axis}
									[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=36mm,width=1.1\textwidth,ytick={1,2,3},yticklabels={1, 0.1, 0.01},]
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.9114,
										upper quartile=0.9644,
										lower quartile=0.1552,
										upper whisker=0.9854,
										lower whisker=0.0892,
									},
									] coordinates {};
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.8664,
										upper quartile=0.9227,
										lower quartile=0.4827,
										upper whisker=0.9818,
										lower whisker=0.0981,
									},
									] coordinates {};
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.3235,
										upper quartile=0.8343,
										lower quartile=0.1461,
										upper whisker=0.9454,
										lower whisker=0.0476,
									},
									] 
									coordinates {};
								\end{axis}
							\end{tikzpicture}
						\end{scriptsize}
						\caption{CNNs}
					\end{subfigure}
					\hfill
					\begin{subfigure}[t]{.32\linewidth}
						\centering
						\begin{scriptsize}
							\begin{tikzpicture}
								\begin{axis}
									[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=36mm,width=1.1\textwidth,ytick={1,2,3},yticklabels={1, 0.1, 0.01},]
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.1359,
										upper quartile=0.6315,
										lower quartile=0.1135,
										upper whisker=0.9503,
										lower whisker=0.1028,
									},
									] coordinates {};
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.1136,
										upper quartile=0.1349,
										lower quartile=0.1135,
										upper whisker=0.685,
										lower whisker=0.0980,
									},
									] coordinates {};
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.1135,
										upper quartile=0.1223,
										lower quartile=0.1031,
										upper whisker=0.1867,
										lower whisker=0.0958,
									},
									] 
									coordinates {};
								\end{axis}
							\end{tikzpicture}
						\end{scriptsize}
						\caption{LSTMs}
					\end{subfigure}
					\vspace{-1mm}
					\caption{Boxplots der Trefferquoten der verschiedenen KNN-Arten mit verschiedenen Lernraten (Quelle: Eigene Darstellungen)}\label{ErsterBoxplot}
				\end{figure}
				
				Als Größe für die Mini-Batches hat sich bei den FFNNs und LSTMs der Wert 8 durchgesetzt, bei den CNNs verwendete das Netz mit der höchsten Trefferquote eine Mini-Batch Größe von 32, wobei es zum besten CNN mit der Mini-Batch Größe 8 weniger als einen halben Prozentpunkt Unterschied gab.
				
				\begin{figure}[h] 
					%Vorlage: Practitioner S. 65-70
					\centering
					\begin{subfigure}[t]{.32\linewidth}
						\centering
						\begin{scriptsize}
							\begin{tikzpicture}
								\begin{axis}
									[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=36mm,width=1.1\textwidth,ytick={1,2,3},yticklabels={128, 32, 8}, ]
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.478,
										upper quartile=0.8834,
										lower quartile=0.2104,
										upper whisker=0.9347,
										lower whisker=0.0650,
									},
									] coordinates {};
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.8391,
										upper quartile=0.9117,
										lower quartile=0.4096,
										upper whisker=0.9616,
										lower whisker=0.1024,
									},
									] coordinates {};
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.9027,
										upper quartile=0.9276,
										lower quartile=0.6774,
										upper whisker=0.9755,
										lower whisker=0.1012,
									},
									] 
									coordinates {};
								\end{axis}
							\end{tikzpicture}
						\end{scriptsize}
						\caption{FFNNs}
					\end{subfigure}
					\hfill
					\begin{subfigure}[t]{.32\linewidth}
						\centering
						\begin{scriptsize}
							\begin{tikzpicture}
								\begin{axis}
									[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=36mm,width=1.1\textwidth,ytick={1,2,3},yticklabels={128, 32, 8}, ]
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.5615,
										upper quartile=0.9046,
										lower quartile=0.1904,
										upper whisker=0.9818,
										lower whisker=0.0476,
									},
									] 
									coordinates {};
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.8348,
										upper quartile=0.9220,
										lower quartile=0.2127,
										upper whisker=0.9854,
										lower whisker=0.0773,
									},
									] coordinates {};
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.8580,
										upper quartile=0.9456,
										lower quartile=0.2172,
										upper whisker=0.9818,
										lower whisker=0.0892,
									},
									] coordinates {};
								\end{axis}
							\end{tikzpicture}
						\end{scriptsize}
						\caption{CNNs}
					\end{subfigure}
					\hfill
					\begin{subfigure}[t]{.32\linewidth}
						\centering
						\begin{scriptsize}
							\begin{tikzpicture}
								\begin{axis}
									[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=36mm,width=1.1\textwidth,ytick={1,2,3},yticklabels={128, 32, 8}, ]
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.1135,
										upper quartile=0.1277,
										lower quartile=0.1135,
										upper whisker=0.410,
										lower whisker=0.0958,
									},
									] coordinates {};
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.1138,
										upper quartile=0.1801,
										lower quartile=0.1135,
										upper whisker=0.886,
										lower whisker=0.0958,
									},
									] coordinates {};
									\addplot [mark=*, boxplot,
									boxplot prepared={
										median=0.1152,
										upper quartile=0.4593,
										lower quartile=0.1135,
										upper whisker=0.950,
										lower whisker=0.0982,
									},
									] 
									coordinates {};
								\end{axis}
							\end{tikzpicture}
						\end{scriptsize}
						\caption{LSTMs}
					\end{subfigure}
					\caption{Boxplots der Trefferquoten der verschiedenen KNN-Arten mit verschiedenen Mini-Batch Größen (Quelle: Eigene Darstellungen)}
				\end{figure}
	
				% Der Zusammenhang zwischen Learning Rate und Trefferquote eines KNNs hat auch mit anderen Hyperparametern zu tun. Wie oft es zu einer Veränderung der Parameter eines KNNs kommt, hängt von der Anzahl an Epochs und der Mini-Batch Größe ab: Je kleiner die Mini-Batches und je mehr Epochs beim Training eines Netzes, desto öfter kommt es zu einer Aktualisierung der Parameter. Je öfter eine Aktualisierung stattfindet, desto kleiner darf der Wert der Learning Rate sein.\footnote{vgl. Rashid, 2017, S. 157}
	
			\subsection{Experimente zu FFNNs}
				Bezüglich des Aufbaus der FFNNs fällt auf, dass das beste FFNN (Nr. 109, 97,55\%) nur einen Hidden Layer mit 1000 Neuronen verwendet, welches jenes Modell mit den meisten Parametern ist. 
				%Bei den Netzen mit zwei Layern kommt es v.a. auf die Reihenfolge der verschieden großen Schichten an: Modell fünf besteht aus zuerst einem Hidden Layer mit 100, dann einem mit zehn Neuronen, Modell sechs genau umgekehrt. 
	
				\begin{figure}[h]
					\begin{scriptsize}
						\begin{tikzpicture}
							\begin{axis}
								[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=56mm,width=\textwidth,ytick={1,2,3,4,5,6,7},yticklabels={Modell 7, Modell 6, Modell 5, Modell 4, Modell 3, Modell 2, Modell 1},]
								\addplot [mark=*, boxplot,%Modell7
								boxplot prepared={
									median=0.849,
									upper quartile=0.9243,
									lower quartile=0.3008,
									upper whisker=0.9657,
									lower whisker=0.0973,
								},
								] 
								coordinates {};
								\addplot [mark=*, boxplot,%Modell6
								boxplot prepared={
									median=0.499,
									upper quartile=0.9130,
									lower quartile=0.1857,
									upper whisker=0.9278,
									lower whisker=0.1009,
								},
								] 
								coordinates {};
								\addplot [mark=*, boxplot,%Modell5
								boxplot prepared={
									median=0.687,
									upper quartile=0.9195,
									lower quartile=0.2619,
									upper whisker=0.9557,
									lower whisker=0.098,
								},
								] 
								coordinates {};
								\addplot [mark=*, boxplot,%Modell4
								boxplot prepared={
									median=0.610,
									upper quartile=0.8993,
									lower quartile=0.1927,
									upper whisker=0.922,
									lower whisker=0.0676,
								},
								] 
								coordinates {};
								\addplot [mark=*, boxplot,%Modell3
								boxplot prepared={
									median=0.894,
									upper quartile=0.9232,
									lower quartile=0.7876,
									upper whisker=0.9755,
									lower whisker=0.2651,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,%Modell2
								boxplot prepared={
									median=0.884,
									upper quartile=0.9275,
									lower quartile=0.6929,
									upper whisker=0.9707,
									lower whisker=0.065,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,%Modell1
								boxplot prepared={
									median=0.809,
									upper quartile=0.9084,
									lower quartile=0.4351,
									upper whisker=0.924,
									lower whisker=0.1375,
								},
								] 
								coordinates {};
							\end{axis}
						\end{tikzpicture}
					\end{scriptsize}
					\vspace{-5mm}
					\caption{Boxplots der Trefferquoten der verschiedenen Aufbauten von \\ FFNNs (Quelle: E. D.)}
				\end{figure}
				
				Neben verschiedenen Aufbauten wurden auch drei unterschiedliche Aktivierungsfunktionen getestet, wobei jenes Netz mit der höchsten Trefferquote die Rectified Linear-Funktion verwendet, jedoch schnitt die TanH-Funktion im Durchschnitt mit einer um zehn Prozentpunkte höheren Trefferquote deutlich besser ab, das beste FFNN mit der TanH-Funktion liegt nur etwa einen Prozentpunkt dahinter. Die Sigmoid-Funktion schnitt am schlechtesten ab.  
				%, was auch im Einklang mit der Literatur steht und deshalb von dieser Funktion abrät. 
	
				\begin{figure}[h!]
					\begin{scriptsize}
						\begin{tikzpicture}
							\begin{axis}
								[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=33mm,width=151mm,ytick={1,2,3},yticklabels={ReL-F., TanH-F., $\sigma$-F.},]
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.8741,
									upper quartile=0.9166,
									lower quartile=0.4395,
									upper whisker=0.9755,
									lower whisker=0.1012,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.894,
									upper quartile=0.9220,
									lower quartile=0.7097,
									upper whisker=0.9657,
									lower whisker=0.1788,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.3739,
									upper quartile=0.8648,
									lower quartile=0.1617,
									upper whisker=0.9438,
									lower whisker=0.065,
								},
								] 
								coordinates {};
							\end{axis}
						\end{tikzpicture}
					\end{scriptsize}
					\vspace{-5mm}
					\caption{Boxplots der Trefferquoten von FFNNs mit verschiedenen Aktivierungsfunktionen (Quelle: E. D.)}
				\end{figure}
	
				Des Weiteren wurde auch der Einsatz von Biases getestet, wobei auffiel, dass die durchschnittliche Trefferquote bei FFNNs ohne Biases um ca. 1,6 Prozentpunkte höher ist als bei jenen mit Biases. Bei den beiden besten FFNNs gibt es einen Unterschied von weniger als einem halben Prozentpunkt. Insgesamt gibt es also keinen großen Unterschied, ob man FFNNs mit oder ohne Biases trainiert, was erklären könnte, warum dieser Parameter nicht in jeder Literatur verwendet wird. 
	
				\begin{figure}[h] 
					%Vorlage: Practitioner S. 65-70
					\begin{scriptsize}
						\begin{tikzpicture}
							\begin{axis}
								[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=27mm,width=146mm,ytick={1,2},yticklabels={Ohne Biases, Mit Biases},]
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.8425,
									upper quartile=0.9139,
									lower quartile=0.3637,
									upper whisker=0.9755,
									lower whisker=0.0980,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.8186,
									upper quartile=0.9125,
									lower quartile=0.3400,
									upper whisker=0.9719,
									lower whisker=0.0650,
								},
								] coordinates {};
							\end{axis}
						\end{tikzpicture}
					\end{scriptsize}
					\vspace{-5mm}
					\caption{Boxplots der Trefferquoten von FFNNs mit und ohne Biases \\ (Quelle: E. D.)}
				\end{figure}
	
			\subsection{Experimente zu CNNs}
				Interessanterweise hatten CNNs mit einfacheren Aufbauten (Modell 1) eine höhere durchschnittliche Trefferquote als andere, das beste CNN (Nr. 173; 98,54\%) verwendet zwei hintereinanderliegende Convolutional Layer, gefolgt von einem Pooling Layer. Jedoch liegen die besten Netze der vier Modelle keinen halben Prozentpunkt auseinander. 
				
				\begin{figure}[h!]
					\begin{scriptsize}
						\begin{tikzpicture}
							\begin{axis}
								[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=40mm,width=151mm,ytick={1,2,3,4},yticklabels={Modell 4, Modell 3, Modell 2, Modell 1},]
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.6077,
									upper quartile=0.9191,
									lower quartile=0.1564,
									upper whisker=0.9818,
									lower whisker=0.0892,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.4707,
									upper quartile=0.9005,
									lower quartile=0.1767,
									upper whisker=0.9822,
									lower whisker=0.0723,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.6402,
									upper quartile=0.9041,
									lower quartile=0.1720,
									upper whisker=0.9854,
									lower whisker=0.0476,
								},
								] 
								coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.9051,
									upper quartile=0.9482,
									lower quartile=0.8216,
									upper whisker=0.981,
									lower whisker=0.1023,
								},
								] 
								coordinates {};
							\end{axis}
						\end{tikzpicture}
					\end{scriptsize}
					\vspace{-5mm}
					\caption{Boxplots der Trefferquoten der verschiedenen Aufbauten von \\ CNNs (Quelle: E. D.)}
				\end{figure}
	
				Die Größe der Kernels hat kaum einen Einfluss, weder beim Durchschnitt noch bei den besten CNNs gibt es nicht mehr als zwei Prozentpunkte Unterschied, das beste Netz verwendet $3 \times 3$ große Kernels.
				
				\begin{figure}[h!]
					\begin{scriptsize}
						\begin{tikzpicture}
							\begin{axis}
								[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=40mm,width=156mm,ytick={1,2,3,4},yticklabels={$3 \times 3$, $3 \times 2$, $2 \times 3$, $2 \times 2$},]
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.7497,
									upper quartile=0.9434,
									lower quartile=0.1872,
									upper whisker=0.9854,
									lower whisker=0.0723,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.7570,
									upper quartile=0.9257,
									lower quartile=0.2171,
									upper whisker=0.9822,
									lower whisker=0.0476,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.7763,
									upper quartile=0.9245,
									lower quartile=0.2058,
									upper whisker=0.9812,
									lower whisker=0.0892,
								},
								] 
								coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.7536,
									upper quartile=0.9124,
									lower quartile=0.1982,
									upper whisker=0.9784,
									lower whisker=0.0682,
								},
								] 
								coordinates {};
							\end{axis}
						\end{tikzpicture}
					\end{scriptsize}
					\vspace{-5mm}
					\caption{Boxplots der Trefferquoten von CNNs mit unterschiedlichen Kernels (Quelle: E. D.)}
				\end{figure}
	
				Viel einflussreicher ist der Stride-Wert, wobei mit größeren Stride-Werten die durchschnittliche Trefferquote der CNNs sinkt, weshalb das leistungsstärkste CNN auch den kleinsten Stride-Wert $(1,1)$ verwendet. Dies lässt sich möglicherweise dadurch erklären, dass Netze mit größerem Stride-Wert kleinere Feature Maps haben und deshalb auch weniger Parameter verwenden, wodurch wiederum die Lernkapazität eingeschränkt ist.
				
				\begin{figure}[h!]
					\begin{scriptsize}
						\begin{tikzpicture}
							\begin{axis}
								[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=40mm,width=156mm,ytick={1,2,3,4},yticklabels={$2 \times 2$, $2 \times 1$, $1 \times 2$, $1 \times 1$},]
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.2809,
									upper quartile=0.7262,
									lower quartile=0.1535,
									upper whisker=0.9686,
									lower whisker=0.0723,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.5774,
									upper quartile=0.8880,
									lower quartile=0.1882,
									upper whisker=0.976,
									lower whisker=0.0682,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.7308,
									upper quartile=0.9041,
									lower quartile=0.1689,
									upper whisker=0.9764,
									lower whisker=0.0476,
								},
								] 
								coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.9270,
									upper quartile=0.9731,
									lower quartile=0.9012,
									upper whisker=0.9854,
									lower whisker=0.1135,
								},
								] 
								coordinates {};
							\end{axis}
						\end{tikzpicture}
					\end{scriptsize}
					\vspace{-5mm}
					\caption{Boxplots der Trefferquoten von CNNs mit unterschiedlichen Stride-Werten (Quelle: E. D.)}
				\end{figure}
	
			\subsection{Experimente zu LSTMs}
				Bei den LSTMs haben zwar  Modelle mit zwei LSTM-Schichten durchschnittlich schlechter abgeschnitten als jene mit nur einem, jedoch haben diese höhere maximale Trefferquoten erzielt und das beste Netz hervorgebracht (Nr. 39; 95,03\%). Das Verwenden eines Fully-Connected Layers direkt nach dem Input Layer scheint die Leistung eines LSTMs nur zu beeinträchtigen. 
				
				\enlargethispage{\baselineskip}
				\begin{figure}[h!]
					\begin{scriptsize}
						\begin{tikzpicture}
							\begin{axis}
								[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=40mm,width=151mm,ytick={1,2,3,4},yticklabels={Modell 4, Modell 3, Modell 2, Modell 1},]
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.1135,
									upper quartile=0.1155,
									lower quartile=0.1135,
									upper whisker=0.9494,
									lower whisker=0.098,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.1135,
									upper quartile=0.1707,
									lower quartile=0.1129,
									upper whisker=0.9148,
									lower whisker=0.0958,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.1135,
									upper quartile=0.1549,
									lower quartile=0.1135,
									upper whisker=0.9503,
									lower whisker=0.0958,
								},
								] 
								coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.1217,
									upper quartile=0.2067,
									lower quartile=0.1153,
									upper whisker=0.9078,
									lower whisker=0.098,
								},
								] 
								coordinates {};
							\end{axis}
						\end{tikzpicture}
					\end{scriptsize}
					\vspace{-5mm}
					\caption{Boxplots der Trefferquoten der verschiedenen Aufbauten von LSTM Netzen (Quelle: E. D.)}
				\end{figure}
	
				Als Funktion für $f_{x;(l)}$ und $f_{z;(l)}$ ist die TanH-Funktion deutlich besser geeignet als die Sigmoid-Funktion, bei der rekurrenten Aktivierungsfunktion ist es jedoch genau umgekehrt. Für diese ist die Sigmoid-Funktion besser geeignet. 
	
				\begin{figure}[h] 
					% Vorlage: Practitioner S. 65-70
					\begin{scriptsize}
						\begin{tikzpicture}
							\begin{axis}
								[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=27mm,width=151mm,ytick={1,2},yticklabels={TanH-F.,$\sigma$-F.},]
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.129,
									upper quartile=0.4079,
									lower quartile=0.1135,
									upper whisker=0.9503,
									lower whisker=0.1026,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.114,
									upper quartile=0.1161,
									lower quartile=0.1129,
									upper whisker=0.8559,
									lower whisker=0.0958,
								},
								] coordinates {};
							\end{axis}
						\end{tikzpicture}
					\end{scriptsize}
					\vspace{-5mm}
					\caption{Boxplots der Trefferquoten von LSTMs mit unterschiedlichen Aktivierungsfunktionen (Quelle: E. D.)}
				\end{figure}
				
				\begin{figure}[h] 
					% Vorlage: Practitioner S. 65-70
					\begin{scriptsize}
						\begin{tikzpicture}
							\begin{axis}
								[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=27mm,width=151mm,ytick={1,2},yticklabels={TanH-F.,$\sigma$-F.},]
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.114,
									upper quartile=0.1243,
									lower quartile=0.1135,
									upper whisker=0.8559,
									lower whisker=0.098,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.120,
									upper quartile=0.2077,
									lower quartile=0.1135,
									upper whisker=0.9503,
									lower whisker=0.0958,
								},
								] coordinates {};
							\end{axis}
						\end{tikzpicture}
					\end{scriptsize}
					\vspace{-5mm}
					\caption{Boxplots der Trefferquoten von LSTMs mit unterschiedlichen rekurrenten Aktivierungsfunktionen (Quelle: E. D.)}
				\end{figure}
	
				Betrachtet man nun die vier möglichen Kombinationen der Funktionen, so schneiden LSTM Netze, welche für die Aktivierungsfunktion und die rekurrente Aktivierungsfunktion die gleiche Funktion einsetzen, schlechter ab, als LSTMs bei denen sich diese unterscheiden. 
				
				\begin{figure}[h!]
					\begin{scriptsize}
						\begin{tikzpicture}
							\begin{axis}
								[tick align=outside, ytick pos=left, xtick pos=left, xmin=0, xmax=1, major tick length=1mm, height=40mm,width=136mm,ytick={1,2,3,4},yticklabels={TanH-F. \& TanH-F., TanH-F. \& $\sigma$-F., $\sigma$-F. \& TanH-F., $\sigma$-F. \& $\sigma$-F.},]
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.1136,
									upper quartile=0.1175,
									lower quartile=0.1135,
									upper whisker=0.6428,
									lower whisker=0.1028,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.2087,
									upper quartile=0.6767,
									lower quartile=0.1390,
									upper whisker=0.9503,
									lower whisker=0.1026,
								},
								] coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.1135,
									upper quartile=0.1276,
									lower quartile=0.1135,
									upper whisker=0.8559,
									lower whisker=0.098,
								},
								] 
								coordinates {};
								\addplot [mark=*, boxplot,
								boxplot prepared={
									median=0.1135,
									upper quartile=0.1135,
									lower quartile=0.1028,
									upper whisker=0.2054,
									lower whisker=0.096,
								},
								] 
								coordinates {};
							\end{axis}
						\end{tikzpicture}
					\end{scriptsize}
					\vspace{-5mm}
					\caption{Boxplots der Trefferquoten der LSTM Netze mit den vier möglichen Kombinationen (Aktivierungsfunktion links, rekurrekte Aktivierungsfunktion rechts) (Quelle: E. D.)}\label{LetzterBoxplot}
				\end{figure}
	

		\section{Erkenntnisse über den MNIST-Datensatz}
			Die Abbildungen \ref{top5FFNN} - \ref{top5LSTM} zeigen jeweils jene fünf Ziffern, welche vom jeweiligen Netztypus am häufigsten falsch klassifiziert wurden. In der jeweiligen Unterbildunterschrift steht die Nummer der MNIST-Ziffer, gefolgt von der dargestellten Ziffer und die Anzahl der KNNs des Netztypus, welche diese Ziffer nicht erkannt haben. Dabei fällt auf, dass die Ziffern Fünf und Neun häufig vorkommen und die Ziffer Nr. 9982 in allen drei Listen vorkommt. Betrachtet man die Ziffern genauer, fällt es möglicherweise auch einem Menschen schwer, genau zu sagen, um welche Ziffer es sich handelt. Die Abbildungen 25a und 27d beispielsweise könnten auch ungenau geschriebene Nullen sein. Es ließe sich bei diesen Beispielen die Frage stellen, wo man genau die Grenze zwischen einer undeutlich geschriebenen Sechs und einer undeutlich geschriebenen Null zieht (Siehe Abb. \ref{6zu0Beispiel}). Ein weiteres Beispiel: Das beste KNN, welches diese Arbeit hervorbrachte, ist ein CNN mit einer Trefferquote von 98,54\%, d.h. nur 146 Ziffern der 10.000 Testbeispiele wurde falsch kategorisiert, welche in Abb. \ref{146ziffern} dargestellt sind. 
			\\ \
			\\ \
			Um nun Einzuschätzen, wie gut die besten Netze der drei Netztypen sind, werden diese mit vom Aufbau vergleichbaren Netzen der Ersteller des MNIST-Datensatzes verglichen. Diese erzielen Trefferquoten zwischen 95,3\% und bis zu 99,77\%, d.h. die besten KNNs dieser Arbeit sind durchaus auf Niveau anderer Forschungsergebnisse.\footnote{vgl. Burges, Cortes \& LeCun, 1998} Auch Menschen können keine Trefferquote von 100\% erzielen. Die sogenannte Human-Level Performance für MNIST liegt bei etwa 99,75\%.\footnote{vgl. Wartala, 2018, S. 126f} 
			
			\begin{figure}[h]
				\vspace{0.0cm} \centering
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5FFNN/6-2118.png}
					\caption{2118: 6 (377)}
				\end{subfigure}
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5FFNN/6-3853.png}
					\caption{3853: 6 (376)}
				\end{subfigure}
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5FFNN/5-9982.png}
					\caption{9982: 5 (375)}
				\end{subfigure}
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5FFNN/5-9729.png}
					\caption{9729: 5 (375)}
				\end{subfigure}
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5FFNN/9-6166.png}
					\caption{6166: 9 (375)}
				\end{subfigure}
				
				\vspace*{-2mm}
				\caption{Top 5 der von FFNNs falsch erkannten Ziffern (Quelle: E. D.)}\label{top5FFNN}
			\end{figure}
			\vspace*{-3mm}
			\begin{figure}[h]
				\vspace{0.0cm} \centering
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5CNN/5-9982.png}
					\caption{9982: 5 (568)}
				\end{subfigure}
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5CNN/9-1247.png}
					\caption{1247: 9 (564)}
				\end{subfigure}
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5CNN/9-3597.png}
					\caption{3597: 9 (562)}
				\end{subfigure}
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5CNN/9-2293.png}
					\caption{2293: 9 (560)}
				\end{subfigure}
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5CNN/9-6505.png}
					\caption{6505: 9 (559)}
				\end{subfigure}
				
				\vspace*{-2mm}
				\caption{Top 5 der von CNNs falsch erkannten Ziffern (Quelle: E. D.)}\label{top5CNN}
			\end{figure}
			\vspace*{-3mm}
			\begin{figure}[h]
				\vspace{0.0cm} \centering
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5LSTM/5-9770.png}
					\caption{9770: 5 (144)}
				\end{subfigure}
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5LSTM/5-3893.png}
					\caption{3893: 5 (144)}
				\end{subfigure}
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5LSTM/5-2573.png}
					\caption{2573: 5 (144)}
				\end{subfigure}
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5LSTM/5-1299.png}
					\caption{1299: 5 (144)}
				\end{subfigure}
				\begin{subfigure}[t]{.19\linewidth}
					\centering
					\includegraphics[height=0.6\linewidth]{imagesAndGraphData/top5LSTM/5-9982.png}
					\caption{9982: 5 (143)}
				\end{subfigure}
				
				\vspace*{-2mm}
				\caption{\mbox{Top 5 der von LSTMs falsch erkannten Ziffern (Quelle: E. D.)}}\label{top5LSTM}
			\end{figure}
			
			\begin{figure}[h!]
				\vspace{0.0cm} \centering
				\begin{subfigure}[h]{0.11\linewidth}
					\includegraphics[height=\linewidth]{imagesAndGraphData/6zu0/6-21.png}
				\end{subfigure}
				\begin{subfigure}[h]{0.11\linewidth}
					\includegraphics[height=\linewidth]{imagesAndGraphData/6zu0/6-439.png}
				\end{subfigure}
				\begin{subfigure}[h]{0.11\linewidth}
					\includegraphics[height=\linewidth]{imagesAndGraphData/6zu0/6-3130.png}
				\end{subfigure}
				\begin{subfigure}[h]{0.11\linewidth}
					\includegraphics[height=\linewidth]{imagesAndGraphData/6zu0/6-965.png}
				\end{subfigure}
				\unskip\ \vrule\ 
				\begin{subfigure}[h]{0.11\linewidth}
					\includegraphics[height=\linewidth]{imagesAndGraphData/6zu0/0-1061.png}
				\end{subfigure}
				\begin{subfigure}[h]{0.11\linewidth}
					\includegraphics[height=\linewidth]{imagesAndGraphData/6zu0/0-5415.png}
				\end{subfigure}
				\begin{subfigure}[h]{0.11\linewidth}
					\includegraphics[height=\linewidth]{imagesAndGraphData/6zu0/0-1533.png}
				\end{subfigure}
				\begin{subfigure}[h]{0.11\linewidth}
					\includegraphics[height=\linewidth]{imagesAndGraphData/6zu0/0-1271.png}
				\end{subfigure}
				
				\caption{Verlauf von Sechs zu Null. Die Ziffern links vom Strich sind Sechsen, rechts davon Nullen des Testdatensatzes (Quelle: E. D.)}\label{6zu0Beispiel}
			\end{figure}
			
			\vspace*{\fill}
			\begin{figure}[h]
				\vspace{0.0cm} \centering
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/3-18.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{3}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-211.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-320.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/3-449.png} \put (85,75) {\footnotesize{\textcolor{white}{5}}} \put (0,75) {\footnotesize{\textcolor{white}{3}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-479.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/8-582.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{8}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-593.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/1-619.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{1}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/2-659.png} \put (85,75) {\footnotesize{\textcolor{white}{1}}} \put (0,75) {\footnotesize{\textcolor{white}{2}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-684.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/0-717.png} \put (85,75) {\footnotesize{\textcolor{white}{6}}} \put (0,75) {\footnotesize{\textcolor{white}{0}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-720.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-726.png} \put (85,75) {\footnotesize{\textcolor{white}{5}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-740.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/8-844.png} \put (85,75) {\footnotesize{\textcolor{white}{7}}} \put (0,75) {\footnotesize{\textcolor{white}{8}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-846.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/3-883.png} \put (85,75) {\footnotesize{\textcolor{white}{5}}} \put (0,75) {\footnotesize{\textcolor{white}{3}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/6-1014.png} \put (85,75) {\footnotesize{\textcolor{white}{5}}} \put (0,75) {\footnotesize{\textcolor{white}{6}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-1039.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/6-1044.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{6}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-1112.png} \put (85,75) {\footnotesize{\textcolor{white}{6}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/6-1182.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{6}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-1226.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-1232.png} \put (85,75) {\footnotesize{\textcolor{white}{4}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-1242.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-1247.png} \put (85,75) {\footnotesize{\textcolor{white}{5}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-1260.png} \put (85,75) {\footnotesize{\textcolor{white}{1}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-1299.png} \put (85,75) {\footnotesize{\textcolor{white}{7}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-1326.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-1393.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-1414.png} \put (85,75) {\footnotesize{\textcolor{white}{7}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-1500.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/1-1527.png} \put (85,75) {\footnotesize{\textcolor{white}{5}}} \put (0,75) {\footnotesize{\textcolor{white}{1}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/8-1530.png} \put (85,75) {\footnotesize{\textcolor{white}{7}}} \put (0,75) {\footnotesize{\textcolor{white}{8}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/0-1621.png} \put (85,75) {\footnotesize{\textcolor{white}{6}}} \put (0,75) {\footnotesize{\textcolor{white}{0}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-1709.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-1737.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-1754.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/8-1878.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{8}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-1901.png} \put (85,75) {\footnotesize{\textcolor{white}{4}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-2035.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-2040.png} \put (85,75) {\footnotesize{\textcolor{white}{6}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-2043.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-2053.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-2070.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/3-2109.png} \put (85,75) {\footnotesize{\textcolor{white}{7}}} \put (0,75) {\footnotesize{\textcolor{white}{3}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-2129.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-2130.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/6-2135.png} \put (85,75) {\footnotesize{\textcolor{white}{1}}} \put (0,75) {\footnotesize{\textcolor{white}{6}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/0-2185.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{0}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-2189.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/1-2266.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{1}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-2293.png} \put (85,75) {\footnotesize{\textcolor{white}{0}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-2406.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-2414.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/2-2488.png} \put (85,75) {\footnotesize{\textcolor{white}{4}}} \put (0,75) {\footnotesize{\textcolor{white}{2}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-2597.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-2607.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/6-2654.png} \put (85,75) {\footnotesize{\textcolor{white}{1}}} \put (0,75) {\footnotesize{\textcolor{white}{6}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-2742.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-2743.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-2760.png} \put (85,75) {\footnotesize{\textcolor{white}{4}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/3-2770.png} \put (85,75) {\footnotesize{\textcolor{white}{5}}} \put (0,75) {\footnotesize{\textcolor{white}{3}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/3-2921.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{3}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-2939.png} \put (85,75) {\footnotesize{\textcolor{white}{0}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-2970.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/6-2995.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{6}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/6-3030.png} \put (85,75) {\footnotesize{\textcolor{white}{0}}} \put (0,75) {\footnotesize{\textcolor{white}{6}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-3060.png} \put (85,75) {\footnotesize{\textcolor{white}{7}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/1-3073.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{1}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-3316.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/6-3422.png} \put (85,75) {\footnotesize{\textcolor{white}{0}}} \put (0,75) {\footnotesize{\textcolor{white}{6}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-3503.png} \put (85,75) {\footnotesize{\textcolor{white}{1}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/6-3520.png} \put (85,75) {\footnotesize{\textcolor{white}{4}}} \put (0,75) {\footnotesize{\textcolor{white}{6}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-3534.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-3558.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-3597.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-3730.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-3751.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-3767.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-3778.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-3806.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-3808.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-3838.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/1-3906.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{1}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-4007.png} \put (85,75) {\footnotesize{\textcolor{white}{4}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-4078.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/2-4176.png} \put (85,75) {\footnotesize{\textcolor{white}{7}}} \put (0,75) {\footnotesize{\textcolor{white}{2}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-4199.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-4238.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/3-4256.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{3}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-4360.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-4500.png} \put (85,75) {\footnotesize{\textcolor{white}{1}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/1-4507.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{1}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/6-4571.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{6}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-4575.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/3-4740.png} \put (85,75) {\footnotesize{\textcolor{white}{5}}} \put (0,75) {\footnotesize{\textcolor{white}{3}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-4761.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-4783.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/8-4807.png} \put (85,75) {\footnotesize{\textcolor{white}{0}}} \put (0,75) {\footnotesize{\textcolor{white}{8}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-4823.png} \put (85,75) {\footnotesize{\textcolor{white}{4}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-4837.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-4860.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/8-4879.png} \put (85,75) {\footnotesize{\textcolor{white}{6}}} \put (0,75) {\footnotesize{\textcolor{white}{8}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/8-4956.png} \put (85,75) {\footnotesize{\textcolor{white}{4}}} \put (0,75) {\footnotesize{\textcolor{white}{8}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-4966.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-5246.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/1-5331.png} \put (85,75) {\footnotesize{\textcolor{white}{6}}} \put (0,75) {\footnotesize{\textcolor{white}{1}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-5600.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/1-5642.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{1}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-5654.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-5752.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-5888.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-5937.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/3-5955.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{3}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-6091.png} \put (85,75) {\footnotesize{\textcolor{white}{5}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-6173.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-6505.png} \put (85,75) {\footnotesize{\textcolor{white}{0}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-6576.png} \put (85,75) {\footnotesize{\textcolor{white}{1}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/0-6597.png} \put (85,75) {\footnotesize{\textcolor{white}{7}}} \put (0,75) {\footnotesize{\textcolor{white}{0}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/0-6651.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{0}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/2-6744.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{2}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-7434.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/3-7849.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{3}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/2-8059.png} \put (85,75) {\footnotesize{\textcolor{white}{1}}} \put (0,75) {\footnotesize{\textcolor{white}{2}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-8081.png} \put (85,75) {\footnotesize{\textcolor{white}{6}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/2-8091.png} \put (85,75) {\footnotesize{\textcolor{white}{1}}} \put (0,75) {\footnotesize{\textcolor{white}{2}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/2-8094.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{2}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/4-8095.png} \put (85,75) {\footnotesize{\textcolor{white}{1}}} \put (0,75) {\footnotesize{\textcolor{white}{4}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/3-8246.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{3}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/0-8273.png} \put (85,75) {\footnotesize{\textcolor{white}{9}}} \put (0,75) {\footnotesize{\textcolor{white}{0}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/8-8408.png} \put (85,75) {\footnotesize{\textcolor{white}{6}}} \put (0,75) {\footnotesize{\textcolor{white}{8}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-9009.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-9015.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/7-9019.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{7}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-9530.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/1-9540.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{1}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-9642.png} \put (85,75) {\footnotesize{\textcolor{white}{7}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/2-9664.png} \put (85,75) {\footnotesize{\textcolor{white}{7}}} \put (0,75) {\footnotesize{\textcolor{white}{2}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/6-9679.png} \put (85,75) {\footnotesize{\textcolor{white}{3}}} \put (0,75) {\footnotesize{\textcolor{white}{6}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/9-9692.png} \put (85,75) {\footnotesize{\textcolor{white}{7}}} \put (0,75) {\footnotesize{\textcolor{white}{9}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-9729.png} \put (85,75) {\footnotesize{\textcolor{white}{6}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-9770.png} \put (85,75) {\footnotesize{\textcolor{white}{0}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/2-9811.png} \put (85,75) {\footnotesize{\textcolor{white}{8}}} \put (0,75) {\footnotesize{\textcolor{white}{2}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/0-9879.png} \put (85,75) {\footnotesize{\textcolor{white}{2}}} \put (0,75) {\footnotesize{\textcolor{white}{0}}} \end{overpic}
				\begin{overpic}[height=1cm]{imagesAndGraphData/CNN173v2/5-9982.png} \put (85,75) {\footnotesize{\textcolor{white}{6}}} \put (0,75) {\footnotesize{\textcolor{white}{5}}} \end{overpic}
				\caption{Alle 146 Ziffern, die das beste KNN (CNN Nr. 173) nicht richtig klassifizieren konnte. Links steht die korrekte Antwort, rechts jene des KNN \\ (Quelle: E. D.)}\label{146ziffern}
			\end{figure}
			\vspace*{\fill}
			
			
			% \section{Vergleich der Leistungen}
			% \subsection{LeCun}
			% \subsection{Zufall}
			% \subsection{Human-Level-Performance}
			% \section{Kritik an den Experimenten} % -> Resümee
			% Kombinationen hätten mehrmals getestet werden müssen wegen Zufall
			% Andere Hyperparameter hätten auch getesteten werden können (CNN, ...)
			% Mehr Modelle
			% Vermischen von Aktivierungsfunktion und sonsitgen Hyperparametern
			% Verändern der Lernrate während dem Lernen (LeCun)
			
			
			
	\chapter{Resümee}
		Die Ziele dieser Arbeit sind einerseits, die Funktionsweise von verschiedenen Arten von KNNs zu verstehen, und andererseits herauszufinden, wie sich diese beim MNIST-Datensatz verhalten. \\
			
		Im zweiten Kapitel werden die Neurone und ihre Gruppierung in verschiedene Schichten als grundlegendes Konzept für die KNNs dieser Arbeit erklärt. Kapitel 3 beschreibt den Aufbau von FFNNs und wie man die Ergebnisse eines solchen Netzes berechnet. Die im vierten Kapitel beschriebenen CNNs bauen auf dem FFNN auf und erweitern dieses durch spezielle Schichten, den Convolutional und Pooling Layern. Die LSTMs bearbeiten die Eingabedaten über mehrere Zeitschritte hinweg und können so durch Eingaben aus der Vergangenheit beeinflusst werden. Kapitel 6 zeigt anhand der FFNNs, wie diese durch iterative Algorithmen lernen, dass der Erfolg dieses Prozesses nicht nur vom Aufbau eines KNNs, sondern auch von den Hyperparametern abhängt und welche Probleme beim Lernen auftreten können. \\
			
		Im siebten Kapitel werden die Ergebnisse der Experimente der verschiedenen Netztypen analysiert. Dabei fällt auf, dass alle drei Netztypen trotz ihrer Unterschiede erfolgreiche Netze hervorgebracht haben. Des Weiteren zeigt sich auch der Zusammenhang zwischen der Learningrate, der Größe der Mini Batches und der Anzahl der Epochs. Bezüglich der FFNNs fällt auf, dass ihre Leistung v. a. vom Aufbau und der verwendeten Aktivierungsfunktion, nicht jedoch von der Verwendung von Biases abhängt. Die Experimente mit CNNs zeigen, dass die Trefferquote bei der Testmenge neben dem Aufbau hauptsächlich von der Stride-Größe, nicht jedoch der Größe der Kernels, abhängt. Die Leistung von LSTMs wird v. a. durch den Aufbau und die Kombination der verschiedenen Aktivierungsfunktionen beeinflusst. \\
			
		An dieser Stelle lässt sich anmerken, dass man sich einerseits noch intensiver mit \mbox{z. B.} anderen Optimierungsmöglichkeiten des Lernprozesses hätte auseinandersetzen können und andererseits, dass die Experimente in einem größeren Umfang durchgeführt hätten werden können. So hätte man mehr Kombinationen und den Einfluss von mehr Hyperparametern, wie beispielsweise andere Cost-Funktionen, \mbox{testen können.}
		
		% KRITIK AN EXPERIMENTEN
		% Schade, dass nicht genauer auf Optimierungsmöglichkeiten eingegangen werden konnte. 
		% Ergebnisse der Experimente mit Vorsicht zu genießen, schließlich nur einmal getestet, sehr beschränkte Anzahl an Hyperparametern, etc.
			

	
	
	% Verzeichnisse
	\begin{sloppypar}
		\cleardoublepage
		\addcontentsline{toc}{chapter}{Literaturverzeichnis}
		\printbibliography[title={Literaturverzeichnis}]
	\end{sloppypar}
	
	\cleardoublepage
	\addcontentsline{toc}{chapter}{Abbildungsverzeichnis}
	\listoffigures
	
	\cleardoublepage
	\addcontentsline{toc}{chapter}{Tabellenverzeichnis}
	\listoftables
	
	
	
	% Glossar
	% Damit das geht muss zuerst Tools -> Befehle -> Makeglossaries ausgeführt werden
	\printglossary[style=list, type=Abk, nonumberlist]
	\printglossary[style=indexgroup, title=Glossar, nonumberlist]
	
	
	
	% Anhang
	\chapter*{Anhang}
	\addcontentsline{toc}{chapter}{Anhang}
	
		\section*{Anhang A: Notationstabelle}
		\addcontentsline{toc}{section}{Anhang A: Notationstabelle}
	
		\vspace{2mm}
		\newcommand{\newNotationRow}{\\[2.5mm]}
	
	
		\begin{longtable}{p{30mm}p{110mm}} %longtable für mehrseitige Tabelle, sonst tabular
			\hline
			\centering\vspace{1mm} $l$ & \vspace{0mm}Nummer einer Schicht (Indexierung: 1)\newNotationRow	
			\centering $L$ & Letzte Schicht eines KNNs\newNotationRow
			\centering $m$ & Nummer eines Neurons der Schicht $l-1$ (Indexierung: 1)\newNotationRow % Ebenfalls 1, da Indexierung innerhalb eines Layers nicht davon abhängt, der wie vielte Layer es ist
			\centering $n$ & Nummer eines Neurons der Schicht $l$ (Indexierung: 1)\newNotationRow
			\centering $d$ & Nummer einer Feature Map in einer Schicht (Indexierung: 1)\newNotationRow
			\centering $t$ & Variable zur Angabe eines Zeitpunkts (Indexierung: 1)\newNotationRow
			\centering $w_{(l-1,m),(l,n)}$ & Verbindungsgewicht zwischen dem Neuron $m$ der Schicht $l-1$ und dem Neuron $n$ der Schicht $l$ (Skalar) \newNotationRow
			\centering $\vec{w}_{(l-1),(l,n)}$ & Verbindungsgewichte zwischen allen Neuronen der Schicht \mbox{$l-1$} und dem Neuron $n$ der Schicht $l$ (Vektor) \newNotationRow
			\centering $W_{(l-1),(l)}$ & Verbindungsgewichte zwischen allen Neuronen der Schicht \mbox{$l-1$} und allen Neuronen der Schicht $l$; Matrix mit der Form $n\times m$, wobei die $m$ die Anzahl der Neurone in der Schicht $l-1$, $n$ die Anzahl der Neurone der Schicht $l$ angibt\newNotationRow
			\centering $W_{(l-1),(l)}^{(t),(t)}$ & Verbindungsgewichte zwischen allen Neuronen der Schicht $l-1$ zum Zeitpunkt $t$ und allen Neuronen der Schicht $l$ zum Zeitpunkt $t$; Matrix gleicher Form wie $W_{(l-1),(l)}$\newNotationRow
			\centering $W_{(l),(l)}^{(t-1),(t)}$ & Verbindungsgewichte zwischen allen Neuronen der Schicht $l$ zum Zeitpunkt $t-1$ und allen Neuronen der Schicht $l$ zum Zeitpunkt $t$; Matrix mit der Form $n \times n$, wobei $n$ die Anzahl der Neurone der Schicht $l$ angibt\newNotationRow
			\centering $W_{u;(l-1),(l)}^{(t),(t)}$ & Verbindungsgewichte zwischen allen Neuronen der Schicht $l-1$ zum Zeitpunkt $t$ und allen Neuronen der Schicht $l$ zum Zeitpunkt $t$ zur Berechnung von $\vec{u}_{(l)}^{(t)}$; Matrix gleicher Form wie $W_{(l-1),(l)}$; gibt auch $W_{i;(l-1),(l)}^{(t),(t)}$, $W_{o;(l-1),(l)}^{(t),(t)}$ und $W_{z;(l-1),(l)}^{(t),(t)}$ zur Berechnung von $\vec{i}_{(l)}^{(t)}$, $\vec{o}_{(l)}^{(t)}$ und $\vec{z}_{(l)}^{(t)}$\newNotationRow
			\centering $W_{u;(l),(l)}^{(t-1),(t)}$ & Verbindungsgewichte zwischen allen Neuronen der Schicht $l$ zum Zeitpunkt $t-1$ und allen Neuronen der Schicht $l$ zum Zeitpunkt $t$ zur Berechnung von $\vec{u}_{(l)}^{(t)}$; Matrix mit der Form $n \times n$, wobei $n$ die Anzahl der Neurone der Schicht $l$ angibt; gibt auch $W_{i;(l),(l)}^{(t-1),(t)}$, $W_{o;(l),(l)}^{(t-1),(t)}$ und $W_{z;(l),(l)}^{(t-1),(t)}$ zur Berechnung von $\vec{i}_{(l)}^{(t)}$, $\vec{o}_{(l)}^{(t)}$ und $\vec{z}_{(l)}^{(t)}$\newNotationRow
			\centering ${W_{(l),(l+1)}}^\intercal$ & Transponierte Matrix mit den Verbindungsgewichten zwischen allen Neuronen der Schicht $l-1$ und allen Neuronen der Schicht $l$; Matrix mit der Form $m\times n$, wobei die $m$ die Anzahl der Neurone in der Schicht $l-1$, $n$ die Anzahl der Neurone der Schicht $l$ angibt\newNotationRow				
			\centering $b_{(l,n)}$ & Bias des Neurons $n$ in der Schicht $l$ \newNotationRow
			\centering $\vec{b}_{(l)}$ & Vektor mit allen Bias-Werten der Schicht $l$ \newNotationRow
			\centering $b_{(l,d)}$ & Bias, welcher für alle Neurone der Feature Map $d$ der \mbox{Schicht $l$} verwendet wird\newNotationRow				
			\centering $z_{(l,n)}$ & Wert des Neurons $n$ der Schicht $l$ vor Anwendung einer Aktivierungsfunktion\newNotationRow
			\centering $\vec{z}_{(l)}$ & Vektor mit allen Werten der Neurone der Schicht $l$ vor Anwendung einer Aktivierungsfunktion \newNotationRow				
			\centering $f_{(l)}$ & Aktivierungsfunktion der Schicht $l$\newNotationRow
			\centering $f_{(l)}^{\prime}$ & Erste Ableitung der Aktivierungsfunktion der Schicht $l$\newNotationRow
			\centering $f_{i;(l)}$\break $f_{o;(l)}$\break $f_{u;(l)}$ & \break \break Rekurrente Aktivierungsfunktionen der Gates der Schicht $l$ eines LSTM Netzes (meistens Sigmoid-Funktion)\newNotationRow
			\centering $f_{x;(l)}$\break $f_{z;(l)}$ & \break Aktivierungsfunktionen der Schicht $l$ bei LSTM Zellen(meistens TanH-Funktion)\newNotationRow				
			\centering $x_{(l,n)}$ & Ausgabewert des Neurons $n$ der Schicht $l$\newNotationRow
			\centering $\vec{x}_{(l)}$ & Vektor mit allen Ausgabewerten aller Neurone der Schicht $l$\newNotationRow
			\centering $x_{(l,d,n)}$ & Ausgabewert des Neurons $n$ in der Feature Map $d$ der \mbox{Schicht $l$}\newNotationRow
			\centering $x_{(l,n)}^{(t)}$ & Ausgabewert des Neurons $n$ der Schicht $l$ zum Zeitpunkt $t$\newNotationRow
			\centering $\vec{x}_{(l)}^{(t)}$ & Vektor mit allen Ausgabewerten der Neurone der Schicht $l$ zum Zeitpunkt $t$\newNotationRow				
			\centering $R_{(l,d,n)}$ & Local Receptive Field (LRF); Ausschnitt aus der Schicht $l-1$ mit dem das Neuron $n$ der Feature Map $d$ der Schicht $l$ verbunden ist\newNotationRow
			\centering $K_{(l,d)}$ & Verbindungsgewichte, mit denen alle LRFs der Feature Map $d$ der Schicht $l$ gewichtet werden\newNotationRow
			\centering $s_{(l)}$ & Stride-Wert der Schicht $l$; gibt an, in welchem Abstand sich die LRFs voneinander befinden\newNotationRow
			\centering $p_{(l)}$ & Padding-Wert der Schicht $l$, gibt an, wie viele zusätzliche Zeilen und Spalten um die Matrix der Schicht $l-1$ mit dem Wert 0 hinzugefügt werden sollen\newNotationRow				
			\centering $\vec{c}_{(l)}^{(t)}$ & Zell Status der Schicht $l$ zum Zeitpunkt $t$\newNotationRow
			\centering $\vec{i}_{(l)}^{(t)}$ & Vektor, welcher angibt, welche Werte von $\vec{z}_{(l)}^{(t)}$ zum Zell Status hinzugefügt werden sollen \newNotationRow
			\centering $\vec{o}_{(l)}^{(t)}$ & Vektor, welcher angibt, welche \newNotationRow
			\centering $\vec{u}_{(l)}^{(t)}$ & Vektor, welcher angibt, welche Werte des vorherigen Zell Status zum Zeitpunkt $t-1$ zur Berechnung des Zell Status zum Zeitpunkt $t$ verwendet werden\newNotationRow
			\centering $\vec{z}_{(l)}^{(t)}$ & Vektor mit den zum Zell Status neu hinzukommenden Werten\newNotationRow				
			\centering $C$ & Cost-Funktion\newNotationRow
			\centering $\textrm{Tr}$ & Anzahl der Trainingsbeispiele\newNotationRow
			\centering $\vec{x}_{(1)}(\textrm{tr}:i)$ & Eingabe des $i$-ten Trainingsbeispiels\newNotationRow
			\centering $\vec{y}_{(1)}(\textrm{tr}:i)$ & Gewünschte Ausgabe des $i$-ten Trainingsbeispiels\newNotationRow
			\centering $\eta$ & Lernrate; gibt an, wie groß die Veränderung der Parameter relativ zu deren partiellen Ableitungen sein sollen \newNotationRow
			\centering $\frac{\partial a}{\partial b}$ & Partielle Ableitung von $a$ in Abhängigkeit von $b$ \newNotationRow
			\centering $\delta_{(l,n)}$ & Fehler des Neurons $n$ der Schicht $l$\newNotationRow
			\centering $\vec{\delta}_{(l)}$ & Vektor mit allen Fehlern aller Neurone der Schicht $l$\newNotationRow
			\centering $\nabla_{\vec{x}_{(L)}}C$ & Gradient; Vektor dessen Elemente die partiellen Ableitungen $\frac{\partial C}{\partial x_{(L,n)}}$ sind; zeigt in die Richtung des steilsten Anstiegs einer Funktion\newNotationRow %Gradient				
			\centering $\langle A,B \rangle_F$ & Frobenius-Skalarprodukt; Zwei Matrizen gleicher Dimensionen werden elementweise miteinander multipliziert und die Produkte aufsummiert und ergeben ein Skalar\newNotationRow
			\centering $\odot$ & Hadamard Produkt; Zwei Vektoren gleicher Dimension werden elementweise miteinander multipliziert und ergeben einen Vektor mit der gleichen Dimension\newNotationRow
			\centering $\textrm{H}(X)$\break$\textrm{W}(X)$\break$\textrm{D}(X)$ & \break \break Gibt die Größe der ersten/zweiten/dritten Dimension einer Matrix $X$ an\newNotationRow
			\centering $\textrm{len}(\vec{x})$ & Gibt die Anzahl der Elemente des Vektors $\vec{x}$ an \newNotationRow
			\centering $\textrm{max}(a,b)$ & Funktion, welche zwei Werte $a$ und $b$ vergleicht und den größeren der beiden als Funktionswert annimmt\newNotationRow
			\hline
			\caption{Notationstabelle (Quelle: Eigene Tabelle)}
		\end{longtable}
	
	
	
		\newpage
		\vspace*{2cm}			
		\section*{Anhang B: Beweise für die Formeln von Backpropagation}
			\addcontentsline{toc}{section}{Anhang B: Beweise für die Formeln von Backpropagation}
			
			\subsection*{Backpropagation Formel 1}
				Die erste Backpropagation Formel dient zur Berechnung eines Vektors mit den Fehlern der Neurone der letzten Schicht.

				\begin{equation}
					\vec{\delta}_{(L)} = \nabla_{\vec{x}_{(L)}}C \odot f_{(L)}^{\prime} (\vec{z}_{(L)})
				\end{equation}
				
				Diese lässt sich für einzelne Fehler $\delta_{(L,n)}$ umschreiben:
				
				\begin{equation}\label{Beweis1Formel2}
					\delta_{(L,n)} = \frac{\partial C}{\partial x_{(L,n)}} \cdot f_{(L)}^{\prime} (z_{(L,n)})
				\end{equation}
				
				Ursprünglich wird in Kapitel \ref{Backpropagation} der einzelne Fehler $\delta_{(l,n)}$ folgendermaßen definiert:
				
				\begin{equation}
					\delta_{(l,n)} = \frac{\partial C}{\partial z_{(l,n)}}
				\end{equation}
				
				Diese Formel lässt sich umschreiben als:
				
				\begin{equation}\label{Beweis1Formel1}
					\delta_{(l,n)} = \frac{\partial C}{\partial x_{(l,n)}} \cdot \frac{\partial x_{(l,n)}}{\partial z_{(l,n)}}
				\end{equation}
				
				Da Folgendes gilt…
				
				\begin{equation}
					\frac{\partial x_{(l,n)}}{\partial z_{(l,n)}} = \frac{\partial f_{(l)}(z_{(l,n)})}{\partial z_{(l,n)}} = f_{(l)}^{\prime} (z_{(l,n)})
				\end{equation}			
				
				…lässt sich dies in Formel \ref{Beweis1Formel1} einsetzen, woraus Formel \ref{Beweis1Formel2} folgt.
				
	
	
			\subsection*{Backpropagation Formel 2}
				Diese Formel dient zur Berechnung der Fehler einer Schicht $l$ in Abhängigkeit der Fehler der nachfolgenden Schicht $l+1$.
				
				\begin{equation}
					\vec{\delta}_{(l)} = ({W_{(l),(l+1)}}^\intercal \cdot \vec{\delta}_{(l+1)}) \odot f_{(l)}^{\prime} (\vec{z}_{(l)})
				\end{equation}
			
				Diese kann für einen einzelnen Fehler $\delta_{(l,n)}$ des Neurons $n$ der Schicht $l$ umgeformt werden, wobei der Vektor $\vec{w}_{(l,n),(l+1)}$ alle Gewichte, welche vom Neuron $n$ der Schicht $l$ zu allen Neuronen der Schicht $l+1$ gehen, beinhaltet.
				
				\begin{equation}\label{Beweis2Formel2}
					\delta_{(l,n)} = \vec{w}_{(l,n),(l+1)} \cdot \vec{\delta}_{(l+1)} \cdot f_{(l)}^{\prime} (z_{(l,n)})
				\end{equation}
				
				Ursprünglich wird in Kapitel \ref{Backpropagation} der einzelne Fehler $\delta_{(l,n)}$ folgendermaßen definiert:
				
				\begin{equation}
					\delta_{(l,n)} = \frac{\partial C}{\partial z_{(l,n)}}
				\end{equation}
				
				Dieser lässt sich umschreiben als:
				
				\begin{equation}
					\delta_{(l,n)} = \frac{\partial C}{\partial \vec{z}_{(l+1)}} \cdot \frac{\partial \vec{z}_{(l+1)}}{\partial z_{(l,n)}}
				\end{equation}
				
				Da $\frac{\partial C}{\partial \vec{z}_{(l+1)}} = \vec{\delta}_{(l+1)}$, lässt sich dies in die vorherige Formel einsetzen:
				
				\begin{equation}\label{Beweis2Formel1}
					\delta_{(l,n)} = \frac{\partial \vec{z}_{(l+1)}}{\partial z_{(l,n)}} \cdot \vec{\delta}_{(l+1)}
				\end{equation}
				
				Aus der Formel von $z_{(l+1,m)}$
				
				\begin{equation}
					\begin{split}
						z_{(l+1,m)} & = \vec{w}_{(l),(l+1,n)} \cdot \vec{x}_{(l)} + b_{(l+1,n)} \\
						&= \vec{w}_{(l),(l+1,n)} \cdot f_{(l)}(\vec{z}_{(l)}) + b_{(l+1,n)} \\
						&= \sum_{i=1}^{j}(w_{(l,i),(l+1,m)} \cdot f_{(l)}(z_{(l,i)})) + b_{(l+1,m)}
					\end{split}
				\end{equation}
				
				wobei $j$ hier die Anzahl aller Neuronen der Schicht $l$ ist, lässt sich folgende partielle Ableitung bilden:
				
				\begin{equation}
					\frac{\partial z_{(l+1,m)}}{\partial z_{(l,n)}} = w_{(l,n),(l+1,m)} \cdot f_{(l)}^{\prime} (z_{(l,n)})
				\end{equation}
				
				Diese partielle Ableitung lässt sich auch für den Vektor $\vec{z}_{(l+1)}$ bilden:
				
				\begin{equation}
					\frac{\partial \vec{z}_{(l+1)}}{\partial z_{(l,n)}} = \vec{w}_{(l,n),(l+1)} \cdot f_{(l)}^{\prime}(z_{(l,n)})
				\end{equation}
				
				Setzt man diese nun in Formel \ref{Beweis2Formel1} ein, so erhält man Formel \ref{Beweis2Formel2}.
				
	
	
			\subsection*{Backpropagation Formel 3}
				Die dritte Backpropagation Formel gibt an, wie sich der Wert der Cost-Funktion in Abhängigkeit des Bias des Neurons $n$ der Schicht $l$ verändert.
	
				\begin{equation}\label{Beweis3Formel1}
					\frac{\partial C}{\partial b_{(l,n)}} = \delta_{(l,n)}
				\end{equation}
				
				Dieser Bruch lässt sich erweitern:
				
				\begin{equation}
					\frac{\partial C}{\partial x_{(l,n)}} \cdot \frac{\partial x_{(l,n)}}{\partial z_{(l,n)}} \cdot \frac{\partial z_{(l,n)}}{\partial b_{(l,n)}} = \delta_{(l,n)}
				\end{equation}
				
				Die beiden Gleichungen $\frac{\partial x_{(l,n)}}{\partial z_{(l,n)}} = \frac{\partial f_{l}(z_{(l,n)})}{\partial z_{(l,n)}} = f_{(l)}^{\prime} (z_{(l,n)})$ und $\frac{\partial z_{(l,n)}}{\partial b_{(l,n)}} = 1$ (da $b$ nur addiert wird, fällt dieser beim Differenzieren weg) lassen sich nun in die vorherige Gleichung einsetzen:
				
				\begin{equation}
					\frac{\partial C}{\partial x_{(l,n)}} \cdot f_{(l)}^{\prime} (z_{(l,n)}) \cdot 1 = \delta_{(l,n)}
				\end{equation}
				
				Da die linke Seite die Definition von $\delta_{(l,n)}$ ist, ist die letzte Gleichung und somit auch die ursprüngliche Formel \ref{Beweis3Formel1} richtig. 
				
				
	
			\subsection*{Backpropagation Formel 4}
				Die letzte Backpropagation Formel gibt an, wie sich der Wert der Cost-Funktion in Abhängigkeit des Gewichts ändert. 
	
				\begin{equation}
					\frac{\partial C}{\partial w_{(l-1,m), (l,n)}} = \delta_{(l,n)} \cdot x_{(l-1,m)}
				\end{equation}
				
				Dieser Bruch lässt sich erweitern:
				
				\begin{equation}
					\frac{\partial C}{\partial x_{(l,n)}} \cdot \frac{\partial x_{(l,n)}}{\partial z_{(l,n)}} \cdot \frac{\partial z_{(l,n)}}{\partial w_{(l-1,m), (l,n)}} = \delta_{(l,n)} \cdot x_{(l-1,m)}
				\end{equation}
				
				Die beiden Gleichungen $\frac{\partial x_{(l,n)}}{\partial z_{(l,n)}} = \frac{\partial f_{(l)}(z_{(l,n)})}{\partial z_{(l,n)}} = f_{(l)}^{\prime}(z_{(l,n)})$ und $\frac{\partial z_{(l,n)}}{\partial w_{(l-1,m), (l,n)}} = x_{(l-1,m)}$ (da $w_{(l-1,m), (l,n)}$ jener Faktor ist, mit dem $x_{(l-1,m)}$ zur Berechnung von $z_{(l,n)}$ multipliziert wird, ist $x_{(l-1,m)}$ die partielle Ableitung von $z_{(l,n)}$ in Abhängigkeit von $w_{(l-1,m), (l,n)}$) lassen sich nun in die vorherige Gleichung einsetzen:
				
				\begin{equation}
					\frac{\partial C}{\partial x_{(l,n)}} \cdot f_{(l)}^{\prime}(z_{(l,n)}) \cdot x_{(l-1,m)} = \delta_{(l,n)} \cdot x_{(l-1,m)}
				\end{equation}
				
				Da nun der Term $\frac{\partial C}{\partial x_{(l,n)}} \cdot f_{(l)}^{\prime}(z_{(l,n)})$ die Definition von $\delta_{(l,n)}$ ist, ist die Gleichung und somit auch die ursprüngliche Formel richtig.
				
	
	
		\newpage
		\vspace*{2cm}	
		\section*{Anhang C: In Experimenten verwendete Modelle}
			\addcontentsline{toc}{section}{Anhang C: In Experimenten verwendete Modelle}
	
			\subsection*{C.1 Modelle für Experimente zu FFNNs}
				\addcontentsline{toc}{subsection}{C.1 Modelle für Experimente zu FFNNs}
				
				\begin{footnotesize}
					\begin{longtable}[l]{|l|l|l|l|}
						\hline
						Modell-Nr             & Aufbau                                                                                                                                                                               & Bias                                                             & Weights und Biases                                      \\ \hline
						\endfirsthead
						%
						\endhead
						%
						\multirow[t]{2}{*}[\myshiftdown]{1} & \multirow[t]{2}{*}{\begin{tabular}[c]{@{}l@{}}\\ 1. Input Layer (784 Neurone)\\ 2. Hidden Layer (10 Neurone)\\ 3. Output Layer (10 Neurone)\\ \end{tabular}}               & \begin{tabular}[c]{@{}l@{}}False\\ \ \end{tabular} & \begin{tabular}[c]{@{}l@{}}7.950\\ \ \end{tabular}   \\ \cline{3-4} 
						&                                                                                                                                                                                      & \begin{tabular}[c]{@{}l@{}}True\\ \ \end{tabular}  & \begin{tabular}[c]{@{}l@{}}7.960\\ \ \end{tabular}   \\ \hline
						\multirow[t]{2}{*}[\myshiftdown]{2} & \multirow[t]{2}{*}{\begin{tabular}[c]{@{}l@{}}\\ 1. Input Layer (784 Neurone)\\ 2. Hidden Layer (100 Neurone)\\ 3. Output Layer (10 Neurone)\end{tabular}}                                 & \begin{tabular}[c]{@{}l@{}}False\\ \ \end{tabular} & \begin{tabular}[c]{@{}l@{}}79.410\\ \ \end{tabular}  \\ \cline{3-4} 
						&                                                                                                                                                                                      & \begin{tabular}[c]{@{}l@{}}True\\ \ \end{tabular}  & \begin{tabular}[c]{@{}l@{}}79.510\\ \ \end{tabular}  \\ \hline
						\multirow[t]{2}{*}[\myshiftdown]{3} & \multirow[t]{2}{*}{\begin{tabular}[c]{@{}l@{}}\\ 1. Input Layer (784 Neurone)\\ 2. Hidden Layer (1000 Neurone)\\ 3. Output Layer (10 Neurone)\end{tabular}}                                & \begin{tabular}[c]{@{}l@{}}False\\ \ \end{tabular} & \begin{tabular}[c]{@{}l@{}}794.010\\ \ \end{tabular} \\ \cline{3-4} 
						&                                                                                                                                                                                      & \begin{tabular}[c]{@{}l@{}}True\\ \ \end{tabular}  & \begin{tabular}[c]{@{}l@{}}795.010\\ \ \end{tabular} \\ \hline
						\multirow[t]{2}{*}[\myshiftdown]{4} & \multirow[t]{2}{*}{\begin{tabular}[c]{@{}l@{}}\\ \\ 1. Input Layer (784 Neurone)\\ 2. Hidden Layer (10 Neurone)\\ 3. Hidden Layer (10 Neurone)\\ 4. Output Layer (10 Neurone)\end{tabular}}   & \begin{tabular}[c]{@{}l@{}}False\\ \ \end{tabular} & \begin{tabular}[c]{@{}l@{}}8.050\\ \ \end{tabular}   \\ \cline{3-4} 
						&                                                                                                                                                                                      & \begin{tabular}[c]{@{}l@{}}True\\ \ \end{tabular}  & \begin{tabular}[c]{@{}l@{}}8.070\\ \ \end{tabular}   \\ \hline
						\multirow[t]{2}{*}[\myshiftdown]{5} & \multirow[t]{2}{*}{\begin{tabular}[c]{@{}l@{}}\\ \\ 1. Input Layer (784 Neurone)\\ 2. Hidden Layer (100 Neurone)\\ 3. Hidden Layer (10 Neurone)\\ 4. Output Layer (10 Neurone)\end{tabular}}  & \begin{tabular}[c]{@{}l@{}}False\\ \ \end{tabular} & \begin{tabular}[c]{@{}l@{}}79.510\\ \ \end{tabular}  \\ \cline{3-4} 
						&                                                                                                                                                                                      & \begin{tabular}[c]{@{}l@{}}True\\ \ \end{tabular}  & \begin{tabular}[c]{@{}l@{}}79.620\\ \ \end{tabular}  \\ \hline
						\multirow[t]{2}{*}[\myshiftdown]{6} & \multirow[t]{2}{*}{\begin{tabular}[c]{@{}l@{}}\\ \\ 1. Input Layer (784 Neurone)\\ 2. Hidden Layer (10 Neurone)\\ 3. Hidden Layer (100 Neurone)\\ 4. Output Layer (10 Neurone)\end{tabular}}  & \begin{tabular}[c]{@{}l@{}}False\\ \ \end{tabular} & \begin{tabular}[c]{@{}l@{}}9.850\\ \ \end{tabular}   \\ \cline{3-4} 
						&                                                                                                                                                                                      & \begin{tabular}[c]{@{}l@{}}True\\ \ \end{tabular}  & \begin{tabular}[c]{@{}l@{}}9.960\\ \ \end{tabular}   \\ \hline
						\multirow[t]{2}{*}[\myshiftdown]{7} & \multirow[t]{2}{*}{\begin{tabular}[c]{@{}l@{}}\\ \\ 1. Input Layer (784 Neurone)\\ 2. Hidden Layer (100 Neurone)\\ 3. Hidden Layer (100 Neurone)\\ 4. Output Layer (10 Neurone)\end{tabular}} & \begin{tabular}[c]{@{}l@{}}False\\ \ \end{tabular} & \begin{tabular}[c]{@{}l@{}}89.410\\ \ \end{tabular}  \\ \cline{3-4} 
						&                                                                                                                                                                                      & \begin{tabular}[c]{@{}l@{}}True\\ \ \end{tabular}  & \begin{tabular}[c]{@{}l@{}}89.610\\ \ \end{tabular}  \\ \hline
						\caption{Die verschiedenen, bei den Experimenten verwendeten Modelle von \\ FFNNs und die Anzahl ihrer Weights und Biases (Quelle: Eigene Tabelle)}
					\end{longtable}
				\end{footnotesize}
	
			\newpage
			\subsection*{C.2 Modelle für Experimente zu CNNs}
				\addcontentsline{toc}{subsection}{C.2 Modelle für Experimente zu CNNs}
				
				\begin{footnotesize}
					\begin{longtable}[l]{|l|p{60mm}|l|l|l|}
						\hline
						Modell-Nr              & Aufbau & Kernel               & Stride & Weights und Biases \\ \hline
						\endfirsthead
						%
						\endhead
						%
						\multirow[t]{16}{*}[\shiftdown]{1} & \multirow[t]{16}{*}[\shiftdown]{\begin{tabular}[c]{@{}l@{}}\\ \\ \\ \\1. Input Layer (784 Neurone)\\ 2. Convolutional Layer (10 FMs)\\ 3. Pooling Layer (10 FMs)\\ 4. Fully-Connected Layer (10 \\ Neurone)\end{tabular}} & \multirow[t]{4}{*}[\shiftdown]{2,2} & 1,1    & 78.460                         \\ \cline{4-5} 
						& &                     & 1,2    & 19.660                         \\ \cline{4-5} 
						& &                     & 2,1    & 19.660                         \\ \cline{4-5} 
						& &                     & 2,2    & 4.960                         \\ \cline{3-5} 
						& & \multirow[t]{4}{*}[\shiftdown]{2,3} & 1,1    & 78.480                         \\ \cline{4-5} 
						& &                     & 1,2    & 19.680                         \\ \cline{4-5} 
						& &                     & 2,1    & 19.680                         \\ \cline{4-5} 
						& &                     & 2,2    & 4.980                         \\ \cline{3-5} 
						& & \multirow[t]{4}{*}[\shiftdown]{3,2} & 1,1    & 78.480                         \\ \cline{4-5} 
						& &                     & 1,2    & 19.680                         \\ \cline{4-5} 
						& &                     & 2,1    & 19.680                         \\ \cline{4-5} 
						& &                     & 2,2    & 4.980                         \\ \cline{3-5} 
						& & \multirow[t]{4}{*}[\shiftdown]{3,3} & 1,1    & 78.510                         \\ \cline{4-5} 
						& &                     & 1,2    & 19.710                         \\ \cline{4-5} 
						& &                     & 2,1    & 19.710                        \\ \cline{4-5} 
						& &                     & 2,2    & 5.010                        \\ \hline
						\multirow[t]{16}{*}[\shiftdown]{2} & \multirow[t]{16}{*}[\shiftdown]{\begin{tabular}[c]{@{}l@{}}\\ \\ \\ \\ \\1. Input Layer (784 Neurone)\\ 2. Convolutional Layer (10 FMs)\\ 3. Convolutional Layer (10 FMs)\\4. Pooling Layer (10 FMs)\\ 5. Fully-Connected Layer (10 \\ Neurone)\end{tabular}}& \multirow[t]{4}{*}[\shiftdown]{2,2} & 1,1    & 78.870                         \\ \cline{4-5} 
						& &                     & 1,2    & 11.670                         \\ \cline{4-5} 
						& &                     & 2,1    & 11.670                         \\ \cline{4-5} 
						& &                     & 2,2    & 2.070                        \\ \cline{3-5} 
						& & \multirow[t]{4}{*}[\shiftdown]{2,3} & 1,1    & 79.090                         \\ \cline{4-5} 
						& &                     & 1,2    & 11.890                         \\ \cline{4-5} 
						& &                     & 2,1    & 11.890                        \\ \cline{4-5} 
						& &                     & 2,2    & 2.290                         \\ \cline{3-5} 
						& & \multirow[t]{4}{*}[\shiftdown]{3,2} & 1,1    & 79.090                         \\ \cline{4-5} 
						& &                     & 1,2    & 11.890                         \\ \cline{4-5} 
						& &                     & 2,1    & 11.890                         \\ \cline{4-5} 
						& &                     & 2,2    & 2.290                         \\ \cline{3-5} 
						& & \multirow[t]{4}{*}[\shiftdown]{3,3} & 1,1    & 79.420                         \\ \cline{4-5} 
						& &                     & 1,2    & 12.220                         \\ \cline{4-5} 
						& &                     & 2,1    & 12.220                         \\ \cline{4-5} 
						& &                     & 2,2    & 2.620                         \\ \hline
						\pagebreak
						\hline
						\multirow[t]{16}{*}[\shiftdown]{3} & \multirow[t]{16}{*}[\shiftdown]{\begin{tabular}[c]{@{}l@{}}\\ \\ \\ \\ \\ \\1. Input Layer (784 Neurone)\\ 2. Convolutional Layer (10 FMs)\\ 3. Pooling Layer (10 FMs)\\4. Convolutional Layer (10 FMs)\\ 5. Pooling Layer (10 FMs)\\ 6. Fully-Connected Layer (10 \\ Neurone)\end{tabular}}& \multirow[t]{4}{*}[\shiftdown]{2,2} & 1,1    & 78.870                         \\* \cline{4-5} 
						& &                     & 1,2    & 6.070                         \\* \cline{4-5} 
						& &                     & 2,1    & 6.070                         \\* \cline{4-5} 
						& &                     & 2,2    & 870                         \\* \cline{3-5} 
						& & \multirow[t]{4}{*}[\shiftdown]{2,3} & 1,1    & 79.090                         \\* \cline{4-5} 
						& &                     & 1,2    & 6.290                         \\ \cline{4-5} 
						& &                     & 2,1    & 6.290                         \\ \cline{4-5} 
						& &                     & 2,2    & 1.090                         \\ \cline{3-5} 
						& & \multirow[t]{4}{*}[\shiftdown]{3,2} & 1,1    & 79.090                         \\ \cline{4-5} 
						& &                     & 1,2    & 6.290                         \\ \cline{4-5} 
						& &                     & 2,1    & 6.290                         \\ \cline{4-5} 
						& &                     & 2,2    & 1.090                         \\ \cline{3-5} 
						%\pagebreak
						& & \multirow[t]{4}{*}[\shiftdown]{3,3} & 1,1    & 79.420                         \\ \cline{4-5} 
						& &                     & 1,2    & 6.620                         \\ \cline{4-5} 
						& &                     & 2,1    & 6.620                         \\ \cline{4-5} 
						& &                     & 2,2    & 1.420                         \\ \hline
						\multirow[t]{16}{*}[\shiftdown]{4} & \multirow[t]{16}{*}[\shiftdown]{\begin{tabular}[c]{@{}l@{}}\\ \\ \\ \\ \\ \\ \\ \\1. Input Layer (784 Neurone)\\ 2. Convolutional Layer (10 FMs)\\ 3. Convolutional Layer (10 FMs)\\4. Pooling Layer (10 FMs)\\ 5. Convolutional Layer (10 FMs)\\ 6. Convolutional Layer (10 FMs)\\7. Pooling Layer (10 FMs)\\ 8. Fully-Connected Layer (10 \\ Neurone)\end{tabular}}& \multirow[t]{4}{*}[\shiftdown]{2,2} & 1,1    & 79.690                         \\ \cline{4-5} 
						& &                     & 1,2    & 4.090                         \\ \cline{4-5} 
						& &                     & 2,1    & 4.090                         \\ \cline{4-5} 
						& &                     & 2,2    & 1.390                         \\ \cline{3-5} 
						& & \multirow[t]{4}{*}[\shiftdown]{2,3} & 1,1    & 80.310                         \\ \cline{4-5} 
						& &                     & 1,2    & 4.710                         \\ \cline{4-5} 
						& &                     & 2,1    & 4.710                         \\ \cline{4-5} 
						& &                     & 2,2    & 2.010                         \\ \cline{3-5} 
						& & \multirow[t]{4}{*}[\shiftdown]{3,2} & 1,1    & 80.310                         \\ \cline{4-5} 
						& &                     & 1,2    & 4.710                         \\ \cline{4-5} 
						& &                     & 2,1    & 4.710                         \\ \cline{4-5} 
						& &                     & 2,2    & 2.010                         \\ \cline{3-5} 
						& & \multirow[t]{4}{*}[\shiftdown]{3,3} & 1,1    & 81.240                         \\ \cline{4-5} 
						& &                     & 1,2    & 5.640                         \\ \cline{4-5} 
						& &                     & 2,1    & 5.640                         \\ \cline{4-5} 
						& &                     & 2,2    & 2.940                         \\ \hline
						\caption{Die verschiedenen, bei den Experimenten verwendeten Modelle von CNNs und die Anzahl ihrer Weights und Biases (Quelle: Eigene Tabelle)}
					\end{longtable}
				\end{footnotesize}
	
			\newpage
			\subsection*{C.3 Modelle für Experimente zu LSTMs}
				\addcontentsline{toc}{subsection}{C.2 Modelle für Experimente zu LSTMs}
				
				\begin{footnotesize}
					\begin{longtable}[l]{|l|l|l|}
						\hline
						Modell-Nr & Aufbau                                                                                                                                                              & Weights und Biases \\ \hline
						\endfirsthead
						%
						\endhead
						%
						1      & \begin{tabular}[t]{@{}l@{}}1. Input Layer (784 Neurone)\\ 2. LSTM Layer ($\textrm{len}(\vec{x}_{(l)}^{(t)}) = 10$)\end{tabular}                                                                    & 1.670              \\ \hline
						2      & \begin{tabular}[t]{@{}l@{}}1. Input Layer (784 Neurone)\\ 2. LSTM Layer ($\textrm{len}(\vec{x}_{(l)}^{(t)}) = 10$)\\ 3. LSTM Layer ($\textrm{len}(\vec{x}_{(l)}^{(t)}) = 10$)\end{tabular}                                        & 2.510              \\ \hline
						3      & \begin{tabular}[t]{@{}l@{}}1. Input Layer (784 Neurone)\\ 2. Fully-Connected Layer (10\\ Neurone)\\ 3. LSTM Layer ($\textrm{len}(\vec{x}_{(l)}^{(t)}) = 10$)\end{tabular}                          & 1.240              \\ \hline
						4      & \begin{tabular}[t]{@{}l@{}}1. Input Layer (784 Neurone)\\ 2. Fully-Connected Layer (10\\ Neurone)\\ 3. LSTM Layer ($\textrm{len}(\vec{x}_{(l)}^{(t)}) = 10$)\\ 4. LSTM Layer ($\textrm{len}(\vec{x}_{(l)}^{(t)}) = 10$)\end{tabular} & 2.080              \\ \hline
						\caption{Die verschiedenen, bei den Experimenten verwendeten Modelle von LSTMs und die Anzahl ihrer Weights und Biases (Quelle: Eigene Tabelle)}
					\end{longtable}
				\end{footnotesize}	
				
				
				
				
		
	

	% Anhang D: Programmcode
	\newpage
	\vspace*{2cm}
	\section*{Anhang D: Programmcode}
		\addcontentsline{toc}{section}{Anhang D: Programmcode}
	
		\subsection*{D.1 Programmcode für FFNNs}
			\addcontentsline{toc}{subsection}{D.1 Programmcode für FFNNs}
			{
				\renewcommand*{\ttdefault}{txtt}
				\lstinputlisting{codeListings/ffnn_for_tex.py}
			}
			
		\newpage
		\subsection*{D.2 Programmcode für CNNs}
			\addcontentsline{toc}{subsection}{D.2 Programmcode für CNNs}		
			{
				\renewcommand*{\ttdefault}{txtt}
				\lstinputlisting{codeListings/cnn_for_tex.py}
			}
	
		\newpage
		\subsection*{D.3 Programmcode für LSTMs}
			\addcontentsline{toc}{subsection}{D.3 Programmcode für LSTMs}
			{
				\renewcommand*{\ttdefault}{txtt}
				\lstinputlisting{codeListings/lstm_for_tex.py}
			}
	
	
	
	% Anhang E: Ergebnisse der Experimente 
	\newpage
	\vspace*{2cm}	
	\section*{Anhang E: Ergebnisse der Experimente}
		\addcontentsline{toc}{section}{Anhang E: Ergebnisse der Experimente}
		
		\def\myrot#1{\rotatebox{90}{\csname csvcol#1\endcsname\ }}
		
		\subsection*{E.1 Tabelle der Ergebnisse der Experimente mit FFNNs}
			Die Spalte mit der Anzahl der Epochs (Wert: 3) wurde weggelassen, da ihr Wert für alle Konfigurationen konstant ist.
			\addcontentsline{toc}{subsection}{E.1 Tabelle der Ergebnisse der Experimente mit FFNNs}
			\csvreader[before reading=\footnotesize, /csv/separator=semicolon,tabular=|r|*{12}{c}|,longtable={|c|c|c|c|c|c|c|c|c|c|c|c|},
			nohead,column count=12,table head=\hline,late after first line=\\\hline,table foot=\hline\caption{Tabelle mit den Ergebnissen der Experimente mit FFNNs (Quelle: Eigene Tabelle)}]{ergebnisseFFNNfinal6.csv}{}{\csviffirstrow{\myrot{i} & \myrot{ii} & \myrot{iii} & \myrot{iv} & \myrot{v} & \myrot{vi} & \myrot{vii} & \myrot{viii} & \myrot{ix} & \myrot{x} & \myrot{xi} & \myrot{xii}}{\csvlinetotablerow}}
		
		
		\newpage
		\normalsize
		\subsection*{E.2 Tabelle der Ergebnisse der Experimente mit CNNs}
			Die Spalten Aktivierungsfunktion (Wert: relu), Pooling (Wert: Max), Bias (Wert: True) und Epoch (Wert: 3) wurden weggelassen, da ihre Werte für alle Konfigurationen konstant sind. Diese Werte lieferten bei Vorversuchen jene CNNs mit den höchsten Trefferquoten. 
			\addcontentsline{toc}{subsection}{E.2 Tabelle der Ergebnisse der Experimente mit CNNs}
			\csvreader[before reading=\footnotesize, /csv/separator=semicolon,tabular=|r|*{13}{c}|,longtable={|c|c|c|c|c|c|c|c|c|c|c|c|c|},
			nohead,column count=13,table head=\hline,late after first line=\\\hline,table foot=\hline\caption{Tabelle mit den Ergebnissen der Experimente mit CNNs (Quelle: Eigene Tabelle)}]{ergebnisseCNNfinal6.csv}{}{\csviffirstrow{\myrot{i} & \myrot{ii} & \myrot{iii} & \myrot{iv} & \myrot{v} & \myrot{vi} & \myrot{vii} & \myrot{viii} & \myrot{ix} & \myrot{x} & \myrot{xi} & \myrot{xii} & \myrot{xiii}}{\csvlinetotablerow}}
		
		\newpage
		\normalsize
		\subsection*{E.3 Tabelle der Ergebnisse der Experimente mit LSTMs}
			Die Spalten Bias (Wert: True) und Epoch (Wert: 3) wurden weggelassen, da ihre Werte für alle Konfigurationen konstant sind.
			\addcontentsline{toc}{subsection}{E.3 Tabelle der Ergebnisse der Experimente mit LSTMs}
			
			\csvreader[before reading=\footnotesize, /csv/separator=semicolon,tabular=|r|*{13}{c}|,longtable={|c|c|c|c|c|c|c|c|c|c|c|c|c|}, nohead,
			column count=13,table head=\hline,late after first line=\\\hline,table foot=\hline\caption{Tabelle mit den Ergebnissen der Experimente mit LSTMs (Quelle: Eigene Tabelle)}]{ergebnisseLSTMfinal6.csv}{}{\csviffirstrow{\myrot{i} & \myrot{ii} & \myrot{iii} & \myrot{iv} & \myrot{v} & \myrot{vi} & \myrot{vii} & \myrot{viii} & \myrot{ix} & \myrot{x} & \myrot{xi} & \myrot{xii} & \myrot{xiii}}{\csvlinetotablerow}}
		
		\normalsize



	% Anhang F: Daten-DVD
	\newpage
	\vspace*{2cm}	
	\section*{Anhang F: Daten-DVD}
	\addcontentsline{toc}{section}{Anhang F: Daten-DVD}
	\begin{center}
		\hspace*{-1cm}
		\begin{tikzpicture}
			\draw (0,0) -- (12.6,0) -- (12.6,12.6) -- (0,12.6) -- (0,0);
			\draw[dashed] (2,3.3) -- (3.5,3.3) -- (3.5,7.3) -- (2,7.3) -- (2,3.3);
			\draw[dashed] (2+5.6+1.5,3.3) -- (3.5+5.6+1.5,3.3) -- (3.5+5.6+1.5,7.3) -- (2+5.6+1.5,7.3) -- (2+5.6+1.5,3.3);
			\draw node[black,midway,yshift=5.3cm,xshift=2.75cm, rotate=90] at (0,0) {\centering Klebestelle};
			\draw node[black,midway,yshift=5.3cm,xshift=9.85cm, rotate=270] at (0,0) {\centering Klebestelle};
		\end{tikzpicture}
	\end{center}
	Dieser Datenträger enthält eine digitale Fassung der Arbeit, den originalen Programmcode für alle drei Netztypen sowie die bei den Experimenten entstandenen KNNs und Daten. 
	
	
	
	% Selbstständigkeitserklärung
	\chapter*{Selbstständigkeitserklärung}
		Name: Tobias Prisching\newline
	
		Ich erkläre, dass ich diese vorwissenschaftliche Arbeit eigenständig angefertigt und nur die im Literaturverzeichnis angeführten Quellen und Hilfsmittel benutzt habe.
	
		\vspace{3cm}
		
		\parbox{6cm}{\centering\hrule
		\strut \centering\footnotesize Ort, Datum} \hfill\parbox{6cm}{\hrule
		\strut \centering\footnotesize Unterschrift}
	
\end{document}